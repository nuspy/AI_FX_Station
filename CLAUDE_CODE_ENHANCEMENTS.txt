═══════════════════════════════════════════════════════════════════════════════
🚀 FOREXGPT - COMPLETE IMPLEMENTATION GUIDE FOR CLAUDE CODE
═══════════════════════════════════════════════════════════════════════════════

📅 Data: 04 Ottobre 2025
🎯 Obiettivo: Portare il progetto da 6.5/10 a 9.0-10.0/10
⚡ Status: POST-implementazione volume reale via cforex
🤖 Target: Claude Code con Claude Sonnet 4.5

═══════════════════════════════════════════════════════════════════════════════
📊 EXECUTIVE SUMMARY - ANALISI INTEGRATA PRE vs POST VOLUME
═══════════════════════════════════════════════════════════════════════════════

## CONFRONTO REVIEWS

┌────────────────┬──────────────────────┬──────────────────────┬────────┐
│ Aspetto        │ PRE-Volume (Teorica) │ POST-Volume (Attuale)│ Delta  │
├────────────────┼──────────────────────┼──────────────────────┼────────┤
│ Voto Globale   │ 6.0/10               │ 6.5/10               │ +0.5   │
│ Architettura   │ 7/10                 │ 7.5/10               │ +0.5   │
│ Training       │ 5/10 (theory)        │ 6/10 (implemented)   │ +1.0   │
│ Features       │ 4/10 (no volume)     │ 7/10 (real volume!)  │ +3.0   │
│ Validation     │ 3/10 (basic split)   │ 3/10 (no WFV)        │ 0      │
│ Production     │ 2/10 (concept)       │ 4/10 (partial)       │ +2.0   │
│ Tests          │ 1/10 (none)          │ 2/10 (minimal)       │ +1.0   │
│ Security       │ N/A                  │ 3/10 (hardcoded!)    │ -      │
└────────────────┴──────────────────────┴──────────────────────┴────────┘

## STATO VOLUME

✅ **RISOLTO** - Implementato acquisizione volume reale via cforex
✅ **RISOLTO** - VolumeDataManager può gestire volume reale
✅ **RISOLTO** - Features volume ora utilizzabili (VP, VSA, MFI, CMF, VWAP, OBV)
🗑️ **OBSOLETO** - VolumeProxyEstimator (da eliminare completamente)

═══════════════════════════════════════════════════════════════════════════════
🔴 PROBLEMI CRITICI IDENTIFICATI
═══════════════════════════════════════════════════════════════════════════════

## 1. LOOK-AHEAD BIAS ⚠️ [SEVERITY: CRITICAL]

**Location**: `src/forex_diffusion/training/train_sklearn.py`
**Impact**: Invalida completamente tutti i backtest

**Current Implementation (WRONG)**:
```python
# train_sklearn.py - Line ~150
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)  # BIAS: test stats leak!
```

**Problem**: Lo StandardScaler calcola mean/std su train+val, ma poi usa queste 
statistiche su test set. Questo introduce info dal futuro nei dati di test.

**Fix Required**:
```python
# CORRECT implementation
class TimeSeriesSplitScaler:
    def __init__(self):
        self.scalers = {}
    
    def fit_transform_by_split(self, X, split_indices):
        """Scala ogni split SOLO con le sue statistiche"""
        scaled_data = []
        for split_idx, (train_idx, test_idx) in enumerate(split_indices):
            scaler = StandardScaler()
            # Fit SOLO su train di questo split
            scaler.fit(X[train_idx])
            # Transform train e test separatamente
            scaled_train = scaler.transform(X[train_idx])
            scaled_test = scaler.transform(X[test_idx])
            scaled_data.append((scaled_train, scaled_test, scaler))
            self.scalers[split_idx] = scaler
        return scaled_data
```

**Impact se non fixato**: 
- Accuracy inflated di 5-15%
- Sharpe ratio sopravvalutato
- Performance live diverge completamente da backtest

---

## 2. NO WALK-FORWARD VALIDATION ⚠️ [SEVERITY: CRITICAL]

**Current**: Simple train/val/test split (60/20/20)
**Problem**: Overfitting mascherato, performance non realistic in live

**Fix Required**: Implementare WFV completo

```python
# NEW FILE: src/forex_diffusion/validation/walk_forward.py
from typing import List, Tuple
import pandas as pd
from sklearn.base import BaseEstimator

class WalkForwardValidator:
    """
    Walk-Forward Validation per time series trading
    
    Example:
        Data: 2020-01-01 to 2024-12-31 (5 years)
        Window: 12 mesi training, 3 mesi testing
        Step: 3 mesi (no overlap tra test sets)
        
        Split 1: Train [2020-01 to 2020-12], Test [2021-01 to 2021-03]
        Split 2: Train [2020-04 to 2021-03], Test [2021-04 to 2021-06]
        Split 3: Train [2020-07 to 2021-06], Test [2021-07 to 2021-09]
        ...
    """
    
    def __init__(
        self,
        train_period_months: int = 12,
        test_period_months: int = 3,
        step_months: int = 3
    ):
        self.train_period = pd.DateOffset(months=train_period_months)
        self.test_period = pd.DateOffset(months=test_period_months)
        self.step = pd.DateOffset(months=step_months)
    
    def split(
        self, 
        df: pd.DataFrame,
        date_column: str = 'timestamp'
    ) -> List[Tuple[pd.Index, pd.Index]]:
        """
        Generate walk-forward splits
        
        Returns:
            List of (train_indices, test_indices) tuples
        """
        splits = []
        dates = pd.to_datetime(df[date_column])
        start_date = dates.min()
        end_date = dates.max()
        
        current_train_start = start_date
        
        while True:
            train_end = current_train_start + self.train_period
            test_start = train_end
            test_end = test_start + self.test_period
            
            # Stop se test_end supera dati disponibili
            if test_end > end_date:
                break
            
            # Indici per questo split
            train_mask = (dates >= current_train_start) & (dates < train_end)
            test_mask = (dates >= test_start) & (dates < test_end)
            
            train_idx = df.index[train_mask]
            test_idx = df.index[test_mask]
            
            splits.append((train_idx, test_idx))
            
            # Move window forward
            current_train_start += self.step
        
        return splits
    
    def validate(
        self,
        model: BaseEstimator,
        X: pd.DataFrame,
        y: pd.Series,
        date_column: str = 'timestamp'
    ) -> dict:
        """
        Run full walk-forward validation
        
        Returns:
            {
                'splits': [...],  # Results per split
                'aggregate': {...}  # Overall metrics
            }
        """
        splits = self.split(X, date_column)
        results = []
        
        for i, (train_idx, test_idx) in enumerate(splits):
            X_train, y_train = X.loc[train_idx], y.loc[train_idx]
            X_test, y_test = X.loc[test_idx], y.loc[test_idx]
            
            # Scale SOLO su train di questo split (fix look-ahead bias)
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            # Train model
            model.fit(X_train_scaled, y_train)
            
            # Evaluate
            y_pred = model.predict(X_test_scaled)
            
            split_result = {
                'split_idx': i,
                'train_start': X_train.index[0],
                'train_end': X_train.index[-1],
                'test_start': X_test.index[0],
                'test_end': X_test.index[-1],
                'train_samples': len(X_train),
                'test_samples': len(X_test),
                'accuracy': accuracy_score(y_test, y_pred),
                'precision': precision_score(y_test, y_pred, average='weighted'),
                'recall': recall_score(y_test, y_pred, average='weighted'),
                'f1': f1_score(y_test, y_pred, average='weighted')
            }
            results.append(split_result)
        
        # Aggregate metrics
        aggregate = {
            'mean_accuracy': np.mean([r['accuracy'] for r in results]),
            'std_accuracy': np.std([r['accuracy'] for r in results]),
            'mean_f1': np.mean([r['f1'] for r in results]),
            'std_f1': np.std([r['f1'] for r in results]),
            'total_splits': len(results)
        }
        
        return {'splits': results, 'aggregate': aggregate}
```

**Integration Required**:
```python
# In train_sklearn.py
from forex_diffusion.validation.walk_forward import WalkForwardValidator

# Replace current validation with:
wfv = WalkForwardValidator(train_period_months=12, test_period_months=3)
results = wfv.validate(model, X, y, date_column='timestamp')

# Log results
logger.info(f"Walk-Forward Validation Results:")
logger.info(f"  Mean Accuracy: {results['aggregate']['mean_accuracy']:.4f} ± {results['aggregate']['std_accuracy']:.4f}")
logger.info(f"  Total Splits: {results['aggregate']['total_splits']}")
```

---

## 3. FEATURE LOSS BUG ⚠️ [SEVERITY: HIGH]

**Location**: `src/forex_diffusion/db_adapter.py:322`
**Impact**: Perdita di features hrel, lrel, crel + tutte le volume-weighted variants

**Current (WRONG)**:
```python
# db_adapter.py - Line 322
feature_cols = [col for col in feature_cols if col in loaded_df.columns]
# SILENTLY drops missing columns!
```

**Fix Required**:
```python
# db_adapter.py - Line 322
missing_cols = [col for col in feature_cols if col not in loaded_df.columns]
if missing_cols:
    logger.error(f"CRITICAL: Missing features in DB: {missing_cols}")
    logger.error(f"Expected columns: {feature_cols}")
    logger.error(f"Available columns: {list(loaded_df.columns)}")
    raise ValueError(f"Missing {len(missing_cols)} required features: {missing_cols[:10]}...")

# Se arrivi qui, tutte le features esistono
feature_data = loaded_df[feature_cols]
```

**Root Cause**: Le features hrel, lrel, crel + volume variants NON sono mai state 
salvate nel database perché:
1. FeatureEngineer le calcola
2. Ma train_sklearn.py NON le passa a save_to_db()
3. Quindi vanno perse

**Fix Complete**:
```python
# In train_sklearn.py, dopo feature computation:
all_feature_cols = feature_cols + [
    'hrel', 'lrel', 'crel',  # Relative features
    'hlc_vwap', 'volume_imbalance', 'volume_momentum',  # Volume-weighted
    # ... tutte le features calcolate da FeatureEngineer
]

# Save to DB con TUTTE le features
db_adapter.save_to_db(
    df=feature_df[all_feature_cols + ['target']],
    table_name=f"features_{symbol}_{interval}",
    if_exists='replace'
)
```

---

## 4. SECURITY ISSUES ⚠️ [SEVERITY: MEDIUM]

**Problem**: Hardcoded paths, API keys in code, no .env

**Locations**:
- `src/forex_diffusion/db_adapter.py`: `DB_PATH = "D:/Projects/ForexGPT/data/forex.db"`
- `src/forex_diffusion/data_providers/cforex_provider.py`: `API_KEY = "..."`
- Various files: absolute paths

**Fix Required**:
```python
# NEW FILE: .env (add to .gitignore!)
DB_PATH=D:/Projects/ForexGPT/data/forex.db
CFOREX_API_KEY=your_api_key_here
LOG_DIR=D:/Projects/ForexGPT/logs

# NEW FILE: src/forex_diffusion/config.py
from pathlib import Path
from dotenv import load_dotenv
import os

load_dotenv()

class Config:
    # Paths
    PROJECT_ROOT = Path(__file__).parent.parent.parent
    DB_PATH = os.getenv('DB_PATH', str(PROJECT_ROOT / 'data' / 'forex.db'))
    LOG_DIR = Path(os.getenv('LOG_DIR', str(PROJECT_ROOT / 'logs')))
    
    # API Keys
    CFOREX_API_KEY = os.getenv('CFOREX_API_KEY')
    
    @classmethod
    def validate(cls):
        """Validate all required configs are set"""
        if not cls.CFOREX_API_KEY:
            raise ValueError("CFOREX_API_KEY not set in .env")
        if not Path(cls.DB_PATH).parent.exists():
            Path(cls.DB_PATH).parent.mkdir(parents=True)

# Usage in other files:
from forex_diffusion.config import Config
db_adapter = DatabaseAdapter(db_path=Config.DB_PATH)
```

---

## 5. INCOMPLETE CACHING ⚠️ [SEVERITY: MEDIUM]

**Current**: Solo feature caching, ma non per tutte le operazioni costose

**Fix Required**: Comprehensive caching strategy

```python
# NEW FILE: src/forex_diffusion/utils/cache.py
from functools import lru_cache, wraps
import hashlib
import pickle
from pathlib import Path
from typing import Any, Callable

class DiskCache:
    """Persistent disk cache for expensive computations"""
    
    def __init__(self, cache_dir: Path):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
    
    def _make_key(self, func_name: str, args, kwargs) -> str:
        """Create unique cache key from function and arguments"""
        key_data = (func_name, args, tuple(sorted(kwargs.items())))
        key_str = str(key_data)
        return hashlib.md5(key_str.encode()).hexdigest()
    
    def get(self, key: str) -> Any:
        cache_file = self.cache_dir / f"{key}.pkl"
        if cache_file.exists():
            with open(cache_file, 'rb') as f:
                return pickle.load(f)
        return None
    
    def set(self, key: str, value: Any):
        cache_file = self.cache_dir / f"{key}.pkl"
        with open(cache_file, 'wb') as f:
            pickle.dump(value, f)
    
    def cache(self, func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs):
            key = self._make_key(func.__name__, args, kwargs)
            cached = self.get(key)
            if cached is not None:
                logger.debug(f"Cache hit: {func.__name__}")
                return cached
            
            result = func(*args, **kwargs)
            self.set(key, result)
            return result
        return wrapper

# Usage:
cache = DiskCache(Config.PROJECT_ROOT / '.cache')

@cache.cache
def compute_expensive_features(df, symbol, interval):
    # ... expensive computation
    return features

# Apply to:
# - Feature computation (current)
# - Volume Profile calculation (NEW)
# - VSA pattern detection (NEW)
# - Indicator calculation (NEW)
```

═══════════════════════════════════════════════════════════════════════════════
✅ COSA È STATO FATTO BENE (da mantenere)
═══════════════════════════════════════════════════════════════════════════════

1. ✅ **Volume Reale Implementato** - cforex provider funziona
2. ✅ **Database Architecture** - SQLite ben strutturato
3. ✅ **Modular Design** - Separazione data_providers, training, features
4. ✅ **UI Exists** - Training tab funzionale
5. ✅ **Feature Engineering** - 40+ features volume (VP, VSA, MFI, CMF, VWAP, OBV)
6. ✅ **Multiple Models** - RandomForest, GradientBoosting, XGBoost support
7. ✅ **Logging** - structured logging presente

═══════════════════════════════════════════════════════════════════════════════
🗑️ CODICE OBSOLETO DA ELIMINARE
═══════════════════════════════════════════════════════════════════════════════

Ora che abbiamo volume reale, TUTTO il codice di stima proxy è obsoleto:

## Files da ELIMINARE completamente:
```
src/forex_diffusion/volume/
├── volume_proxy_estimator.py    # DELETE ENTIRE FILE
└── __init__.py                  # UPDATE imports
```

## Code da RIMUOVERE da file esistenti:

### volume_data_manager.py:
```python
# REMOVE Lines ~50-150:
if self.use_real_volume:
    # Real volume path (keep)
else:
    # REMOVE ENTIRE ELSE BLOCK (proxy estimation)
    self.proxy_estimator = VolumeProxyEstimator()  # DELETE
    proxy_volume = self.proxy_estimator.estimate()  # DELETE
```

### feature_engineer.py:
```python
# REMOVE any references to:
- tick_volume
- proxy_volume
- estimated_volume
- volume_quality_score

# KEEP only:
- real_volume
- volume-based features (VP, VSA, MFI, etc.)
```

## Perché eliminare invece di mantenere come fallback?

1. **Confusione**: Due code path aumentano complessità
2. **Manutenzione**: Code path non testato diventa buggy
3. **Misleading**: Proxy volume NON è affidabile, meglio fallire che dare dati wrong
4. **Clarity**: Una sola way of doing things = più chiaro

**Decision**: Se volume reale non disponibile → ERROR, non fallback a proxy

═══════════════════════════════════════════════════════════════════════════════
🚀 NUOVE FEATURES ABILITABILI (ora che abbiamo volume reale)
═══════════════════════════════════════════════════════════════════════════════

## 1. Volume Profile (Priority: HIGH)

**What**: Distribution of volume at each price level

```python
# NEW FILE: src/forex_diffusion/features/volume_profile.py
import pandas as pd
import numpy as np
from typing import Dict, Tuple

class VolumeProfile:
    """
    Volume Profile Analysis
    
    Identifies:
    - POC (Point of Control): Price level with highest volume
    - VAH (Value Area High): Top of 70% volume area
    - VAL (Value Area Low): Bottom of 70% volume area
    - HVN (High Volume Nodes): Local volume maxima
    - LVN (Low Volume Nodes): Local volume minima
    """
    
    def __init__(self, num_bins: int = 50):
        self.num_bins = num_bins
    
    def calculate(
        self,
        df: pd.DataFrame,
        lookback_periods: int = 100
    ) -> pd.DataFrame:
        """
        Calculate volume profile for each candle using lookback window
        
        Args:
            df: DataFrame with columns ['high', 'low', 'close', 'volume']
            lookback_periods: Number of periods to look back
        
        Returns:
            DataFrame with new columns:
            - poc_distance: Distance from current price to POC (%)
            - vah_distance: Distance to Value Area High (%)
            - val_distance: Distance to Value Area Low (%)
            - in_value_area: Boolean, is price in 70% value area
            - hvn_nearby: Boolean, is there HVN within 0.1%
            - lvn_nearby: Boolean, is there LVN within 0.1%
        """
        results = []
        
        for i in range(lookback_periods, len(df)):
            window = df.iloc[i-lookback_periods:i]
            current_price = df.iloc[i]['close']
            
            # Create price levels
            price_min = window['low'].min()
            price_max = window['high'].max()
            price_bins = np.linspace(price_min, price_max, self.num_bins)
            
            # Aggregate volume at each price level
            volume_at_price = np.zeros(self.num_bins)
            
            for _, row in window.iterrows():
                # Each candle contributes volume proportionally to price levels it touched
                candle_bins = np.digitize([row['low'], row['high']], price_bins)
                start_bin = max(0, candle_bins[0] - 1)
                end_bin = min(self.num_bins - 1, candle_bins[1])
                
                # Distribute volume across touched bins
                bins_touched = end_bin - start_bin + 1
                volume_per_bin = row['volume'] / bins_touched
                volume_at_price[start_bin:end_bin+1] += volume_per_bin
            
            # Find POC
            poc_idx = np.argmax(volume_at_price)
            poc_price = price_bins[poc_idx]
            
            # Find Value Area (70% of volume)
            total_volume = volume_at_price.sum()
            target_volume = total_volume * 0.7
            
            # Start from POC and expand until we have 70% volume
            sorted_indices = np.argsort(volume_at_price)[::-1]
            cumsum = 0
            value_area_indices = []
            
            for idx in sorted_indices:
                cumsum += volume_at_price[idx]
                value_area_indices.append(idx)
                if cumsum >= target_volume:
                    break
            
            vah_price = price_bins[max(value_area_indices)]
            val_price = price_bins[min(value_area_indices)]
            
            # Identify HVN and LVN (local extrema)
            from scipy.signal import argrelextrema
            hvn_indices = argrelextrema(volume_at_price, np.greater, order=3)[0]
            lvn_indices = argrelextrema(volume_at_price, np.less, order=3)[0]
            
            hvn_prices = price_bins[hvn_indices]
            lvn_prices = price_bins[lvn_indices]
            
            # Calculate features
            poc_distance = (current_price - poc_price) / poc_price * 100
            vah_distance = (current_price - vah_price) / vah_price * 100
            val_distance = (current_price - val_price) / val_price * 100
            in_value_area = val_price <= current_price <= vah_price
            
            # Check if HVN/LVN nearby (within 0.1%)
            hvn_nearby = any(abs(current_price - p) / p < 0.001 for p in hvn_prices)
            lvn_nearby = any(abs(current_price - p) / p < 0.001 for p in lvn_prices)
            
            results.append({
                'poc_distance': poc_distance,
                'vah_distance': vah_distance,
                'val_distance': val_distance,
                'in_value_area': int(in_value_area),
                'hvn_nearby': int(hvn_nearby),
                'lvn_nearby': int(lvn_nearby)
            })
        
        # Pad beginning with NaN
        for _ in range(lookback_periods):
            results.insert(0, {k: np.nan for k in results[0].keys()})
        
        return pd.DataFrame(results)
```

**Integration**:
```python
# In feature_engineer.py
from forex_diffusion.features.volume_profile import VolumeProfile

def compute_volume_profile_features(self, df):
    vp = VolumeProfile(num_bins=50)
    vp_features = vp.calculate(df, lookback_periods=100)
    return df.join(vp_features)
```

---

## 2. VSA (Volume Spread Analysis) Patterns

**What**: Identify accumulation/distribution phases using volume + price spread

```python
# NEW FILE: src/forex_diffusion/features/vsa.py
import pandas as pd
import numpy as np

class VSAPatternDetector:
    """
    Volume Spread Analysis Pattern Detection
    
    Patterns:
    - Accumulation: High volume + narrow spread + up close
    - Distribution: High volume + narrow spread + down close
    - Climax: Very high volume + wide spread
    - No Demand: Low volume + narrow spread + down close
    - No Supply: Low volume + narrow spread + up close
    """
    
    def detect_patterns(self, df: pd.DataFrame, lookback: int = 20) -> pd.DataFrame:
        """
        Detect VSA patterns
        
        Returns DataFrame with columns:
        - accumulation: 1 if pattern detected, 0 otherwise
        - distribution: 1 if pattern detected, 0 otherwise
        - buying_climax: 1 if pattern detected, 0 otherwise
        - selling_climax: 1 if pattern detected, 0 otherwise
        - no_demand: 1 if pattern detected, 0 otherwise
        - no_supply: 1 if pattern detected, 0 otherwise
        """
        # Calculate spread and relative position of close
        df = df.copy()
        df['spread'] = df['high'] - df['low']
        df['close_position'] = (df['close'] - df['low']) / df['spread']  # 0-1
        
        # Rolling statistics
        df['volume_ma'] = df['volume'].rolling(lookback).mean()
        df['volume_std'] = df['volume'].rolling(lookback).std()
        df['spread_ma'] = df['spread'].rolling(lookback).mean()
        df['spread_std'] = df['spread'].rolling(lookback).std()
        
        # Classify volume and spread
        df['high_volume'] = df['volume'] > (df['volume_ma'] + df['volume_std'])
        df['low_volume'] = df['volume'] < (df['volume_ma'] - 0.5 * df['volume_std'])
        df['wide_spread'] = df['spread'] > (df['spread_ma'] + df['spread_std'])
        df['narrow_spread'] = df['spread'] < df['spread_ma']
        
        # Up/down close
        df['up_close'] = df['close_position'] > 0.6
        df['down_close'] = df['close_position'] < 0.4
        
        # Pattern detection
        patterns = pd.DataFrame(index=df.index)
        
        # Accumulation: High volume + narrow spread + up close
        patterns['accumulation'] = (
            df['high_volume'] & df['narrow_spread'] & df['up_close']
        ).astype(int)
        
        # Distribution: High volume + narrow spread + down close
        patterns['distribution'] = (
            df['high_volume'] & df['narrow_spread'] & df['down_close']
        ).astype(int)
        
        # Buying Climax: Very high volume + wide spread + up close
        patterns['buying_climax'] = (
            (df['volume'] > df['volume_ma'] + 2*df['volume_std']) & 
            df['wide_spread'] & 
            df['up_close']
        ).astype(int)
        
        # Selling Climax: Very high volume + wide spread + down close
        patterns['selling_climax'] = (
            (df['volume'] > df['volume_ma'] + 2*df['volume_std']) & 
            df['wide_spread'] & 
            df['down_close']
        ).astype(int)
        
        # No Demand: Low volume + narrow spread + down close
        patterns['no_demand'] = (
            df['low_volume'] & df['narrow_spread'] & df['down_close']
        ).astype(int)
        
        # No Supply: Low volume + narrow spread + up close
        patterns['no_supply'] = (
            df['low_volume'] & df['narrow_spread'] & df['up_close']
        ).astype(int)
        
        return patterns

# Usage in feature_engineer.py:
def compute_vsa_features(self, df):
    vsa = VSAPatternDetector()
    vsa_patterns = vsa.detect_patterns(df, lookback=20)
    return df.join(vsa_patterns)
```

---

## 3. Smart Money Detection

**What**: Identify institutional order flow using volume characteristics

```python
# NEW FILE: src/forex_diffusion/features/smart_money.py
import pandas as pd
import numpy as np

class SmartMoneyDetector:
    """
    Detect smart money (institutional) activity
    
    Signals:
    - Large unusual volume
    - Volume spike with small price movement (absorption)
    - Volume dry-up before reversal
    - Climatic volume at extremes
    """
    
    def detect(self, df: pd.DataFrame, lookback: int = 50) -> pd.DataFrame:
        df = df.copy()
        
        # Volume characteristics
        df['volume_ma'] = df['volume'].rolling(lookback).mean()
        df['volume_std'] = df['volume'].rolling(lookback).std()
        df['volume_zscore'] = (df['volume'] - df['volume_ma']) / df['volume_std']
        
        # Price movement
        df['price_change'] = df['close'].pct_change()
        df['price_change_abs'] = df['price_change'].abs()
        
        # Calculate volume/price efficiency
        df['volume_price_ratio'] = df['volume'] / (df['price_change_abs'] + 1e-10)
        
        features = pd.DataFrame(index=df.index)
        
        # Unusual volume (>2 std devs)
        features['unusual_volume'] = (df['volume_zscore'] > 2).astype(int)
        
        # Absorption: High volume + small price change
        features['absorption'] = (
            (df['volume_zscore'] > 1.5) & 
            (df['price_change_abs'] < df['price_change_abs'].rolling(lookback).quantile(0.3))
        ).astype(int)
        
        # Volume dry-up: Volume drops below 0.5 std dev
        features['volume_dryup'] = (df['volume_zscore'] < -0.5).astype(int)
        
        # Climatic volume at price extremes
        df['at_high'] = df['close'] > df['close'].rolling(lookback).quantile(0.9)
        df['at_low'] = df['close'] < df['close'].rolling(lookback).quantile(0.1)
        
        features['climax_at_high'] = (
            (df['volume_zscore'] > 2) & df['at_high']
        ).astype(int)
        
        features['climax_at_low'] = (
            (df['volume_zscore'] > 2) & df['at_low']
        ).astype(int)
        
        # Smart money score (aggregate)
        features['smart_money_score'] = (
            features['unusual_volume'] * 1 +
            features['absorption'] * 2 +
            features['climax_at_high'] * 3 +
            features['climax_at_low'] * 3
        )
        
        return features
```

═══════════════════════════════════════════════════════════════════════════════
📈 PERFORMANCE EXPECTATIONS (REALISTIC)
═══════════════════════════════════════════════════════════════════════════════

┌─────────────────────────┬──────────┬────────┬────────┬────────┐
│ Phase                   │ Accuracy │  RMSE  │ Sharpe │  Voto  │
├─────────────────────────┼──────────┼────────┼────────┼────────┤
│ Baseline (attuale)      │ 48-52%   │ 0.0042 │  0.9   │ 6.5/10 │
│ Post fix critici        │ 58-65%   │ 0.0035 │  1.4   │ 8.0/10 │
│ Con volume features     │ 65-72%   │ 0.0030 │  1.9   │ 9.0/10 │
│ Perfect (oltre scope)   │ >75%     │<0.0028 │ >2.5   │ 10/10  │
└─────────────────────────┴──────────┴────────┴────────┴────────┘

**Notes**:
- "Baseline attuale" = con look-ahead bias + no WFV (performance inflated!)
- "Post fix critici" = realistic dopo fix bias + WFV
- "Con volume features" = dopo integrazione VP, VSA, Smart Money
- 10/10 richiede: ensemble models, online learning, alternative data

**Why accuracy caps at ~72%?**
1. Forex è intrinsecamente noisy (efficient markets)
2. High-frequency noise difficile da predire
3. Fundamentals non inclusi (news, sentiment)
4. Regime changes non gestiti

**Come arrivare a 9.0/10**:
1. Fix look-ahead bias ✅
2. Implement WFV ✅
3. Use real volume features ✅
4. Optimize hyperparameters
5. Feature selection (rimuovi noise)

═══════════════════════════════════════════════════════════════════════════════
🎯 ROADMAP TO 9.0/10
═══════════════════════════════════════════════════════════════════════════════

## PHASE 1: CRITICAL FIXES (Week 1-2) ⚠️

### ✅ Task 1.1: Fix Look-Ahead Bias
**File**: `src/forex_diffusion/training/train_sklearn.py`
**Changes**:
1. Implement TimeSeriesSplitScaler class (code provided above)
2. Replace StandardScaler usage with TimeSeriesSplitScaler
3. Ensure scaler fitted ONLY on train data per split
4. Add validation: assert test stats != train stats

**Test**:
```python
# After fix, verify:
scaler_train_mean = scaler.mean_
X_test_mean = X_test.mean(axis=0)
assert not np.allclose(scaler_train_mean, X_test_mean), "Look-ahead bias still present!"
```

---

### ✅ Task 1.2: Implement Walk-Forward Validation
**New File**: `src/forex_diffusion/validation/walk_forward.py`
**Changes**:
1. Create WalkForwardValidator class (code provided above)
2. Integrate into train_sklearn.py
3. Replace current train/val/test split with WFV
4. Log per-split results + aggregate metrics
5. Save WFV results to DB for analysis

**Config**:
- Train period: 12 months
- Test period: 3 months
- Step: 3 months (no overlap)

---

### ✅ Task 1.3: Fix Feature Loss Bug
**File**: `src/forex_diffusion/db_adapter.py`
**Changes**:
1. Line 322: Replace silent dropping with explicit error
2. Add feature completeness check before save
3. Update save_to_db() to include ALL computed features
4. Add unit test: assert all expected features in DB

**Verification**:
```python
# After save, query DB and verify:
expected_features = ['hrel', 'lrel', 'crel', 'hlc_vwap', ...]
db_features = db_adapter.get_feature_columns(symbol, interval)
missing = set(expected_features) - set(db_features)
assert len(missing) == 0, f"Missing features in DB: {missing}"
```

---

### ✅ Task 1.4: Security Fixes
**New Files**: 
- `.env` (add to .gitignore)
- `src/forex_diffusion/config.py`

**Changes**:
1. Create Config class with environment variables
2. Replace all hardcoded paths with Config.PATH
3. Move API keys to .env
4. Add Config.validate() call at startup

**Files to update**:
- db_adapter.py
- cforex_provider.py
- train_sklearn.py
- Any file with hardcoded paths

---

### ✅ Task 1.5: Complete Caching System
**New File**: `src/forex_diffusion/utils/cache.py`
**Changes**:
1. Create DiskCache class (code provided above)
2. Apply @cache.cache decorator to:
   - Feature computation
   - Volume Profile calculation
   - VSA pattern detection
   - Indicator calculation
3. Add cache invalidation on data update
4. Add CLI command: `python -m forex_diffusion.cli cache clear`

**Expected Speedup**: 5-10× on re-runs with same data

---

## PHASE 2: VOLUME FEATURES (Week 3-4) 🚀

### ✅ Task 2.1: Delete Obsolete Code
**Files to DELETE**:
- `src/forex_diffusion/volume/volume_proxy_estimator.py`

**Files to MODIFY**:
- `volume_data_manager.py`: Remove proxy estimation code path
- `feature_engineer.py`: Remove proxy volume references

**Verification**: 
- `grep -r "proxy" src/` should return no results
- All tests should pass after deletion

---

### ✅ Task 2.2: Implement Volume Profile
**New File**: `src/forex_diffusion/features/volume_profile.py`
**Integration**: Add to feature_engineer.py

**Features Added** (6 new columns):
- poc_distance
- vah_distance
- val_distance
- in_value_area
- hvn_nearby
- lvn_nearby

**Test**: Generate sample data, verify POC/VAH/VAL calculation manually

---

### ✅ Task 2.3: Implement VSA Patterns
**New File**: `src/forex_diffusion/features/vsa.py`
**Integration**: Add to feature_engineer.py

**Features Added** (6 new columns):
- accumulation
- distribution
- buying_climax
- selling_climax
- no_demand
- no_supply

**Test**: Verify patterns on known market phases (trending vs ranging)

---

### ✅ Task 2.4: Implement Smart Money Detection
**New File**: `src/forex_diffusion/features/smart_money.py`
**Integration**: Add to feature_engineer.py

**Features Added** (6 new columns):
- unusual_volume
- absorption
- volume_dryup
- climax_at_high
- climax_at_low
- smart_money_score

---

### ✅ Task 2.5: Feature Persistence
**Changes**: `db_adapter.py` save_to_db()

Ensure ALL new features are saved:
```python
volume_features = [
    # Volume Profile
    'poc_distance', 'vah_distance', 'val_distance', 
    'in_value_area', 'hvn_nearby', 'lvn_nearby',
    # VSA
    'accumulation', 'distribution', 'buying_climax', 
    'selling_climax', 'no_demand', 'no_supply',
    # Smart Money
    'unusual_volume', 'absorption', 'volume_dryup',
    'climax_at_high', 'climax_at_low', 'smart_money_score'
]

all_features = base_features + volume_features
db_adapter.save_to_db(df[all_features], ...)
```

---

## PHASE 3: INTEGRATION & VALIDATION (Week 5-6) ✅

### ✅ Task 3.1: Integration Tests
**New File**: `tests/integration/test_full_pipeline.py`

```python
def test_full_training_pipeline():
    """Test entire pipeline: data -> features -> training -> validation"""
    # 1. Fetch data
    provider = CForexProvider()
    df = provider.get_historical_data('EURUSD', '1h', start_date=...)
    
    # 2. Compute features (with volume)
    engineer = FeatureEngineer()
    df_features = engineer.compute_all_features(df)
    
    # 3. Save to DB
    db_adapter.save_to_db(df_features, ...)
    
    # 4. Load from DB (verify no feature loss)
    df_loaded = db_adapter.load_features('EURUSD', '1h')
    assert set(df_features.columns) == set(df_loaded.columns)
    
    # 5. Train with WFV
    wfv = WalkForwardValidator()
    results = wfv.validate(model, X, y)
    
    # 6. Verify metrics in realistic range
    assert 0.55 <= results['aggregate']['mean_accuracy'] <= 0.75
    
    # 7. Verify no look-ahead bias
    # (test stats should differ from train stats)
```

---

### ✅ Task 3.2: Performance Benchmarking
**New File**: `scripts/benchmark.py`

Track metrics over time:
```python
# Run on same validation set each time
metrics = {
    'accuracy': ...,
    'precision': ...,
    'recall': ...,
    'f1': ...,
    'sharpe': ...,
    'max_drawdown': ...,
    'win_rate': ...,
    'avg_win': ...,
    'avg_loss': ...,
    'profit_factor': ...
}

# Save to benchmark.db
db.save_benchmark(version='v1.3.0', metrics=metrics, timestamp=now)
```

**Visualization**: Plot metrics over versions to track progress

---

### ✅ Task 3.3: Documentation
Update docs:
1. **README.md**: 
   - Add "Features" section listing all 40+ features
   - Update architecture diagram with volume flow
   - Add walk-forward validation explanation
   
2. **docs/FEATURES.md** (NEW):
   - Detailed description of each feature
   - Feature importance from model
   - How to add custom features
   
3. **docs/VALIDATION.md** (NEW):
   - Walk-forward validation methodology
   - How to interpret WFV results
   - Common pitfalls (look-ahead bias)
   
4. **docs/DEPLOYMENT.md** (NEW):
   - How to deploy model to production
   - Real-time inference
   - Monitoring & alerts

---

### ✅ Task 3.4: Hyperparameter Optimization
**New File**: `src/forex_diffusion/training/hyperopt.py`

Use Optuna for hyperparameter search:
```python
import optuna

def objective(trial):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
        'max_depth': trial.suggest_int('max_depth', 3, 20),
        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1, log=True),
        ...
    }
    
    model = XGBClassifier(**params)
    
    # Use WFV for evaluation
    wfv = WalkForwardValidator()
    results = wfv.validate(model, X, y)
    
    return results['aggregate']['mean_f1']  # Optimize F1

# Run optimization
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=100)

print(f"Best params: {study.best_params}")
print(f"Best F1: {study.best_value}")
```

**Expected Gain**: +2-5% accuracy from optimal hyperparams

---

## PHASE 4: PRODUCTION READINESS (Week 7-8) 🎬

### ✅ Task 4.1: Real-time Inference API
**New File**: `src/forex_diffusion/inference/api.py`

```python
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI()

class PredictionRequest(BaseModel):
    symbol: str
    interval: str
    timestamp: str

class PredictionResponse(BaseModel):
    symbol: str
    prediction: int  # 0=down, 1=up
    probability: float
    confidence: float  # Model's confidence
    features: dict  # Key features for interpretability

@app.post("/predict", response_model=PredictionResponse)
async def predict(request: PredictionRequest):
    # 1. Fetch latest data
    provider = CForexProvider()
    df = provider.get_latest_data(request.symbol, request.interval, periods=200)
    
    # 2. Compute features
    engineer = FeatureEngineer()
    features = engineer.compute_all_features(df).iloc[-1]
    
    # 3. Load model
    model = load_model(request.symbol, request.interval)
    
    # 4. Predict
    X = features[model.feature_names_]
    prediction = model.predict_proba([X])[0]
    
    return PredictionResponse(
        symbol=request.symbol,
        prediction=int(prediction[1] > 0.5),
        probability=float(prediction[1]),
        confidence=float(max(prediction)),
        features={k: float(v) for k, v in X.items()[:10]}  # Top 10 features
    )
```

**Deployment**: Docker container with FastAPI + model

---

### ✅ Task 4.2: Monitoring & Alerts
**New File**: `src/forex_diffusion/monitoring/monitor.py`

Track model performance in production:
```python
class ModelMonitor:
    def __init__(self):
        self.db = MonitoringDB()
    
    def log_prediction(self, symbol, prediction, actual=None):
        """Log each prediction"""
        self.db.insert({
            'timestamp': now,
            'symbol': symbol,
            'prediction': prediction,
            'actual': actual,  # filled later
            'accuracy': self._compute_accuracy()
        })
    
    def check_drift(self):
        """Detect concept drift"""
        recent_accuracy = self._get_recent_accuracy(days=7)
        baseline_accuracy = self._get_baseline_accuracy()
        
        if recent_accuracy < baseline_accuracy - 0.05:  # 5% drop
            self._send_alert("Model drift detected!")
    
    def check_data_quality(self, df):
        """Detect data issues"""
        # Check for missing values
        if df.isnull().any().any():
            self._send_alert("Missing data detected!")
        
        # Check for outliers
        z_scores = np.abs((df - df.mean()) / df.std())
        if (z_scores > 5).any().any():
            self._send_alert("Outliers detected!")
```

**Alerts**: Email/Slack when:
- Accuracy drops >5% from baseline
- Data quality issues
- High latency (>1s for prediction)

---

### ✅ Task 4.3: Backtesting Framework
**New File**: `src/forex_diffusion/backtest/engine.py`

```python
class Backtester:
    """
    Realistic backtesting with:
    - Transaction costs
    - Slippage
    - Position sizing
    - Risk management
    """
    
    def __init__(
        self,
        initial_capital: float = 10000,
        commission: float = 0.0001,  # 1 pip
        slippage: float = 0.0001,  # 1 pip
        max_position_size: float = 0.1  # 10% of capital per trade
    ):
        self.initial_capital = initial_capital
        self.commission = commission
        self.slippage = slippage
        self.max_position_size = max_position_size
    
    def run(self, predictions, prices, timestamps):
        """
        Run backtest
        
        Args:
            predictions: Array of predictions (0/1)
            prices: Array of prices
            timestamps: Array of timestamps
        
        Returns:
            {
                'total_return': ...,
                'sharpe_ratio': ...,
                'max_drawdown': ...,
                'win_rate': ...,
                'profit_factor': ...,
                'trades': [...]  # List of all trades
            }
        """
        capital = self.initial_capital
        position = 0  # 1=long, -1=short, 0=flat
        trades = []
        equity_curve = [capital]
        
        for i in range(1, len(predictions)):
            # Current prediction
            pred = predictions[i]
            price = prices[i]
            prev_price = prices[i-1]
            
            # Entry logic
            if position == 0:
                if pred == 1:  # Buy signal
                    position = 1
                    entry_price = price * (1 + self.slippage)
                    position_size = min(
                        capital * self.max_position_size,
                        capital
                    )
                    shares = position_size / entry_price
                    cost = position_size * self.commission
                    capital -= (position_size + cost)
                    
                    trades.append({
                        'timestamp': timestamps[i],
                        'action': 'BUY',
                        'price': entry_price,
                        'shares': shares,
                        'cost': cost
                    })
                
                elif pred == 0:  # Sell signal
                    position = -1
                    entry_price = price * (1 - self.slippage)
                    # Similar logic for short
            
            # Exit logic (if position open and prediction changes)
            elif position != 0 and pred != (position == 1):
                # Close position
                exit_price = price * (1 - self.slippage if position == 1 else 1 + self.slippage)
                pnl = (exit_price - entry_price) * shares * position
                cost = exit_price * shares * self.commission
                capital += (exit_price * shares - cost)
                position = 0
                
                trades.append({
                    'timestamp': timestamps[i],
                    'action': 'SELL' if position == 1 else 'COVER',
                    'price': exit_price,
                    'shares': shares,
                    'pnl': pnl,
                    'cost': cost
                })
            
            equity_curve.append(capital + (position * shares * price if position != 0 else 0))
        
        # Calculate metrics
        returns = np.diff(equity_curve) / equity_curve[:-1]
        sharpe = np.mean(returns) / np.std(returns) * np.sqrt(252)  # Annualized
        
        max_dd = self._calculate_max_drawdown(equity_curve)
        
        winning_trades = [t for t in trades if t.get('pnl', 0) > 0]
        losing_trades = [t for t in trades if t.get('pnl', 0) < 0]
        
        win_rate = len(winning_trades) / len([t for t in trades if 'pnl' in t])
        
        avg_win = np.mean([t['pnl'] for t in winning_trades]) if winning_trades else 0
        avg_loss = abs(np.mean([t['pnl'] for t in losing_trades])) if losing_trades else 0
        profit_factor = (avg_win * len(winning_trades)) / (avg_loss * len(losing_trades)) if losing_trades else np.inf
        
        return {
            'initial_capital': self.initial_capital,
            'final_capital': equity_curve[-1],
            'total_return': (equity_curve[-1] - self.initial_capital) / self.initial_capital,
            'sharpe_ratio': sharpe,
            'max_drawdown': max_dd,
            'win_rate': win_rate,
            'avg_win': avg_win,
            'avg_loss': avg_loss,
            'profit_factor': profit_factor,
            'num_trades': len([t for t in trades if 'pnl' in t]),
            'equity_curve': equity_curve,
            'trades': trades
        }
```

**Usage**:
```python
# After WFV, collect all test predictions
all_predictions = []
all_prices = []
all_timestamps = []

for split_result in wfv_results['splits']:
    all_predictions.extend(split_result['predictions'])
    all_prices.extend(split_result['prices'])
    all_timestamps.extend(split_result['timestamps'])

# Run backtest
backtester = Backtester(initial_capital=10000)
bt_results = backtester.run(all_predictions, all_prices, all_timestamps)

print(f"Total Return: {bt_results['total_return']:.2%}")
print(f"Sharpe Ratio: {bt_results['sharpe_ratio']:.2f}")
print(f"Max Drawdown: {bt_results['max_drawdown']:.2%}")
print(f"Win Rate: {bt_results['win_rate']:.2%}")
```

---

### ✅ Task 4.4: Risk Management
**New File**: `src/forex_diffusion/risk/manager.py`

```python
class RiskManager:
    """
    Manage trading risk:
    - Position sizing
    - Stop loss
    - Take profit
    - Max daily loss
    - Max drawdown
    """
    
    def __init__(
        self,
        max_position_pct: float = 0.1,  # 10% per trade
        stop_loss_pct: float = 0.02,  # 2% stop loss
        take_profit_pct: float = 0.04,  # 4% take profit (2:1 R:R)
        max_daily_loss_pct: float = 0.05,  # 5% max daily loss
        max_drawdown_pct: float = 0.15  # 15% max drawdown
    ):
        self.max_position_pct = max_position_pct
        self.stop_loss_pct = stop_loss_pct
        self.take_profit_pct = take_profit_pct
        self.max_daily_loss_pct = max_daily_loss_pct
        self.max_drawdown_pct = max_drawdown_pct
        
        self.daily_pnl = 0
        self.peak_equity = 0
    
    def calculate_position_size(
        self,
        capital: float,
        entry_price: float,
        prediction_confidence: float
    ) -> float:
        """
        Calculate position size based on Kelly Criterion + confidence
        
        Kelly = (p * b - q) / b
        where:
        - p = win probability (prediction confidence)
        - q = loss probability (1 - p)
        - b = win/loss ratio (take_profit / stop_loss)
        """
        p = prediction_confidence
        q = 1 - p
        b = self.take_profit_pct / self.stop_loss_pct  # 2:1 R:R
        
        kelly = (p * b - q) / b
        
        # Use fractional Kelly (0.5 * kelly) for safety
        kelly_fraction = 0.5
        position_pct = kelly * kelly_fraction
        
        # Cap at max_position_pct
        position_pct = min(position_pct, self.max_position_pct)
        
        # Don't trade if Kelly is negative (edge is negative)
        if position_pct <= 0:
            return 0
        
        position_size = capital * position_pct
        
        return position_size
    
    def check_risk_limits(
        self,
        current_equity: float,
        entry_equity: float
    ) -> bool:
        """Check if trade violates risk limits"""
        # Daily loss check
        daily_loss = (current_equity - entry_equity) / entry_equity
        if daily_loss < -self.max_daily_loss_pct:
            logger.warning("Max daily loss reached!")
            return False
        
        # Max drawdown check
        if current_equity > self.peak_equity:
            self.peak_equity = current_equity
        
        drawdown = (self.peak_equity - current_equity) / self.peak_equity
        if drawdown > self.max_drawdown_pct:
            logger.warning("Max drawdown reached!")
            return False
        
        return True
```

═══════════════════════════════════════════════════════════════════════════════
🎯 PROMPT PER CLAUDE CODE
═══════════════════════════════════════════════════════════════════════════════

```
# MISSION BRIEFING FOR CLAUDE CODE

You are tasked with upgrading the ForexGPT trading system from 6.5/10 to 9.0/10.

## PROJECT CONTEXT

ForexGPT is a machine learning system for forex price prediction. It recently 
integrated real volume data via cforex provider, but has several critical issues:

1. ⚠️ Look-ahead bias in data preprocessing (invalidates all backtests)
2. ⚠️ No walk-forward validation (overfitting masked)
3. ⚠️ Feature loss bug (missing hrel, lrel, crel + volume features)
4. ⚠️ Security issues (hardcoded paths, no .env)
5. ⚠️ Incomplete caching (slow reruns)

Additionally, now that real volume is available, we can delete all proxy volume 
estimation code and implement advanced volume features (Volume Profile, VSA, 
Smart Money detection).

## YOUR OBJECTIVES

### PRIORITY 1: Fix Critical Bugs (Week 1-2)
✅ Task 1.1: Fix look-ahead bias in train_sklearn.py
✅ Task 1.2: Implement walk-forward validation
✅ Task 1.3: Fix feature loss bug in db_adapter.py
✅ Task 1.4: Security fixes (.env + Config class)
✅ Task 1.5: Complete caching system

### PRIORITY 2: Advanced Volume Features (Week 3-4)
✅ Task 2.1: Delete obsolete proxy volume code
✅ Task 2.2: Implement Volume Profile analysis
✅ Task 2.3: Implement VSA pattern detection
✅ Task 2.4: Implement Smart Money detection
✅ Task 2.5: Ensure all features persisted to DB

### PRIORITY 3: Integration & Testing (Week 5-6)
✅ Task 3.1: Integration tests for full pipeline
✅ Task 3.2: Performance benchmarking system
✅ Task 3.3: Documentation update
✅ Task 3.4: Hyperparameter optimization with Optuna

### PRIORITY 4: Production (Week 7-8, Optional)
✅ Task 4.1: Real-time inference API (FastAPI)
✅ Task 4.2: Monitoring & alerting
✅ Task 4.3: Backtesting framework
✅ Task 4.4: Risk management system

## PROJECT STRUCTURE

```
ForexGPT/
├── src/forex_diffusion/
│   ├── training/
│   │   ├── train_sklearn.py       # MODIFY: Fix look-ahead bias
│   │   └── hyperopt.py            # NEW: Hyperparameter optimization
│   ├── validation/
│   │   └── walk_forward.py        # NEW: Walk-forward validation
│   ├── features/
│   │   ├── volume_profile.py      # NEW: Volume Profile
│   │   ├── vsa.py                 # NEW: VSA patterns
│   │   └── smart_money.py         # NEW: Smart Money detection
│   ├── db_adapter.py              # MODIFY: Fix feature loss
│   ├── config.py                  # NEW: Configuration management
│   ├── utils/
│   │   └── cache.py               # NEW: Caching system
│   ├── inference/
│   │   └── api.py                 # NEW: REST API
│   ├── monitoring/
│   │   └── monitor.py             # NEW: Model monitoring
│   ├── backtest/
│   │   └── engine.py              # NEW: Backtesting
│   └── risk/
│       └── manager.py             # NEW: Risk management
├── tests/
│   ├── unit/
│   └── integration/
│       └── test_full_pipeline.py  # NEW: Integration tests
├── .env                            # NEW: Environment variables
└── .gitignore                      # MODIFY: Add .env
```

## IMPLEMENTATION GUIDELINES

### 1. Read This File First
Before starting, read the entire CLAUDE_CODE_COMPLETE_IMPLEMENTATION.txt file 
(this file) carefully. It contains:
- Complete code for all new classes
- Detailed explanations of each bug
- Integration instructions
- Test cases
- Expected performance metrics

### 2. Work Incrementally
Don't try to implement everything at once. Follow the priority order:
1. Fix one bug at a time
2. Test after each fix
3. Commit with descriptive message
4. Move to next task

### 3. Testing Strategy
After each task:
```bash
# Run unit tests
pytest tests/unit/

# Run integration test
pytest tests/integration/test_full_pipeline.py

# Manual verification
python -m forex_diffusion.training.train_sklearn --symbol EURUSD --interval 1h
```

### 4. Validation Criteria
Your implementation is successful when:
- ✅ All tests pass
- ✅ No hardcoded paths (all use Config)
- ✅ Walk-forward validation shows realistic accuracy (55-70%)
- ✅ No look-ahead bias (verified by statistical test)
- ✅ All 40+ features present in DB
- ✅ Caching provides 5-10× speedup

### 5. Code Quality Standards
- Type hints everywhere
- Docstrings for all public methods (Google style)
- Error handling with proper logging
- No silent failures
- Performance: <100ms for feature computation (with cache)

## DETAILED TASK BREAKDOWN

[See sections above for complete implementation of each task]

## PERFORMANCE TARGETS

After all fixes, expect:
- Accuracy: 58-65% (realistic with WFV)
- Sharpe: 1.4+
- Max Drawdown: <15%
- Win Rate: 55-60%

Current metrics (6.5/10) are inflated due to look-ahead bias!

## FINAL CHECKLIST

Before marking complete, verify:
- [ ] Look-ahead bias eliminated (statistical test passes)
- [ ] Walk-forward validation implemented
- [ ] All features saved to DB (no feature loss)
- [ ] No hardcoded paths/secrets
- [ ] Caching works (speedup verified)
- [ ] Obsolete code deleted (proxy volume)
- [ ] New volume features implemented (VP, VSA, Smart Money)
- [ ] Integration tests pass
- [ ] Documentation updated
- [ ] Performance metrics logged

## SUCCESS METRICS

Grade after completion:
- With fixes: 8.0/10 ⭐
- With volume features: 9.0/10 ⭐⭐
- With production (optional): 9.5/10 ⭐⭐⭐

## COMMON PITFALLS TO AVOID

1. ❌ Don't fit scaler on full dataset
   ✅ Fit only on train split

2. ❌ Don't use test data for any preprocessing
   ✅ Keep test set completely isolated until final evaluation

3. ❌ Don't ignore missing features
   ✅ Raise error if expected features not in DB

4. ❌ Don't hardcode paths
   ✅ Use Config class with .env

5. ❌ Don't skip tests
   ✅ Test after each task

## QUESTIONS?

If you encounter issues:
1. Check the detailed implementation above
2. Review existing codebase for patterns
3. Check test cases for examples
4. Look for similar code in the project

## LET'S GO! 🚀

Start with Priority 1, Task 1.1 (Fix look-ahead bias). 

Good luck! Remember: Quality > Speed. Take time to understand each bug before 
fixing it.

═══════════════════════════════════════════════════════════════════════════════
END OF IMPLEMENTATION GUIDE - GOOD LUCK!
═══════════════════════════════════════════════════════════════════════════════
```

═══════════════════════════════════════════════════════════════════════════════
📚 APPENDIX: ADDITIONAL RESOURCES
═══════════════════════════════════════════════════════════════════════════════

## A. Statistical Test for Look-Ahead Bias

```python
# tests/unit/test_look_ahead_bias.py
import numpy as np
from scipy import stats

def test_no_look_ahead_bias():
    """
    Verify no look-ahead bias by checking that test set statistics
    are significantly different from training set statistics
    """
    # Load train and test data
    X_train, X_test = load_split_data()
    
    # For each feature
    for col in X_train.columns:
        train_mean = X_train[col].mean()
        test_mean = X_test[col].mean()
        
        train_std = X_train[col].std()
        test_std = X_test[col].std()
        
        # Test distributions are different (two-sample t-test)
        t_stat, p_value = stats.ttest_ind(X_train[col], X_test[col])
        
        # If p < 0.05, distributions are significantly different (good!)
        # If p >= 0.05, distributions are too similar (potential bias!)
        
        if p_value >= 0.05:
            # Check if means are suspiciously close
            mean_diff_pct = abs(train_mean - test_mean) / train_mean
            if mean_diff_pct < 0.01:  # Less than 1% difference
                raise AssertionError(
                    f"Potential look-ahead bias detected in feature '{col}'!\n"
                    f"Train mean: {train_mean:.6f}, Test mean: {test_mean:.6f}\n"
                    f"Difference: {mean_diff_pct:.2%}\n"
                    f"p-value: {p_value:.4f}"
                )
```

## B. Feature Importance Analysis

```python
# src/forex_diffusion/analysis/feature_importance.py
import matplotlib.pyplot as plt
import seaborn as sns

def analyze_feature_importance(model, feature_names):
    """Analyze and visualize feature importance"""
    importance = model.feature_importances_
    
    # Sort by importance
    indices = np.argsort(importance)[::-1]
    
    # Print top 20
    print("Top 20 Most Important Features:")
    for i, idx in enumerate(indices[:20]):
        print(f"{i+1}. {feature_names[idx]}: {importance[idx]:.4f}")
    
    # Plot
    plt.figure(figsize=(12, 8))
    plt.title("Feature Importances")
    plt.barh(range(20), importance[indices[:20]])
    plt.yticks(range(20), [feature_names[i] for i in indices[:20]])
    plt.xlabel("Importance")
    plt.tight_layout()
    plt.savefig("feature_importance.png")
    
    return importance

# Expected top features with real volume:
# 1. volume_ma (volume moving average)
# 2. poc_distance (Volume Profile)
# 3. smart_money_score
# 4. rsi
# 5. accumulation (VSA)
# 6. hlc_vwap
# 7. macd
# 8. volume_imbalance
# ...
```

## C. Performance Comparison: Before vs After

```python
# scripts/compare_versions.py
"""
Compare performance before and after fixes

Expected results:
                  BEFORE (with bias)  AFTER (realistic)
Accuracy:         68-75%              58-65%
Sharpe:           2.1                 1.4
Max Drawdown:     8%                  15%
Win Rate:         67%                 57%

Why lower? Because BEFORE had look-ahead bias!
AFTER is the TRUE performance.
"""

import pandas as pd

def compare_versions():
    # Load results
    before = pd.read_csv("results_before_fix.csv")
    after = pd.read_csv("results_after_fix.csv")
    
    metrics = ['accuracy', 'sharpe', 'max_drawdown', 'win_rate']
    
    comparison = pd.DataFrame({
        'Before (Biased)': [before[m].mean() for m in metrics],
        'After (Realistic)': [after[m].mean() for m in metrics],
        'Change': [after[m].mean() - before[m].mean() for m in metrics]
    }, index=metrics)
    
    print(comparison)
    
    # The "After" numbers are lower but MORE TRUSTWORTHY!
```

## D. Debugging Checklist

If metrics are worse than expected:

1. **Data Quality**:
   - [ ] No missing values?
   - [ ] No extreme outliers?
   - [ ] Sufficient data (>10k samples)?
   - [ ] Volume data accurate?

2. **Preprocessing**:
   - [ ] No look-ahead bias? (statistical test passes)
   - [ ] Features scaled correctly?
   - [ ] No data leakage between splits?

3. **Model**:
   - [ ] Hyperparameters optimized?
   - [ ] No overfitting (train vs test similar)?
   - [ ] Feature importance makes sense?

4. **Validation**:
   - [ ] Walk-forward validation used?
   - [ ] Multiple splits averaged?
   - [ ] Out-of-sample performance tracked?

5. **Features**:
   - [ ] All 40+ features present?
   - [ ] Volume features computing correctly?
   - [ ] No NaN values?

═══════════════════════════════════════════════════════════════════════════════
🎓 LEARNING RESOURCES
═══════════════════════════════════════════════════════════════════════════════

## Recommended Reading

1. **Look-Ahead Bias**:
   - "Common Pitfalls in Machine Learning for Trading" (Lopez de Prado)
   - "Backtesting and Simulation" (Zivot)

2. **Walk-Forward Validation**:
   - "Evidence-Based Technical Analysis" (Aronson)
   - "Advances in Financial Machine Learning" (Lopez de Prado, Ch. 7)

3. **Volume Analysis**:
   - "Volume Profile: The Insider's Guide to Trading" (Goldberger)
   - "Master the Markets" (Williams) - VSA
   - "Trade What You See" (Bel Air) - Volume patterns

4. **Risk Management**:
   - "The Kelly Capital Growth Investment Criterion" (MacLean)
   - "Risk Management in Trading" (Davis)

═══════════════════════════════════════════════════════════════════════════════
📝 NOTES FOR FUTURE IMPROVEMENTS (10/10 territory)
═══════════════════════════════════════════════════════════════════════════════

After reaching 9.0/10, consider these enhancements:

1. **Ensemble Models** (+0.3 points):
   - Combine multiple models (RF, XGB, LightGBM)
   - Weighted voting based on recent performance
   - Different models for different market regimes

2. **Online Learning** (+0.2 points):
   - Incremental model updates
   - Adapt to changing market conditions
   - Concept drift detection and retraining triggers

3. **Alternative Data** (+0.2 points):
   - News sentiment (NLP on financial news)
   - Social media sentiment (Twitter, Reddit)
   - Options data (implied volatility, put/call ratio)
   - Economic calendar (scheduled news events)

4. **Regime Detection** (+0.2 points):
   - Detect trending vs ranging markets
   - Train separate models for each regime
   - Switch models based on current regime

5. **Deep Learning** (+0.1 points):
   - LSTM for sequential patterns
   - Transformer models for attention mechanisms
   - Requires significantly more data

Total potential: 9.0 + 1.0 = 10.0/10 🏆

═══════════════════════════════════════════════════════════════════════════════
✅ FINAL NOTES
═══════════════════════════════════════════════════════════════════════════════

This implementation guide provides everything needed to bring ForexGPT from 
6.5/10 to 9.0/10. The key improvements are:

1. **Fixing critical bugs** (look-ahead bias, feature loss)
2. **Proper validation** (walk-forward validation)
3. **Advanced volume features** (Volume Profile, VSA, Smart Money)
4. **Production readiness** (API, monitoring, backtesting)

Estimated timeline: 6-8 weeks full-time

Expected outcome: 
- More realistic but trustworthy performance metrics
- Production-ready system
- Solid foundation for future improvements

Good luck! 🚀

═══════════════════════════════════════════════════════════════════════════════
