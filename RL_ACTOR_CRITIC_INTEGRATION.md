# ğŸ¤– Actor-Critic Deep RL Integration - Complete Specification

## ğŸ¯ Overview

Integrazione di un sistema **Actor-Critic Deep Reinforcement Learning** per Portfolio Management e Trading, basato su:
- **PPO (Proximal Policy Optimization)** - state-of-the-art continuous action RL
- **SAC (Soft Actor-Critic)** - maximum entropy RL per exploration
- **A3C (Asynchronous Advantage Actor-Critic)** - parallel training
- **TD3 (Twin Delayed DDPG)** - stable continuous control

---

## ğŸ“š Teoria: Actor-Critic per Portfolio Management

### **Problema di RL**
```
State (s_t):
  - Portfolio weights: [w_EUR, w_GBP, w_JPY, w_CASH]
  - Market features: [returns, volatility, Sharpe, correlations]
  - Risk metrics: [VaR, CVaR, drawdown]
  - Sentiment: [VIX, orderbook imbalance, news sentiment]
  
Action (a_t):
  - Target portfolio weights: [w'_EUR, w'_GBP, w'_JPY, w'_CASH]
  - Continuous actions in [-1, 1], mapped to [0%, 100%]
  
Reward (r_t):
  - Primary: Sharpe ratio improvement
  - Penalties: Transaction costs, VaR violations, correlation breaches
  
Transition: s_t â†’ a_t â†’ s_(t+1), r_t
```

### **Actor-Critic Architecture**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ACTOR NETWORK                        â”‚
â”‚  Input: State (portfolio, market, risk)                   â”‚
â”‚  â”œâ”€ Dense(256) + ReLU                                      â”‚
â”‚  â”œâ”€ Dense(128) + ReLU                                      â”‚
â”‚  â”œâ”€ LSTM(64) - temporal patterns                           â”‚
â”‚  â””â”€ Dense(n_assets) + Softmax â†’ Portfolio Weights         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       CRITIC NETWORK                        â”‚
â”‚  Input: State + Action                                     â”‚
â”‚  â”œâ”€ Dense(256) + ReLU                                      â”‚
â”‚  â”œâ”€ Dense(128) + ReLU                                      â”‚
â”‚  â””â”€ Dense(1) â†’ Q-value (expected return)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Reward Function** (Multi-Objective)
```python
reward = (
    alpha * sharpe_improvement          # +1.0 for +1 Sharpe
    - beta * transaction_costs          # -0.1 for 1% costs
    - gamma * var_penalty               # -2.0 if VaR > limit
    - delta * correlation_penalty       # -1.0 if corr > limit
    + epsilon * diversification_bonus   # +0.5 if well-diversified
)

# Typical values:
alpha = 5.0    # Sharpe is primary objective
beta = 10.0    # Transaction costs heavily penalized
gamma = 20.0   # VaR violations severely penalized
delta = 5.0    # Correlation breaches penalized
epsilon = 1.0  # Diversification mildly rewarded
```

---

## ğŸ“‚ File Structure

```
src/forex_diffusion/
â”œâ”€â”€ rl/                                    # NEW MODULE
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ actor_critic/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ ppo_agent.py                  # PPO implementation
â”‚   â”‚   â”œâ”€â”€ sac_agent.py                  # SAC implementation
â”‚   â”‚   â”œâ”€â”€ a3c_agent.py                  # A3C implementation
â”‚   â”‚   â”œâ”€â”€ td3_agent.py                  # TD3 implementation
â”‚   â”‚   â””â”€â”€ base_agent.py                 # Base RL Agent
â”‚   â”œâ”€â”€ environments/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ portfolio_env.py              # Gym environment for portfolio
â”‚   â”‚   â””â”€â”€ trading_env.py                # Gym environment for trading
â”‚   â”œâ”€â”€ networks/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ actor_network.py              # Actor architecture
â”‚   â”‚   â”œâ”€â”€ critic_network.py             # Critic architecture
â”‚   â”‚   â””â”€â”€ shared_networks.py            # Shared layers (LSTM, etc)
â”‚   â”œâ”€â”€ replay_buffer.py                  # Experience replay
â”‚   â”œâ”€â”€ rewards.py                        # Reward function builder
â”‚   â””â”€â”€ trainer.py                        # RL training loop
â”‚
â”œâ”€â”€ ui/
â”‚   â””â”€â”€ rl_config_tab.py                  # NEW: RL Configuration Tab
â”‚
â””â”€â”€ trading/
    â””â”€â”€ rl_portfolio_manager.py           # NEW: RL-driven portfolio manager
```

---

## ğŸ¨ UI: RL Configuration Tab (Level 2 under Trading Intelligence)

### **NEW Tab Structure**

```
Trading Intelligence (Level 1)
â”œâ”€â”€ Portfolio (Level 2) - [6 sub-tabs Level 3]
â”œâ”€â”€ Signals (Level 2)
â””â”€â”€ RL Agent (Level 2) â† NEW
    â”œâ”€â”€ Agent Configuration (Level 3)      # Algorithm, hyperparameters
    â”œâ”€â”€ Training Settings (Level 3)         # Episodes, batch size, etc.
    â”œâ”€â”€ State & Action Space (Level 3)      # Input features, output actions
    â”œâ”€â”€ Reward Function (Level 3)           # Multi-objective weights
    â”œâ”€â”€ Training Progress (Level 3)         # Live training metrics
    â””â”€â”€ Deployment & Testing (Level 3)      # Backtest RL agent
```

---

## ğŸ”§ Level 3 - Tab 1: Agent Configuration

```python
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Reinforcement Learning Agent                                â”‚
â”‚                                                              â”‚
â”‚ â˜‘ Enable RL-Based Portfolio Management                     â”‚
â”‚   When enabled, RL agent co-pilots with Riskfolio optimizerâ”‚
â”‚                                                              â”‚
â”‚ RL Algorithm:                                                â”‚
â”‚ âš« PPO - Proximal Policy Optimization (Recommended)         â”‚
â”‚   â””â”€ Stable, sample-efficient, good for continuous actions â”‚
â”‚                                                              â”‚
â”‚ â—‹ SAC - Soft Actor-Critic                                   â”‚
â”‚   â””â”€ Maximum entropy, better exploration                   â”‚
â”‚                                                              â”‚
â”‚ â—‹ A3C - Asynchronous Advantage Actor-Critic                 â”‚
â”‚   â””â”€ Parallel training, faster convergence                 â”‚
â”‚                                                              â”‚
â”‚ â—‹ TD3 - Twin Delayed DDPG                                   â”‚
â”‚   â””â”€ Very stable, low variance                             â”‚
â”‚                                                              â”‚
â”‚ â—‹ Ensemble - Combine Multiple Agents                        â”‚
â”‚   â””â”€ Agents: â˜‘ PPO  â˜‘ SAC  â˜ A3C  â˜ TD3                  â”‚
â”‚   â””â”€ Aggregation: âš« Weighted Average  â—‹ Voting            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Widgets**:
- `enable_rl_check`: QCheckBox (default False)
- `rl_algorithm_combo`: QComboBox ["PPO", "SAC", "A3C", "TD3", "Ensemble"]
- `ensemble_agents_group`: QGroupBox with checkboxes (enabled only if Ensemble)
- `ensemble_aggregation_combo`: QComboBox ["Weighted Average", "Voting", "Meta-Learner"]

### **PPO Hyperparameters**
```python
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PPO Hyperparameters                                          â”‚
â”‚                                                              â”‚
â”‚ Clip Epsilon (Îµ):        [0.20] (0.1-0.3)  ğŸ›ˆ              â”‚
â”‚   â†’ Policy update clipping, higher = larger updates        â”‚
â”‚                                                              â”‚
â”‚ Value Function Coef (c1): [0.50] (0.1-1.0) ğŸ›ˆ              â”‚
â”‚   â†’ Weight of value loss in total loss                     â”‚
â”‚                                                              â”‚
â”‚ Entropy Coef (c2):       [0.01] (0.001-0.1) ğŸ›ˆ             â”‚
â”‚   â†’ Exploration bonus, higher = more exploration           â”‚
â”‚                                                              â”‚
â”‚ Learning Rate (Î±):       [3e-4] (1e-5 to 1e-3) ğŸ›ˆ          â”‚
â”‚   Adaptive: â˜‘ Linear Decay  Start: [3e-4] â†’ End: [1e-5]  â”‚
â”‚                                                              â”‚
â”‚ Discount Factor (Î³):     [0.99] (0.9-0.999) ğŸ›ˆ             â”‚
â”‚   â†’ Future reward discounting                              â”‚
â”‚                                                              â”‚
â”‚ GAE Lambda (Î»):          [0.95] (0.9-0.99) ğŸ›ˆ              â”‚
â”‚   â†’ Generalized Advantage Estimation smoothing            â”‚
â”‚                                                              â”‚
â”‚ Mini-Batch Size:         [64  ] (32-256) ğŸ›ˆ                â”‚
â”‚ Optimization Epochs:     [10  ] (5-20) ğŸ›ˆ                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Widgets**:
- `ppo_clip_epsilon_spin`: QDoubleSpinBox (0.1-0.3, default 0.2)
- `ppo_value_coef_spin`: QDoubleSpinBox (0.1-1.0, default 0.5)
- `ppo_entropy_coef_spin`: QDoubleSpinBox (0.001-0.1, default 0.01)
- `ppo_learning_rate_spin`: QDoubleSpinBox (1e-5 to 1e-3, default 3e-4, scientific notation)
- `ppo_lr_decay_check`: QCheckBox (default True)
- `ppo_lr_start_spin`: QDoubleSpinBox (enabled if decay)
- `ppo_lr_end_spin`: QDoubleSpinBox (enabled if decay)
- `ppo_gamma_spin`: QDoubleSpinBox (0.9-0.999, default 0.99)
- `ppo_gae_lambda_spin`: QDoubleSpinBox (0.9-0.99, default 0.95)
- `ppo_batch_size_spin`: QSpinBox (32-256, default 64)
- `ppo_epochs_spin`: QSpinBox (5-20, default 10)

### **Actor-Critic Networks Architecture**
```python
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Neural Network Architecture                                  â”‚
â”‚                                                              â”‚
â”‚ Actor Network:                                               â”‚
â”‚   Hidden Layers: [256, 128, 64]  (comma-separated)         â”‚
â”‚   Activation:    âš« ReLU  â—‹ Tanh  â—‹ LeakyReLU              â”‚
â”‚   â˜‘ Use LSTM Layer (64 units) for temporal patterns        â”‚
â”‚   â˜‘ Batch Normalization                                     â”‚
â”‚   Dropout:       [0.2 ] (0.0-0.5)                          â”‚
â”‚                                                              â”‚
â”‚ Critic Network:                                              â”‚
â”‚   Hidden Layers: [256, 128]  (comma-separated)             â”‚
â”‚   Activation:    âš« ReLU  â—‹ Tanh  â—‹ LeakyReLU              â”‚
â”‚   â˜ Use LSTM Layer                                          â”‚
â”‚   â˜‘ Batch Normalization                                     â”‚
â”‚   Dropout:       [0.2 ] (0.0-0.5)                          â”‚
â”‚                                                              â”‚
â”‚ Optimizer:                                                   â”‚
â”‚   âš« Adam    â—‹ AdamW   â—‹ RMSprop   â—‹ SGD                   â”‚
â”‚   Beta1: [0.9]  Beta2: [0.999]  Epsilon: [1e-8]           â”‚
â”‚                                                              â”‚
â”‚ Device:                                                      â”‚
â”‚   âš« GPU (cuda:0) âœ“ Available                              â”‚
â”‚   â—‹ CPU (fallback)                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Widgets**:
- `actor_hidden_layers_edit`: QLineEdit (default "256,128,64")
- `actor_activation_combo`: QComboBox ["ReLU", "Tanh", "LeakyReLU", "ELU"]
- `actor_use_lstm_check`: QCheckBox (default True)
- `actor_lstm_units_spin`: QSpinBox (32-256, default 64)
- `actor_batch_norm_check`: QCheckBox (default True)
- `actor_dropout_spin`: QDoubleSpinBox (0-0.5, default 0.2)
- `critic_hidden_layers_edit`: QLineEdit (default "256,128")
- `critic_activation_combo`: QComboBox
- `critic_use_lstm_check`: QCheckBox (default False)
- `critic_batch_norm_check`: QCheckBox
- `critic_dropout_spin`: QDoubleSpinBox
- `optimizer_combo`: QComboBox ["Adam", "AdamW", "RMSprop", "SGD"]
- `optimizer_beta1_spin`: QDoubleSpinBox (0.8-0.99, default 0.9)
- `optimizer_beta2_spin`: QDoubleSpinBox (0.9-0.999, default 0.999)
- `optimizer_epsilon_spin`: QDoubleSpinBox (1e-10 to 1e-6, default 1e-8)
- `device_combo`: QComboBox ["GPU (cuda:0)", "CPU", "Auto"]

---

## ğŸ”§ Level 3 - Tab 2: Training Settings

```python
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Training Configuration                                       â”‚
â”‚                                                              â”‚
â”‚ Training Mode:                                               â”‚
â”‚ âš« Offline (Historical Data)                                 â”‚
â”‚   â””â”€ Train on: Last [365] days                             â”‚
â”‚   â””â”€ Validation Split: [20]%                               â”‚
â”‚                                                              â”‚
â”‚ â—‹ Online (Live Trading + Learning)                          â”‚
â”‚   â””â”€ Update Frequency: [Daily] (Hourly/Daily/Weekly)      â”‚
â”‚   â””â”€ âš  Warning: Can be unstable in production              â”‚
â”‚                                                              â”‚
â”‚ â—‹ Hybrid (Pretrain Offline, Fine-tune Online)               â”‚
â”‚   â””â”€ Offline Episodes: [1000]                              â”‚
â”‚   â””â”€ Online Learning Rate: [1e-5] (10Ã— lower)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Widgets**:
- `training_mode_combo`: QComboBox ["Offline", "Online", "Hybrid"]
- `offline_days_spin`: QSpinBox (30-1000, default 365, enabled if Offline/Hybrid)
- `validation_split_spin`: QSpinBox (10-30%, default 20%)
- `online_update_frequency_combo`: QComboBox ["Hourly", "Daily", "Weekly"]
- `hybrid_offline_episodes_spin`: QSpinBox (100-10000, default 1000)
- `hybrid_online_lr_spin`: QDoubleSpinBox

### **Episode Configuration**
```python
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Training Episodes                                            â”‚
â”‚                                                              â”‚
â”‚ Total Episodes:          [5000 ] (100-50000)  ğŸ›ˆ           â”‚
â”‚   Each episode = 1 portfolio rebalancing trajectory        â”‚
â”‚                                                              â”‚
â”‚ Steps per Episode:       [252  ] (trading days)  ğŸ›ˆ        â”‚
â”‚   1 year = 252 trading days (default)                      â”‚
â”‚                                                              â”‚
â”‚ Parallel Environments:   [4    ] (1-16)  ğŸ›ˆ                â”‚
â”‚   Run N environments in parallel (A3C-style)               â”‚
â”‚                                                              â”‚
â”‚ Replay Buffer Size:      [100000] (10k-1M)  ğŸ›ˆ             â”‚
â”‚   Store last N transitions for experience replay           â”‚
â”‚                                                              â”‚
â”‚ Warmup Steps:            [1000 ] (0-10000)  ğŸ›ˆ             â”‚
â”‚   Random actions before training starts                    â”‚
â”‚                                                              â”‚
â”‚ Target Update Frequency: [1000 ] steps  ğŸ›ˆ                 â”‚
â”‚   Copy online network â†’ target network every N steps      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Widgets**:
- `total_episodes_spin`: QSpinBox (100-50000, default 5000)
- `steps_per_episode_spin`: QSpinBox (50-500, default 252)
- `parallel_envs_spin`: QSpinBox (1-16, default 4)
- `replay_buffer_size_spin`: QSpinBox (10000-1000000, default 100000)
- `warmup_steps_spin`: QSpinBox (0-10000, default 1000)
- `target_update_frequency_spin`: QSpinBox (100-10000, default 1000)

### **Early Stopping & Checkpointing**
```python
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Training Control                                             â”‚
â”‚                                                              â”‚
â”‚ â˜‘ Early Stopping                                            â”‚
â”‚   Metric:    âš« Mean Episode Reward  â—‹ Validation Sharpe   â”‚
â”‚   Patience:  [50  ] episodes without improvement           â”‚
â”‚   Min Delta: [0.01] minimum improvement to count           â”‚
â”‚                                                              â”‚
â”‚ â˜‘ Save Best Model                                           â”‚
â”‚   Checkpoint every: [100] episodes                          â”‚
â”‚   Keep best:        [5  ] checkpoints                       â”‚
â”‚   Save path:        [artifacts/rl_checkpoints/]            â”‚
â”‚                                                              â”‚
â”‚ â˜‘ TensorBoard Logging                                       â”‚
â”‚   Log directory:    [runs/rl_training/]                    â”‚
â”‚   Update every:     [10 ] episodes                          â”‚
â”‚                                                              â”‚
â”‚ â˜‘ Evaluation During Training                                â”‚
â”‚   Eval frequency:   [100] episodes                          â”‚
â”‚   Eval episodes:    [10 ] episodes                          â”‚
â”‚   Eval on:          âš« Validation Set  â—‹ Test Set          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Widgets**:
- `early_stopping_check`: QCheckBox (default True)
- `early_stopping_metric_combo`: QComboBox ["Mean Reward", "Validation Sharpe", "Validation Sortino"]
- `early_stopping_patience_spin`: QSpinBox (10-200, default 50)
- `early_stopping_delta_spin`: QDoubleSpinBox (0.001-0.1, default 0.01)
- `save_best_model_check`: QCheckBox (default True)
- `checkpoint_frequency_spin`: QSpinBox (10-1000, default 100)
- `keep_best_checkpoints_spin`: QSpinBox (1-10, default 5)
- `checkpoint_path_edit`: QLineEdit (with Browse button)
- `tensorboard_logging_check`: QCheckBox (default True)
- `tensorboard_log_dir_edit`: QLineEdit
- `tensorboard_update_frequency_spin`: QSpinBox (1-100, default 10)
- `evaluation_during_training_check`: QCheckBox (default True)
- `eval_frequency_spin`: QSpinBox (10-1000, default 100)
- `eval_episodes_spin`: QSpinBox (5-50, default 10)
- `eval_dataset_combo`: QComboBox ["Validation Set", "Test Set"]

---

## ğŸ”§ Level 3 - Tab 3: State & Action Space

### **State Space Configuration**
```python
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ State Representation (Input to Actor-Critic)                â”‚
â”‚                                                              â”‚
â”‚ Portfolio Features (Always Included):                        â”‚
â”‚   â˜‘ Current Weights (n_assets)                             â”‚
â”‚   â˜‘ Current P&L (1)                                         â”‚
â”‚   â˜‘ Days in Position (n_assets)                            â”‚
â”‚   â˜‘ Unrealized P&L per Asset (n_assets)                    â”‚
â”‚                                                              â”‚
â”‚ Market Features:                                             â”‚
â”‚   â˜‘ Returns (lookback: [20] days)                          â”‚
â”‚   â˜‘ Volatility (lookback: [20] days)                       â”‚
â”‚   â˜‘ Correlation Matrix (flattened, nÃ—n)                    â”‚
â”‚   â˜‘ Momentum (lookback: [10] days)                         â”‚
â”‚   â˜‘ RSI (14-period)                                         â”‚
â”‚   â˜ MACD                                                    â”‚
â”‚   â˜ Bollinger Bands (% position)                           â”‚
â”‚   â˜ ATR (normalized)                                        â”‚
â”‚                                                              â”‚
â”‚ Risk Features:                                               â”‚
â”‚   â˜‘ Portfolio VaR (95%)                                     â”‚
â”‚   â˜‘ Portfolio CVaR (95%)                                    â”‚
â”‚   â˜‘ Current Drawdown                                        â”‚
â”‚   â˜‘ Sharpe Ratio (rolling 30d)                             â”‚
â”‚   â˜‘ Sortino Ratio (rolling 30d)                            â”‚
â”‚   â˜ Max Drawdown Duration                                  â”‚
â”‚                                                              â”‚
â”‚ Sentiment Features (if available):                          â”‚
â”‚   â˜‘ VIX Level                                               â”‚
â”‚   â˜‘ VIX Percentile (historical)                            â”‚
â”‚   â˜ News Sentiment Score                                   â”‚
â”‚   â˜ Orderbook Imbalance                                    â”‚
â”‚   â˜ Twitter Sentiment                                       â”‚
â”‚                                                              â”‚
â”‚ Total State Dimension: [137] (calculated)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Widgets**:
- **Portfolio Features** (always enabled, grayed out):
  - Labels only (no widgets)
- **Market Features**:
  - `state_returns_check`: QCheckBox (default True)
  - `state_returns_lookback_spin`: QSpinBox (5-60, default 20)
  - `state_volatility_check`: QCheckBox (default True)
  - `state_volatility_lookback_spin`: QSpinBox
  - `state_correlation_check`: QCheckBox (default True)
  - `state_momentum_check`: QCheckBox (default True)
  - `state_momentum_lookback_spin`: QSpinBox (5-30, default 10)
  - `state_rsi_check`: QCheckBox (default True)
  - `state_macd_check`: QCheckBox (default False)
  - `state_bollinger_check`: QCheckBox (default False)
  - `state_atr_check`: QCheckBox (default False)
- **Risk Features**:
  - `state_var_check`: QCheckBox (default True)
  - `state_cvar_check`: QCheckBox (default True)
  - `state_drawdown_check`: QCheckBox (default True)
  - `state_sharpe_check`: QCheckBox (default True)
  - `state_sortino_check`: QCheckBox (default True)
  - `state_max_dd_duration_check`: QCheckBox (default False)
- **Sentiment Features**:
  - `state_vix_check`: QCheckBox (default True)
  - `state_vix_percentile_check`: QCheckBox (default True)
  - `state_news_sentiment_check`: QCheckBox (default False)
  - `state_orderbook_imbalance_check`: QCheckBox (default False)
  - `state_twitter_sentiment_check`: QCheckBox (default False)
- `total_state_dimension_label`: QLabel (auto-calculated)

### **Action Space Configuration**
```python
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Action Representation (Output from Actor)                   â”‚
â”‚                                                              â”‚
â”‚ Action Type:                                                 â”‚
â”‚ âš« Continuous - Target Portfolio Weights [0, 1]             â”‚
â”‚   Each asset gets weight in [0%, 100%], sum = 100%        â”‚
â”‚   Actor outputs: Softmax(logits) â†’ normalized weights     â”‚
â”‚                                                              â”‚
â”‚ â—‹ Discrete - Rebalancing Decisions {-1, 0, +1}             â”‚
â”‚   -1 = Reduce weight, 0 = Hold, +1 = Increase weight      â”‚
â”‚   Step size: [5]% per action                               â”‚
â”‚                                                              â”‚
â”‚ Action Constraints:                                          â”‚
â”‚   Min Weight per Asset: [0.0 ] % (from Portfolio Tab)     â”‚
â”‚   Max Weight per Asset: [25.0] % (from Portfolio Tab)     â”‚
â”‚   â˜‘ Force Sum to 100% (renormalize if needed)             â”‚
â”‚   â˜‘ Long-Only (no shorting, weights â‰¥ 0)                  â”‚
â”‚                                                              â”‚
â”‚ Action Smoothing (reduce volatility):                       â”‚
â”‚   â˜‘ Exponential Moving Average                             â”‚
â”‚     Î± (smoothing): [0.3] (0.1=smooth, 0.9=reactive)       â”‚
â”‚                                                              â”‚
â”‚ Total Action Dimension: [4] (n_assets)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Widgets**:
- `action_type_combo`: QComboBox ["Continuous (Weights)", "Discrete (Steps)"]
- `discrete_step_size_spin`: QDoubleSpinBox (1-20%, default 5%, enabled only if Discrete)
- `min_weight_per_asset_label`: QLabel (read from Portfolio Tab, grayed)
- `max_weight_per_asset_label`: QLabel (read from Portfolio Tab, grayed)
- `force_sum_100_check`: QCheckBox (default True)
- `long_only_check`: QCheckBox (default True)
- `action_smoothing_check`: QCheckBox (default True)
- `action_smoothing_alpha_spin`: QDoubleSpinBox (0.1-0.9, default 0.3)
- `total_action_dimension_label`: QLabel (auto-calculated)

---

## ğŸ”§ Level 3 - Tab 4: Reward Function

```python
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Multi-Objective Reward Function                             â”‚
â”‚                                                              â”‚
â”‚ Total Reward = Î£(weight_i Ã— component_i)                   â”‚
â”‚                                                              â”‚
â”‚ â˜‘ Sharpe Ratio Improvement                                  â”‚
â”‚   Weight: [5.0 ] (-10 to +10)  ğŸ›ˆ                          â”‚
â”‚   Reward: +weight Ã— Î” Sharpe                               â”‚
â”‚   â””â”€ Example: +5.0 if Sharpe improves by 1.0              â”‚
â”‚                                                              â”‚
â”‚ â˜‘ Transaction Cost Penalty                                  â”‚
â”‚   Weight: [-10.0] (always negative!)  ğŸ›ˆ                   â”‚
â”‚   Reward: weight Ã— (cost / portfolio_value)                â”‚
â”‚   â””â”€ Example: -1.0 if costs are 0.1% of portfolio         â”‚
â”‚                                                              â”‚
â”‚ â˜‘ VaR Violation Penalty                                     â”‚
â”‚   Weight: [-20.0] (always negative!)  ğŸ›ˆ                   â”‚
â”‚   Reward: weight if VaR > limit, else 0                    â”‚
â”‚   â””â”€ Example: -20.0 if VaR exceeds 10% limit              â”‚
â”‚                                                              â”‚
â”‚ â˜‘ CVaR Violation Penalty                                    â”‚
â”‚   Weight: [-15.0] (always negative!)  ğŸ›ˆ                   â”‚
â”‚   Reward: weight if CVaR > limit, else 0                   â”‚
â”‚                                                              â”‚
â”‚ â˜‘ Correlation Violation Penalty                             â”‚
â”‚   Weight: [-5.0 ] (always negative!)  ğŸ›ˆ                   â”‚
â”‚   Reward: weight if correlated_exposure > limit            â”‚
â”‚                                                              â”‚
â”‚ â˜‘ Diversification Bonus                                     â”‚
â”‚   Weight: [1.0  ] (always positive!)  ğŸ›ˆ                   â”‚
â”‚   Reward: weight Ã— (1 - HHI)  where HHI = Î£(wÂ²)           â”‚
â”‚   â””â”€ Example: +0.8 if portfolio well-diversified          â”‚
â”‚                                                              â”‚
â”‚ â˜ Drawdown Penalty                                          â”‚
â”‚   Weight: [-10.0] (always negative!)  ğŸ›ˆ                   â”‚
â”‚   Reward: weight Ã— drawdown_pct                            â”‚
â”‚                                                              â”‚
â”‚ â˜ Turnover Penalty                                          â”‚
â”‚   Weight: [-3.0 ] (always negative!)  ğŸ›ˆ                   â”‚
â”‚   Reward: weight Ã— (turnover_pct)                          â”‚
â”‚   â””â”€ Penalize excessive rebalancing                       â”‚
â”‚                                                              â”‚
â”‚ â˜ Sortino Ratio Improvement                                 â”‚
â”‚   Weight: [3.0  ] (always positive!)  ğŸ›ˆ                   â”‚
â”‚   Reward: +weight Ã— Î” Sortino                              â”‚
â”‚                                                              â”‚
â”‚ Custom Reward Components:                                    â”‚
â”‚   [ + Add Custom Component ]                                â”‚
â”‚                                                              â”‚
â”‚ Preview Reward Function:                                     â”‚
â”‚   [ Show Reward Breakdown Chart ]                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Widgets**:
- `reward_sharpe_check`: QCheckBox (default True)
- `reward_sharpe_weight_spin`: QDoubleSpinBox (-10 to +10, default 5.0)
- `reward_transaction_cost_check`: QCheckBox (default True)
- `reward_transaction_cost_weight_spin`: QDoubleSpinBox (-20 to 0, default -10.0)
- `reward_var_violation_check`: QCheckBox (default True)
- `reward_var_violation_weight_spin`: QDoubleSpinBox (-50 to 0, default -20.0)
- `reward_cvar_violation_check`: QCheckBox (default True)
- `reward_cvar_violation_weight_spin`: QDoubleSpinBox (-30 to 0, default -15.0)
- `reward_correlation_violation_check`: QCheckBox (default True)
- `reward_correlation_violation_weight_spin`: QDoubleSpinBox (-10 to 0, default -5.0)
- `reward_diversification_check`: QCheckBox (default True)
- `reward_diversification_weight_spin`: QDoubleSpinBox (0 to +10, default 1.0)
- `reward_drawdown_check`: QCheckBox (default False)
- `reward_drawdown_weight_spin`: QDoubleSpinBox (-20 to 0, default -10.0)
- `reward_turnover_check`: QCheckBox (default False)
- `reward_turnover_weight_spin`: QDoubleSpinBox (-10 to 0, default -3.0)
- `reward_sortino_check`: QCheckBox (default False)
- `reward_sortino_weight_spin`: QDoubleSpinBox (0 to +10, default 3.0)
- `add_custom_reward_btn`: QPushButton (opens dialog for custom Python function)
- `preview_reward_btn`: QPushButton (shows matplotlib chart)

### **Reward Shaping & Normalization**
```python
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Reward Processing                                            â”‚
â”‚                                                              â”‚
â”‚ Reward Normalization:                                        â”‚
â”‚ âš« Running Z-Score (mean=0, std=1)                          â”‚
â”‚ â—‹ Min-Max Scaling [0, 1]                                    â”‚
â”‚ â—‹ Clipping [-10, +10]                                       â”‚
â”‚ â—‹ None (use raw rewards)                                    â”‚
â”‚                                                              â”‚
â”‚ Reward Clipping:                                             â”‚
â”‚   Min Reward: [-100.0] (prevent extreme penalties)         â”‚
â”‚   Max Reward: [+100.0] (prevent extreme bonuses)           â”‚
â”‚                                                              â”‚
â”‚ â˜‘ Reward Shaping (Potential-Based)                          â”‚
â”‚   Add intermediate rewards to guide learning               â”‚
â”‚   Example: +0.1 per step if Sharpe > 1.0                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Widgets**:
- `reward_normalization_combo`: QComboBox ["Z-Score", "Min-Max", "Clipping", "None"]
- `reward_min_clip_spin`: QDoubleSpinBox (-1000 to 0, default -100)
- `reward_max_clip_spin`: QDoubleSpinBox (0 to +1000, default +100)
- `reward_shaping_check`: QCheckBox (default True)

---

## ğŸ”§ Level 3 - Tab 5: Training Progress

### **Live Training Metrics** (Real-time updates during training)
```python
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Training Status                                              â”‚
â”‚                                                              â”‚
â”‚ Status: âš« Training (Episode 1243 / 5000)                   â”‚
â”‚ Progress: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  24.86%             â”‚
â”‚ Elapsed Time:  02:45:33                                     â”‚
â”‚ Est. Remaining: 08:23:15                                     â”‚
â”‚ ETA: 2025-01-21 18:45:00                                    â”‚
â”‚                                                              â”‚
â”‚ Current Episode Metrics:                                     â”‚
â”‚   Episode Reward:    +35.42                                 â”‚
â”‚   Episode Length:    252 steps (1 year)                    â”‚
â”‚   Avg Sharpe:        1.85                                   â”‚
â”‚   Transaction Costs: $45.20 (0.45%)                        â”‚
â”‚   VaR Violations:    0                                      â”‚
â”‚                                                              â”‚
â”‚ [ Pause Training ]  [ Stop Training ]  [ Save Checkpoint ] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Widgets**:
- `training_status_label`: QLabel (auto-updated)
- `training_progress_bar`: QProgressBar (0-100%)
- `elapsed_time_label`: QLabel (HH:MM:SS)
- `remaining_time_label`: QLabel (HH:MM:SS)
- `eta_label`: QLabel (datetime)
- `current_episode_reward_label`: QLabel (live)
- `current_episode_length_label`: QLabel
- `current_avg_sharpe_label`: QLabel
- `current_transaction_costs_label`: QLabel
- `current_var_violations_label`: QLabel
- `pause_training_btn`: QPushButton
- `stop_training_btn`: QPushButton
- `save_checkpoint_btn`: QPushButton

### **Training Curves** (Matplotlib live plots)
```python
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Training Performance                                         â”‚
â”‚                                                              â”‚
â”‚ [Matplotlib Figure with 4 subplots]                         â”‚
â”‚                                                              â”‚
â”‚ 1) Mean Episode Reward (rolling 100 episodes)               â”‚
â”‚    Y-axis: Reward, X-axis: Episode                         â”‚
â”‚    Line: Mean Â± Std                                         â”‚
â”‚                                                              â”‚
â”‚ 2) Actor Loss + Critic Loss                                 â”‚
â”‚    Y-axis: Loss, X-axis: Training Step                     â”‚
â”‚    Two lines: Actor (blue), Critic (orange)                â”‚
â”‚                                                              â”‚
â”‚ 3) Portfolio Sharpe Ratio (eval episodes)                   â”‚
â”‚    Y-axis: Sharpe, X-axis: Episode                         â”‚
â”‚    Scatter points + trendline                               â”‚
â”‚                                                              â”‚
â”‚ 4) Transaction Costs (cumulative)                           â”‚
â”‚    Y-axis: Total Costs ($), X-axis: Episode                â”‚
â”‚    Bar chart per evaluation                                 â”‚
â”‚                                                              â”‚
â”‚ Update Frequency: âš« Real-time  â—‹ Every 10 episodes         â”‚
â”‚ [ Export Plots PNG ]  [ Open in TensorBoard ]              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Widgets**:
- `training_curves_canvas`: Custom QWidget with embedded matplotlib FigureCanvas (4 subplots)
- `update_frequency_combo`: QComboBox ["Real-time", "Every 10 episodes", "Every 50 episodes"]
- `export_plots_btn`: QPushButton (saves PNG)
- `open_tensorboard_btn`: QPushButton (launches TensorBoard server)

### **Best Models Leaderboard**
```python
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Top 5 Checkpoints (by Validation Sharpe)                    â”‚
â”‚                                                              â”‚
â”‚ Rank  Episode  Val Sharpe  Val Sortino  Max DD  Costs      â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚  1    1156     2.34        3.12         -6.2%   $320       â”‚
â”‚  2    1089     2.28        3.05         -7.1%   $340       â”‚
â”‚  3    1201     2.25        2.98         -5.8%   $380       â”‚
â”‚  4    982      2.20        2.89         -8.3%   $290       â”‚
â”‚  5    1134     2.18        2.85         -6.9%   $350       â”‚
â”‚                                                              â”‚
â”‚ Selected Model: Rank 1 (Episode 1156)                      â”‚
â”‚ [ Load for Deployment ]  [ Compare Models ]  [ Delete ]    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Widgets**:
- `best_models_table`: QTableWidget (6 columns, top 5 rows)
- `selected_model_label`: QLabel (shows current selection)
- `load_for_deployment_btn`: QPushButton (loads selected model)
- `compare_models_btn`: QPushButton (opens comparison dialog)
- `delete_model_btn`: QPushButton (deletes checkpoint file)

---

## ğŸ”§ Level 3 - Tab 6: Deployment & Testing

### **Backtest RL Agent**
```python
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Backtest Configuration                                       â”‚
â”‚                                                              â”‚
â”‚ Model to Test:                                               â”‚
â”‚   âš« Best Checkpoint (Episode 1156, Sharpe 2.34)            â”‚
â”‚   â—‹ Latest Checkpoint (Episode 1243)                        â”‚
â”‚   â—‹ Load from File: [Browse...]                            â”‚
â”‚                                                              â”‚
â”‚ Backtest Period:                                             â”‚
â”‚   Start Date:  [2024-01-01]  (date picker)                 â”‚
â”‚   End Date:    [2024-12-31]  (date picker)                 â”‚
â”‚   â””â”€ Duration: 365 days (calculated)                       â”‚
â”‚                                                              â”‚
â”‚ Initial Capital: [$10,000.00]                               â”‚
â”‚ Rebalancing:     âš« RL Agent Decisions  â—‹ Daily  â—‹ Weekly  â”‚
â”‚                                                              â”‚
â”‚ Comparison Baselines:                                        â”‚
â”‚   â˜‘ Buy & Hold Equal Weight                                â”‚
â”‚   â˜‘ Riskfolio Optimizer (no RL)                            â”‚
â”‚   â˜‘ Random Agent                                            â”‚
â”‚   â˜ Custom Strategy: [___________]                         â”‚
â”‚                                                              â”‚
â”‚ [ Run Backtest ]                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Widgets**:
- `backtest_model_combo`: QComboBox ["Best Checkpoint", "Latest Checkpoint", "Load from File"]
- `backtest_model_path_edit`: QLineEdit (with Browse button, enabled if "Load from File")
- `backtest_start_date`: QDateEdit (calendar popup)
- `backtest_end_date`: QDateEdit
- `backtest_duration_label`: QLabel (auto-calculated)
- `backtest_initial_capital_spin`: QDoubleSpinBox (1000-1000000, default 10000)
- `backtest_rebalancing_combo`: QComboBox ["RL Agent", "Daily", "Weekly", "Monthly"]
- `compare_equal_weight_check`: QCheckBox (default True)
- `compare_riskfolio_check`: QCheckBox (default True)
- `compare_random_check`: QCheckBox (default True)
- `compare_custom_check`: QCheckBox (default False)
- `compare_custom_edit`: QLineEdit (Python class name)
- `run_backtest_btn`: QPushButton (starts backtest)

### **Backtest Results** (After running)
```python
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Backtest Results (2024-01-01 to 2024-12-31)                â”‚
â”‚                                                              â”‚
â”‚ Strategy         Final  Return  Sharpe  Sortino MaxDD Costsâ”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ RL Agent (PPO)   $13,456 +34.6%  2.28    3.05   -6.2% $380â”‚
â”‚ Riskfolio        $12,890 +28.9%  1.85    2.45   -8.1% $420â”‚
â”‚ Equal Weight     $11,234 +12.3%  0.98    1.23  -12.5% $150â”‚
â”‚ Random Agent     $ 9,567  -4.3% -0.23   -0.45  -18.3% $650â”‚
â”‚                                                              â”‚
â”‚ Winner: ğŸ† RL Agent (PPO) - Best Sharpe Ratio              â”‚
â”‚                                                              â”‚
â”‚ [Equity Curve Chart]  [Drawdown Chart]  [Export CSV]       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Widgets**:
- `backtest_results_table`: QTableWidget (7 columns, N strategies)
- `winner_label`: QLabel (highlights best strategy)
- `show_equity_curve_btn`: QPushButton (opens matplotlib plot)
- `show_drawdown_chart_btn`: QPushButton
- `export_results_csv_btn`: QPushButton

### **Deploy to Live Trading**
```python
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Production Deployment                                        â”‚
â”‚                                                              â”‚
â”‚ âš  WARNING: Deploy to live trading ONLY after validation!   â”‚
â”‚                                                              â”‚
â”‚ Deployment Mode:                                             â”‚
â”‚ âš« RL Agent Only (RL decisions override Riskfolio)          â”‚
â”‚ â—‹ RL + Riskfolio Hybrid (average both agents)              â”‚
â”‚ â—‹ RL Advisory (RL suggests, Riskfolio decides)             â”‚
â”‚                                                              â”‚
â”‚ Safety Limits:                                               â”‚
â”‚   â˜‘ Max Daily Trades: [10  ] (prevent overtrading)        â”‚
â”‚   â˜‘ Max Position Deviation from Riskfolio: [10]%          â”‚
â”‚   â˜‘ Disable RL if Sharpe < [0.5] for [7] days             â”‚
â”‚   â˜‘ Emergency Stop if Portfolio Loss > [15]%               â”‚
â”‚                                                              â”‚
â”‚ Monitoring:                                                  â”‚
â”‚   â˜‘ Log all RL decisions to database                       â”‚
â”‚   â˜‘ Email alerts on anomalous actions                      â”‚
â”‚   â˜‘ Daily performance report                                â”‚
â”‚                                                              â”‚
â”‚ Current Status: â—‹ Not Deployed                              â”‚
â”‚                                                              â”‚
â”‚ [ Deploy to Production ]  (requires admin password)         â”‚
â”‚ [ Disable RL Agent ]                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Widgets**:
- `deployment_mode_combo`: QComboBox ["RL Only", "RL + Riskfolio Hybrid", "RL Advisory"]
- `max_daily_trades_check`: QCheckBox
- `max_daily_trades_spin`: QSpinBox (1-100, default 10)
- `max_deviation_check`: QCheckBox
- `max_deviation_spin`: QSpinBox (5-50%, default 10%)
- `disable_on_low_sharpe_check`: QCheckBox
- `disable_sharpe_threshold_spin`: QDoubleSpinBox (0-2.0, default 0.5)
- `disable_sharpe_days_spin`: QSpinBox (1-30, default 7)
- `emergency_stop_check`: QCheckBox
- `emergency_stop_loss_spin`: QSpinBox (5-50%, default 15%)
- `log_decisions_check`: QCheckBox (default True)
- `email_alerts_check`: QCheckBox (default True)
- `daily_report_check`: QCheckBox (default True)
- `deployment_status_label`: QLabel ("Not Deployed" / "Active" with color)
- `deploy_to_production_btn`: QPushButton (protected, requires password dialog)
- `disable_rl_agent_btn`: QPushButton (immediate disable)

---

## ğŸ”— Backend Implementation

### **Key Classes**

#### 1. **RLPortfolioManager** (`rl_portfolio_manager.py`)
```python
class RLPortfolioManager:
    """
    Manages RL agent for portfolio optimization
    
    Integrates with:
    - PortfolioOptimizer (Riskfolio)
    - AutomatedTradingEngine
    - RL Agent (PPO/SAC/A3C/TD3)
    """
    
    def __init__(self, config: RLConfig, portfolio_optimizer, trading_engine):
        self.config = config
        self.portfolio_optimizer = portfolio_optimizer
        self.trading_engine = trading_engine
        
        # Create RL agent based on config
        self.agent = self._create_agent()
        
        # Gym environment
        self.env = PortfolioEnvironment(...)
    
    def train(self, episodes: int):
        """Train RL agent"""
        for episode in range(episodes):
            state = self.env.reset()
            done = False
            
            while not done:
                action = self.agent.select_action(state)
                next_state, reward, done, info = self.env.step(action)
                self.agent.store_transition(state, action, reward, next_state, done)
                state = next_state
            
            # Update agent
            self.agent.update()
            
            # Log metrics
            self._log_episode_metrics(episode, info)
    
    def get_target_weights(self, state: np.ndarray) -> np.ndarray:
        """Get portfolio weights from RL agent"""
        action = self.agent.select_action(state, deterministic=True)
        weights = self._action_to_weights(action)
        return weights
    
    def _create_agent(self):
        """Factory for RL agents"""
        if self.config.algorithm == 'PPO':
            return PPOAgent(...)
        elif self.config.algorithm == 'SAC':
            return SACAgent(...)
        elif self.config.algorithm == 'A3C':
            return A3CAgent(...)
        elif self.config.algorithm == 'TD3':
            return TD3Agent(...)
```

#### 2. **PortfolioEnvironment** (`environments/portfolio_env.py`)
```python
class PortfolioEnvironment(gym.Env):
    """
    OpenAI Gym environment for portfolio management
    
    State: [weights, returns, volatility, correlations, VaR, ...]
    Action: Target portfolio weights [w1, w2, ..., wn]
    Reward: Multi-objective (Sharpe, costs, risk violations, ...)
    """
    
    def __init__(self, data, config):
        self.data = data
        self.config = config
        
        # State space: 137-dim continuous
        self.observation_space = gym.spaces.Box(
            low=-np.inf, high=np.inf, shape=(137,)
        )
        
        # Action space: n_assets-dim continuous [0, 1]
        self.action_space = gym.spaces.Box(
            low=0.0, high=1.0, shape=(n_assets,)
        )
    
    def reset(self):
        """Reset to initial state"""
        self.current_step = 0
        self.portfolio_weights = np.array([1/n_assets] * n_assets)
        return self._get_state()
    
    def step(self, action):
        """Execute action, return next state and reward"""
        # Action = target weights
        new_weights = self._normalize_weights(action)
        
        # Calculate reward
        reward = self._calculate_reward(self.portfolio_weights, new_weights)
        
        # Update state
        self.portfolio_weights = new_weights
        self.current_step += 1
        
        # Check termination
        done = self.current_step >= self.max_steps
        
        state = self._get_state()
        info = self._get_info()
        
        return state, reward, done, info
    
    def _calculate_reward(self, old_weights, new_weights):
        """Multi-objective reward function"""
        reward = 0.0
        
        # Sharpe improvement
        if self.config.reward_sharpe:
            sharpe_delta = self._calculate_sharpe_delta(old_weights, new_weights)
            reward += self.config.reward_sharpe_weight * sharpe_delta
        
        # Transaction costs
        if self.config.reward_transaction_cost:
            turnover = np.sum(np.abs(new_weights - old_weights))
            cost = turnover * self.config.transaction_cost_bps / 10000
            reward += self.config.reward_transaction_cost_weight * cost
        
        # VaR violation
        if self.config.reward_var_violation:
            var = self._calculate_var(new_weights)
            if var > self.config.max_var:
                reward += self.config.reward_var_violation_weight
        
        # ... other reward components
        
        return reward
```

#### 3. **PPOAgent** (`actor_critic/ppo_agent.py`)
```python
class PPOAgent:
    """
    Proximal Policy Optimization agent
    
    Based on: "Proximal Policy Optimization Algorithms" (Schulman et al., 2017)
    """
    
    def __init__(self, state_dim, action_dim, config):
        self.actor = ActorNetwork(state_dim, action_dim, config)
        self.critic = CriticNetwork(state_dim, config)
        self.old_policy = copy.deepcopy(self.actor)
        
        self.optimizer_actor = torch.optim.Adam(
            self.actor.parameters(), lr=config.learning_rate
        )
        self.optimizer_critic = torch.optim.Adam(
            self.critic.parameters(), lr=config.learning_rate
        )
        
        self.replay_buffer = ReplayBuffer(config.buffer_size)
        self.config = config
    
    def select_action(self, state, deterministic=False):
        """Select action using current policy"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        
        with torch.no_grad():
            action, log_prob = self.actor.sample(state_tensor, deterministic)
        
        return action.cpu().numpy()[0]
    
    def update(self):
        """PPO update step"""
        # Sample minibatch from replay buffer
        states, actions, rewards, next_states, dones = self.replay_buffer.sample(
            self.config.batch_size
        )
        
        # Calculate advantages using GAE
        advantages = self._calculate_gae(states, rewards, next_states, dones)
        
        # PPO policy update
        for _ in range(self.config.ppo_epochs):
            # Get current policy probabilities
            _, log_probs_new = self.actor.evaluate(states, actions)
            
            # Get old policy probabilities
            with torch.no_grad():
                _, log_probs_old = self.old_policy.evaluate(states, actions)
            
            # Calculate ratio and clipped surrogate
            ratio = torch.exp(log_probs_new - log_probs_old)
            surr1 = ratio * advantages
            surr2 = torch.clamp(
                ratio, 
                1 - self.config.ppo_clip_epsilon, 
                1 + self.config.ppo_clip_epsilon
            ) * advantages
            
            # Actor loss
            actor_loss = -torch.min(surr1, surr2).mean()
            
            # Critic loss
            values = self.critic(states)
            critic_loss = F.mse_loss(values, rewards + self.config.gamma * next_values)
            
            # Entropy bonus
            entropy = self.actor.entropy(states).mean()
            
            # Total loss
            total_loss = (
                actor_loss + 
                self.config.ppo_value_coef * critic_loss -
                self.config.ppo_entropy_coef * entropy
            )
            
            # Update
            self.optimizer_actor.zero_grad()
            self.optimizer_critic.zero_grad()
            total_loss.backward()
            self.optimizer_actor.step()
            self.optimizer_critic.step()
        
        # Update old policy
        self.old_policy.load_state_dict(self.actor.state_dict())
```

---

## ğŸ”— Integration with Trading Intelligence

### **Modified Portfolio Tab Structure**
```
Trading Intelligence (Level 1)
â”œâ”€â”€ Portfolio (Level 2)
â”‚   â”œâ”€â”€ Portfolio Optimization (Level 3)
â”‚   â”œâ”€â”€ Transaction Costs (Level 3)
â”‚   â”œâ”€â”€ Risk Management (Level 3)
â”‚   â”œâ”€â”€ Correlation (Level 3)
â”‚   â”œâ”€â”€ Rebalancing (Level 3)
â”‚   â””â”€â”€ Performance (Level 3)
â”œâ”€â”€ Signals (Level 2)
â””â”€â”€ RL Agent (Level 2) â† NEW
    â”œâ”€â”€ Agent Configuration (Level 3)
    â”œâ”€â”€ Training Settings (Level 3)
    â”œâ”€â”€ State & Action Space (Level 3)
    â”œâ”€â”€ Reward Function (Level 3)
    â”œâ”€â”€ Training Progress (Level 3)
    â””â”€â”€ Deployment & Testing (Level 3)
```

### **Bridge: RL â†” Riskfolio â†” Trading Engine**
```python
class IntelligentPortfolioManager:
    """
    Master controller integrating:
    1. Riskfolio Optimizer (analytical)
    2. RL Agent (learned)
    3. Trading Engine (execution)
    """
    
    def __init__(self, config):
        self.riskfolio = PortfolioOptimizer(...)
        self.rl_manager = RLPortfolioManager(...) if config.enable_rl else None
        self.trading_engine = AutomatedTradingEngine(...)
    
    def get_optimal_weights(self, market_data):
        """Get portfolio weights from both systems"""
        
        # 1. Riskfolio analytical optimization
        riskfolio_weights = self.riskfolio.optimize(market_data)
        
        # 2. RL agent learned policy
        if self.rl_manager and self.rl_manager.enabled:
            state = self._prepare_state(market_data)
            rl_weights = self.rl_manager.get_target_weights(state)
            
            # Combine based on deployment mode
            if self.config.deployment_mode == 'RL Only':
                final_weights = rl_weights
            elif self.config.deployment_mode == 'RL + Riskfolio Hybrid':
                # Weighted average
                final_weights = (
                    0.6 * rl_weights + 
                    0.4 * riskfolio_weights
                )
            elif self.config.deployment_mode == 'RL Advisory':
                # Riskfolio decides, RL suggests
                final_weights = riskfolio_weights
                # Log RL suggestion for analysis
                self._log_rl_suggestion(rl_weights)
        else:
            final_weights = riskfolio_weights
        
        return final_weights
```

---

## ğŸ“Š Summary

**Total Implementation**:
- **New Module**: `src/forex_diffusion/rl/` (~5,000 lines)
- **New UI Tab**: `rl_config_tab.py` (~2,000 lines)
- **6 Level-3 Sub-Tabs**: Agent Config, Training, State/Action, Reward, Progress, Deployment
- **Total Widgets**: ~120 widgets (settings + live monitoring)
- **i18n Tooltips**: ~120 professional tooltips
- **Backend Classes**: 10 new classes (Agent, Environment, Networks, Trainer, ...)

**Key Features**:
âœ… **4 RL Algorithms**: PPO, SAC, A3C, TD3
âœ… **Customizable State Space**: 137-dim default, extendable
âœ… **Multi-Objective Rewards**: 9 components, fully configurable
âœ… **Live Training Monitoring**: Real-time plots, TensorBoard
âœ… **Comprehensive Backtesting**: Compare RL vs Riskfolio vs baselines
âœ… **Production Deployment**: Safety limits, monitoring, emergency stops
âœ… **Hybrid Modes**: RL-only, RL+Riskfolio, RL-advisory
âœ… **Fully Integrated**: Works with existing Portfolio Tab and Trading Engine

---

**Pronto per implementazione?**

**A)** Fase 1: RL Backend (PPO agent + Environment) - 2 settimane
**B)** Fase 2: RL UI Tab (120 widgets + tooltips) - 1 settimana
**C)** Fase 3: Training Infrastructure (TensorBoard, checkpoints) - 1 settimana
**D)** Fase 4: Integration with Portfolio Tab - 1 settimana
**E)** Fase 5: Testing & Validation - 1 settimana

**TOTALE: 6 settimane per sistema RL completo**

Quale fase vuoi iniziare?
