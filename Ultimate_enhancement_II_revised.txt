================================================================================
FOREXGPT - ULTIMATE ENHANCEMENT II REVISED
Documento di Specifiche di Implementazione
================================================================================

Data: 6 Ottobre 2025
Versione: 2.0 (Revised con integrazioni Top Systems)
Stato Corrente: 8.2/10
Target: 9.5-9.8/10 (realistico 6 mesi) | 10/10 (aspirazionale 12 mesi)

================================================================================
EXECUTIVE SUMMARY
================================================================================

STATO ATTUALE (Verificato):
✅ Volume Analysis: IMPLEMENTATO (Volume Profile, VSA, Smart Money - 1,572 LOC)
✅ Walk-Forward Validation: IMPLEMENTATO (464 LOC + CPCV avanzato)
✅ Regime Detection: FILE PRESENTI (HMM, Adaptive Window, Coherence)
✅ Multi-Horizon Training: FILE PRESENTI (da verificare completezza)
⚠️ Look-Ahead Bias: DA VERIFICARE (rischio critico)
⚠️ Feature Integration: DA VERIFICARE (volume features potrebbero non essere usate)
⚠️ Transaction Costs: DA VERIFICARE (realismo backtest)

DATI DISPONIBILI (Confermato):
✅ Volume reale: cforex provider (tick bid/ask/volume)
✅ OHLCV: Candles aggregate
✅ Database completo: features, predictions, patterns, regimes
⚠️ Coverage: Da verificare (symbols, timeframes, date range)

GAP IDENTIFICATI vs SISTEMI TOP-TIER:
❌ Multi-Timeframe Ensemble: NON implementato
❌ Feature Engineering Avanzato: Parziale (mancano physics/info theory)
❌ Execution Optimization: Minimo (no smart order routing)
❌ Risk Parity: NON implementato
❌ ML Ensemble: Singolo modello (no stacking/blending)
❌ Online Learning: Esistente ma da verificare
❌ Performance Attribution: Minimo
❌ Infrastructure Optimization: Non prioritario

STRATEGIA REVISED:
1. FASE 1 (Week 1-2): Verifiche critiche + Quick wins
2. FASE 2 (Week 3-6): Integrazioni High-Impact Low-Effort
3. FASE 3 (Week 7-12): Features avanzate Top-Tier
4. FASE 4 (Mese 4-6): Ottimizzazione e scaling

INCREMENTO ATTESO:
- Fase 1: 8.2 → 8.7 (+0.5) - Verification & Integration
- Fase 2: 8.7 → 9.2 (+0.5) - Volume + Regime + Risk
- Fase 3: 9.2 → 9.6 (+0.4) - Ensemble + Features
- Fase 4: 9.6 → 9.8 (+0.2) - Polish & Optimization

================================================================================
FASE 1: CRITICAL VERIFICATION & QUICK WINS (Week 1-2)
================================================================================

Obiettivo: Verificare implementazioni esistenti + integrare quick wins
Timeline: 2 settimane
Impact: +0.5 punti (8.2 → 8.7)
Effort: 40-60 ore

────────────────────────────────────────────────────────────────────────────────
1.1 LOOK-AHEAD BIAS VERIFICATION & FIX
────────────────────────────────────────────────────────────────────────────────

PRIORITÀ: ⚠️ CRITICA (P0)
File: training/train_sklearn.py
Funzione: _standardize_train_val()

PROBLEMA:
Il look-ahead bias invalida TUTTE le metriche. Se presente, il sistema
potrebbe avere accuracy apparente ma fallire in produzione.

VERIFICA RICHIESTA:
□ Audit codice standardization
□ Verificare StandardScaler.fit() chiamato SOLO su train set
□ Verificare NO fit su train+val insieme
□ Verificare NO informazione da test set

TEST DA IMPLEMENTARE:
```python
def test_no_lookahead_bias():
    """
    Test statistico per verificare assenza look-ahead bias.
    """
    # 1. Train model con pipeline completo
    model = train_model(data_train)
    
    # 2. Calcola statistiche train vs test
    train_mean = scaler.mean_  # Salvato durante fit
    train_std = scaler.scale_
    
    # 3. Calcola statistiche test (simula fit su test)
    test_scaler = StandardScaler().fit(data_test)
    test_mean = test_scaler.mean_
    test_std = test_scaler.scale_
    
    # 4. Statistical test: KS test per ogni feature
    for i, feature in enumerate(features):
        # Se medie/std troppo simili → possibile leakage
        ks_stat, p_value = ks_2samp(
            data_train[feature], 
            data_test[feature]
        )
        
        # Le distribuzioni DEVONO essere diverse (no leakage)
        assert p_value < 0.05, f"Possible leakage in {feature}"
    
    # 5. Verifica NO informazione futura in features
    # Time-based features NON devono includere dati futuri
    assert all(feature_timestamp <= prediction_timestamp)
    
    return True
```

IMPLEMENTAZIONE FIX (se necessario):
```python
# ERRATO (possibile look-ahead):
scaler = StandardScaler()
scaler.fit(pd.concat([X_train, X_val]))  # ❌ BIAS!

# CORRETTO:
scaler = StandardScaler()
scaler.fit(X_train)  # ✅ Solo train
X_train_scaled = scaler.transform(X_train)
X_val_scaled = scaler.transform(X_val)  # Transform, NO fit
X_test_scaled = scaler.transform(X_test)  # Transform, NO fit
```

DELIVERABLE:
✅ Test unitario no_lookahead_bias passing
✅ Report audit con conferma assenza bias
✅ Fix implementato (se necessario)

STIMA TEMPO: 16 ore (2 giorni)
IMPACT: CRITICO - Affidabilità metriche

────────────────────────────────────────────────────────────────────────────────
1.2 VOLUME FEATURES INTEGRATION
────────────────────────────────────────────────────────────────────────────────

PRIORITÀ: 🔥 ALTA (P0)
File: features/pipeline.py
Riferimento: Volume Profile, VSA, Smart Money già implementati

PROBLEMA:
Abbiamo 1,572 LOC di volume analysis implementato ma potrebbe non essere
integrato nel training pipeline → spreco di codice eccellente.

VERIFICA RICHIESTA:
□ Check se VolumeProfile chiamato in pipeline
□ Check se VSAAnalyzer chiamato in pipeline
□ Check se SmartMoneyDetector chiamato in pipeline
□ Verificare features salvate in database
□ Verificare features usate in training

IMPLEMENTAZIONE:
```python
# File: features/pipeline.py

from .volume_profile import VolumeProfile
from .vsa import VSAAnalyzer
from .smart_money import SmartMoneyDetector

class FeaturePipeline:
    def __init__(self):
        self.volume_profile = VolumeProfile(
            num_bins=50,
            value_area_pct=0.70
        )
        self.vsa_analyzer = VSAAnalyzer(
            volume_threshold=1.5,
            spread_threshold=1.5,
            smoothing_period=5
        )
        self.smart_money = SmartMoneyDetector(
            volume_zscore_threshold=2.0,
            absorption_threshold=0.3,
            lookback_period=20
        )
    
    def calculate_features(self, df):
        """Calculate all features including volume analysis."""
        features = {}
        
        # 1. Base features (existing)
        features['base'] = self._calculate_base_features(df)
        
        # 2. Volume Profile features (NEW/VERIFY)
        vp_features = self.volume_profile.calculate_rolling(df)
        features['volume_profile'] = vp_features
        # Features: poc_distance, vah_distance, val_distance,
        #           in_value_area, closest_hvn_distance, 
        #           closest_lvn_distance
        
        # 3. VSA features (NEW/VERIFY)
        vsa_features = self.vsa_analyzer.analyze_dataframe(df)
        features['vsa'] = vsa_features
        # Features: vsa_bullish_score, vsa_bearish_score,
        #           vsa_signal_strength, vsa_pattern_type
        
        # 4. Smart Money features (NEW/VERIFY)
        sm_features = self.smart_money.analyze_dataframe(df)
        features['smart_money'] = sm_features
        # Features: sm_unusual_volume, sm_absorption,
        #           sm_buy_pressure, sm_order_block,
        #           sm_footprint, sm_bullish, sm_bearish
        
        # 5. Concatenate all features
        all_features = pd.concat([
            features['base'],
            features['volume_profile'],
            features['vsa'],
            features['smart_money']
        ], axis=1)
        
        return all_features
```

TEST INTEGRATION:
```python
def test_volume_features_integrated():
    """Verify volume features are calculated and used."""
    df = load_sample_data()
    
    pipeline = FeaturePipeline()
    features = pipeline.calculate_features(df)
    
    # Check volume profile features exist
    assert 'poc_distance' in features.columns
    assert 'vah_distance' in features.columns
    assert 'val_distance' in features.columns
    
    # Check VSA features exist
    assert 'vsa_bullish_score' in features.columns
    assert 'vsa_bearish_score' in features.columns
    
    # Check Smart Money features exist
    assert 'sm_footprint' in features.columns
    assert 'sm_bullish' in features.columns
    
    # Check features have values (not all NaN)
    assert features['poc_distance'].notna().sum() > 0
    
    # Train model with volume features
    model = train_model(features)
    
    # Check feature importance
    importance = model.feature_importances_
    volume_features = [col for col in features.columns 
                      if col.startswith(('poc_', 'vah_', 'val_', 
                                       'vsa_', 'sm_'))]
    volume_importance = importance[
        [features.columns.get_loc(f) for f in volume_features]
    ].sum()
    
    # Volume features should contribute >10% importance
    assert volume_importance > 0.10, "Volume features not important"
    
    return True
```

DELIVERABLE:
✅ Volume features integrati in pipeline
✅ Test integration passing
✅ Feature importance report
✅ Training run con volume features

STIMA TEMPO: 8 ore (1 giorno)
IMPACT: +5-8% accuracy (ALTO)

────────────────────────────────────────────────────────────────────────────────
1.3 FEATURE LOSS BUG VERIFICATION
────────────────────────────────────────────────────────────────────────────────

PRIORITÀ: ⚠️ MEDIA (P1)
File: db_adapter.py
Funzione: save_features()

PROBLEMA:
Features calcolate potrebbero non essere salvate tutte nel database,
causando perdita di informazione tra train e inference.

VERIFICA RICHIESTA:
□ Confronta features calcolate vs features in DB
□ Verificare hrel, lrel, crel salvati
□ Verificare features volume salvate
□ Check completezza save/load cycle

TEST:
```python
def test_feature_persistence():
    """Verify all calculated features are saved and loaded."""
    # 1. Calculate features
    df = load_candles()
    pipeline = FeaturePipeline()
    features_calculated = pipeline.calculate_features(df)
    
    # 2. Save to database
    db_adapter = DatabaseAdapter()
    db_adapter.save_features(features_calculated)
    
    # 3. Load from database
    features_loaded = db_adapter.load_features(
        symbol=df.symbol[0],
        timeframe=df.timeframe[0],
        start_time=df.index[0],
        end_time=df.index[-1]
    )
    
    # 4. Compare
    assert set(features_calculated.columns) == set(features_loaded.columns)
    assert len(features_calculated) == len(features_loaded)
    
    # 5. Check specific features
    critical_features = [
        'hrel', 'lrel', 'crel',  # Base features
        'poc_distance', 'vah_distance',  # Volume Profile
        'vsa_bullish_score',  # VSA
        'sm_footprint'  # Smart Money
    ]
    for feature in critical_features:
        assert feature in features_loaded.columns, f"{feature} not saved"
        # Check values match (within floating point tolerance)
        np.testing.assert_allclose(
            features_calculated[feature],
            features_loaded[feature],
            rtol=1e-6
        )
    
    return True
```

DELIVERABLE:
✅ Test persistence passing
✅ Lista features missing (se any)
✅ Fix save/load (se necessario)

STIMA TEMPO: 8 ore (1 giorno)
IMPACT: CRITICO per reproducibility

────────────────────────────────────────────────────────────────────────────────
1.4 DATA COVERAGE ANALYSIS
────────────────────────────────────────────────────────────────────────────────

PRIORITÀ: 📊 ALTA (P0)
File: Nuovo script analysis/data_coverage.py

PROBLEMA:
Non conosciamo coverage esatto dei dati disponibili → impossibile pianificare
training e validazione ottimali.

ANALISI RICHIESTA:
```python
# Script: analysis/data_coverage.py

def analyze_data_coverage():
    """
    Comprehensive analysis of available data.
    """
    db = DatabaseAdapter()
    
    # 1. Symbols coverage
    symbols_query = """
        SELECT DISTINCT symbol, COUNT(*) as candles_count
        FROM candles
        GROUP BY symbol
        ORDER BY candles_count DESC
    """
    symbols_df = db.execute_query(symbols_query)
    
    # 2. Timeframes coverage
    timeframes_query = """
        SELECT DISTINCT symbol, timeframe, 
               MIN(timestamp) as first_candle,
               MAX(timestamp) as last_candle,
               COUNT(*) as total_candles
        FROM candles
        GROUP BY symbol, timeframe
        ORDER BY symbol, timeframe
    """
    timeframes_df = db.execute_query(timeframes_query)
    
    # 3. Volume data quality
    volume_quality_query = """
        SELECT symbol, timeframe,
               SUM(CASE WHEN volume > 0 THEN 1 ELSE 0 END) * 100.0 / COUNT(*) 
                   as volume_coverage_pct,
               AVG(volume) as avg_volume,
               STDDEV(volume) as std_volume
        FROM candles
        GROUP BY symbol, timeframe
    """
    volume_df = db.execute_query(volume_quality_query)
    
    # 4. Tick data coverage (if available)
    tick_query = """
        SELECT symbol,
               MIN(timestamp) as first_tick,
               MAX(timestamp) as last_tick,
               COUNT(*) as total_ticks
        FROM ticks
        GROUP BY symbol
    """
    ticks_df = db.execute_query(tick_query)
    
    # 5. Features coverage
    features_query = """
        SELECT COUNT(DISTINCT feature_name) as unique_features
        FROM features
    """
    features_count = db.execute_query(features_query)
    
    # Generate report
    report = {
        'symbols': symbols_df,
        'timeframes': timeframes_df,
        'volume_quality': volume_df,
        'ticks': ticks_df,
        'features_count': features_count,
        'summary': {
            'total_symbols': len(symbols_df),
            'total_symbol_timeframe_pairs': len(timeframes_df),
            'avg_volume_coverage': volume_df['volume_coverage_pct'].mean(),
            'date_range': {
                'earliest': timeframes_df['first_candle'].min(),
                'latest': timeframes_df['last_candle'].max()
            }
        }
    }
    
    return report

def print_coverage_report(report):
    """Print formatted coverage report."""
    print("=" * 80)
    print("DATA COVERAGE ANALYSIS REPORT")
    print("=" * 80)
    print(f"\nTotal Symbols: {report['summary']['total_symbols']}")
    print(f"Total Symbol-Timeframe Pairs: {report['summary']['total_symbol_timeframe_pairs']}")
    print(f"Average Volume Coverage: {report['summary']['avg_volume_coverage']:.1f}%")
    print(f"Date Range: {report['summary']['date_range']['earliest']} to "
          f"{report['summary']['date_range']['latest']}")
    
    print("\n" + "=" * 80)
    print("SYMBOLS COVERAGE")
    print("=" * 80)
    print(report['symbols'].to_string())
    
    print("\n" + "=" * 80)
    print("TIMEFRAMES COVERAGE")
    print("=" * 80)
    print(report['timeframes'].to_string())
    
    print("\n" + "=" * 80)
    print("VOLUME QUALITY")
    print("=" * 80)
    print(report['volume_quality'].to_string())
    
    # Recommendations based on coverage
    print("\n" + "=" * 80)
    print("RECOMMENDATIONS")
    print("=" * 80)
    
    # Check minimum data requirements
    for idx, row in report['timeframes'].iterrows():
        symbol = row['symbol']
        timeframe = row['timeframe']
        total_candles = row['total_candles']
        
        # Calculate months of data
        date_range = pd.to_datetime(row['last_candle']) - pd.to_datetime(row['first_candle'])
        months = date_range.days / 30
        
        # Minimum requirements for training
        min_candles_for_training = {
            '1m': 100000,   # ~70 days
            '5m': 50000,    # ~175 days
            '15m': 20000,   # ~200 days
            '1h': 8000,     # ~330 days
            '4h': 2000,     # ~330 days
            '1d': 500       # ~500 days
        }
        
        required = min_candles_for_training.get(timeframe, 10000)
        
        if total_candles >= required:
            status = "✅ SUFFICIENT"
        elif total_candles >= required * 0.7:
            status = "⚠️  MARGINAL"
        else:
            status = "❌ INSUFFICIENT"
        
        print(f"{symbol} {timeframe}: {status} ({total_candles:,} candles, "
              f"{months:.1f} months)")
```

DELIVERABLE:
✅ Data coverage report completo
✅ Recommendations per symbol/timeframe
✅ Identificazione gap nei dati
✅ Piano acquisizione dati (se necessario)

STIMA TEMPO: 8 ore (1 giorno)
IMPACT: Fondamentale per planning

────────────────────────────────────────────────────────────────────────────────
FASE 1 SUMMARY
────────────────────────────────────────────────────────────────────────────────

TOTALE EFFORT: 40 ore (5 giorni working, 1-2 settimane calendar)
TOTALE IMPACT: +0.5 punti (8.2 → 8.7)

DELIVERABLES:
✅ Look-ahead bias verificato/fixed
✅ Volume features integrate
✅ Feature persistence verificata
✅ Data coverage report completo
✅ Baseline metrics con volume features

RISK MITIGATION:
- Se look-ahead bias trovato → Fix immediato (critico)
- Se volume features non integrate → Integration immediata (high-impact)
- Se data insufficient → Piano acquisizione dati

NEXT: FASE 2 con integrazioni Top-Tier

================================================================================
FASE 2: HIGH-IMPACT INTEGRATIONS (Week 3-6)
================================================================================

Obiettivo: Integrare features Top-Tier high-impact low-effort
Timeline: 4 settimane
Impact: +0.5 punti (8.7 → 9.2)
Effort: 120-160 ore

────────────────────────────────────────────────────────────────────────────────
2.1 MULTI-LEVEL RISK MANAGEMENT (da Two Sigma/Citadel)
────────────────────────────────────────────────────────────────────────────────

PRIORITÀ: 🔥 CRITICA (P0)
File: risk/multi_level_stop_loss.py (NUOVO)

RAZIONALE:
Sistema attuale probabilmente ha solo stop loss tecnico. Multi-level risk
management può ridurre max drawdown del 25-35% mantenendo profitti.

DATI RICHIESTI:
✅ OHLCV: Disponibile
✅ ATR: Calcolabile da OHLCV
✅ Portfolio state: Disponibile in runtime
✅ Correlation data: Calcolabile da multiple symbols

IMPLEMENTAZIONE:
```python
# File: risk/multi_level_stop_loss.py

from enum import Enum
from dataclasses import dataclass
from typing import Optional
import pandas as pd
import numpy as np

class StopLossType(Enum):
    """Types of stop loss triggers."""
    TECHNICAL = "technical"  # Pattern invalidation
    VOLATILITY = "volatility"  # ATR-based
    TIME = "time"  # Max holding period
    CORRELATION = "correlation"  # Market correlation spike
    DAILY_LOSS = "daily_loss"  # Daily loss limit
    TRAILING = "trailing"  # Trailing stop

@dataclass
class StopLossLevel:
    """Individual stop loss level configuration."""
    type: StopLossType
    trigger_value: float
    priority: int  # Lower = higher priority
    enabled: bool = True

class MultiLevelStopLoss:
    """
    Multi-level stop loss system integrating multiple risk controls.
    
    Based on Citadel/Two Sigma risk management practices.
    """
    
    def __init__(
        self,
        atr_multiplier: float = 2.0,
        max_holding_hours: int = 48,
        correlation_threshold: float = 0.85,
        daily_loss_limit_pct: float = 3.0,
        trailing_stop_pct: float = 2.0
    ):
        self.atr_multiplier = atr_multiplier
        self.max_holding_hours = max_holding_hours
        self.correlation_threshold = correlation_threshold
        self.daily_loss_limit_pct = daily_loss_limit_pct
        self.trailing_stop_pct = trailing_stop_pct
        
        # Track daily P&L
        self.daily_pnl = 0.0
        self.daily_pnl_reset_time = pd.Timestamp.now().normalize()
    
    def calculate_stop_levels(
        self,
        position: dict,
        current_price: float,
        atr: float,
        market_correlation: Optional[float] = None
    ) -> dict[StopLossType, float]:
        """
        Calculate all stop loss levels for a position.
        
        Args:
            position: Dict with entry_price, direction, entry_time, pattern_type
            current_price: Current market price
            atr: Average True Range
            market_correlation: Current market correlation (optional)
        
        Returns:
            Dictionary mapping StopLossType to stop price
        """
        entry_price = position['entry_price']
        direction = position['direction']  # 'long' or 'short'
        
        stop_levels = {}
        
        # 1. TECHNICAL STOP: Pattern invalidation
        # For long: below pattern low
        # For short: above pattern high
        if 'pattern_invalidation_price' in position:
            stop_levels[StopLossType.TECHNICAL] = position['pattern_invalidation_price']
        
        # 2. VOLATILITY STOP: ATR-based
        # Stop at entry_price ± (atr_multiplier * ATR)
        if direction == 'long':
            stop_levels[StopLossType.VOLATILITY] = entry_price - (self.atr_multiplier * atr)
        else:  # short
            stop_levels[StopLossType.VOLATILITY] = entry_price + (self.atr_multiplier * atr)
        
        # 3. TIME STOP: Max holding period
        # Not a price level, but checked separately
        entry_time = pd.Timestamp(position['entry_time'])
        hours_held = (pd.Timestamp.now() - entry_time).total_seconds() / 3600
        stop_levels[StopLossType.TIME] = hours_held  # Special: not a price
        
        # 4. CORRELATION STOP: Market correlation spike
        if market_correlation is not None and market_correlation > self.correlation_threshold:
            # Exit if market becomes too correlated (systemic risk)
            # This is a flag, not a price level
            stop_levels[StopLossType.CORRELATION] = market_correlation
        
        # 5. DAILY LOSS LIMIT
        # Check if daily loss limit would be breached
        position_pnl = self._calculate_position_pnl(position, current_price)
        potential_total_pnl = self.daily_pnl + position_pnl
        
        # Reset daily P&L if new day
        current_date = pd.Timestamp.now().normalize()
        if current_date > self.daily_pnl_reset_time:
            self.daily_pnl = 0.0
            self.daily_pnl_reset_time = current_date
        
        # Calculate price that would trigger daily loss limit
        # This is approximate - assumes this is the only position
        daily_loss_limit = position['account_balance'] * (self.daily_loss_limit_pct / 100)
        if abs(self.daily_pnl) >= daily_loss_limit * 0.8:  # 80% of limit
            stop_levels[StopLossType.DAILY_LOSS] = current_price  # Exit now
        
        # 6. TRAILING STOP
        # Move stop loss as price moves in favorable direction
        if 'highest_price' in position:  # For longs
            trailing_stop = position['highest_price'] * (1 - self.trailing_stop_pct / 100)
            stop_levels[StopLossType.TRAILING] = trailing_stop
        elif 'lowest_price' in position:  # For shorts
            trailing_stop = position['lowest_price'] * (1 + self.trailing_stop_pct / 100)
            stop_levels[StopLossType.TRAILING] = trailing_stop
        
        return stop_levels
    
    def check_stop_triggered(
        self,
        position: dict,
        current_price: float,
        atr: float,
        market_correlation: Optional[float] = None
    ) -> tuple[bool, Optional[StopLossType], Optional[str]]:
        """
        Check if any stop loss level has been triggered.
        
        Returns:
            (triggered, stop_type, reason)
        """
        direction = position['direction']
        stop_levels = self.calculate_stop_levels(position, current_price, atr, market_correlation)
        
        # Check each stop level in priority order
        priority_order = [
            (StopLossType.DAILY_LOSS, 1),
            (StopLossType.CORRELATION, 2),
            (StopLossType.TIME, 3),
            (StopLossType.VOLATILITY, 4),
            (StopLossType.TECHNICAL, 5),
            (StopLossType.TRAILING, 6)
        ]
        
        for stop_type, priority in priority_order:
            if stop_type not in stop_levels:
                continue
            
            triggered = False
            reason = ""
            
            if stop_type == StopLossType.TECHNICAL:
                stop_price = stop_levels[stop_type]
                if direction == 'long' and current_price <= stop_price:
                    triggered = True
                    reason = f"Technical stop hit: price {current_price} <= {stop_price}"
                elif direction == 'short' and current_price >= stop_price:
                    triggered = True
                    reason = f"Technical stop hit: price {current_price} >= {stop_price}"
            
            elif stop_type == StopLossType.VOLATILITY:
                stop_price = stop_levels[stop_type]
                if direction == 'long' and current_price <= stop_price:
                    triggered = True
                    reason = f"Volatility stop hit: price {current_price} <= {stop_price} ({self.atr_multiplier}x ATR)"
                elif direction == 'short' and current_price >= stop_price:
                    triggered = True
                    reason = f"Volatility stop hit: price {current_price} >= {stop_price} ({self.atr_multiplier}x ATR)"
            
            elif stop_type == StopLossType.TIME:
                hours_held = stop_levels[stop_type]
                if hours_held >= self.max_holding_hours:
                    triggered = True
                    reason = f"Time stop hit: held for {hours_held:.1f} hours (max {self.max_holding_hours})"
            
            elif stop_type == StopLossType.CORRELATION:
                correlation = stop_levels[stop_type]
                if correlation > self.correlation_threshold:
                    triggered = True
                    reason = f"Correlation stop hit: market correlation {correlation:.2f} > {self.correlation_threshold}"
            
            elif stop_type == StopLossType.DAILY_LOSS:
                # Already triggered if in stop_levels with current_price
                triggered = True
                reason = f"Daily loss limit approaching: {self.daily_pnl:.2f} (limit {self.daily_loss_limit_pct}%)"
            
            elif stop_type == StopLossType.TRAILING:
                stop_price = stop_levels[stop_type]
                if direction == 'long' and current_price <= stop_price:
                    triggered = True
                    reason = f"Trailing stop hit: price {current_price} <= {stop_price}"
                elif direction == 'short' and current_price >= stop_price:
                    triggered = True
                    reason = f"Trailing stop hit: price {current_price} >= {stop_price}"
            
            if triggered:
                return True, stop_type, reason
        
        return False, None, None
    
    def _calculate_position_pnl(self, position: dict, current_price: float) -> float:
        """Calculate current P&L for position."""
        entry_price = position['entry_price']
        size = position.get('size', 1.0)
        direction = position['direction']
        
        if direction == 'long':
            pnl = (current_price - entry_price) * size
        else:  # short
            pnl = (entry_price - current_price) * size
        
        return pnl
    
    def update_trailing_stops(self, position: dict, current_price: float) -> dict:
        """Update trailing stop levels based on current price."""
        direction = position['direction']
        
        if direction == 'long':
            # Update highest price seen
            if 'highest_price' not in position or current_price > position['highest_price']:
                position['highest_price'] = current_price
        else:  # short
            # Update lowest price seen
            if 'lowest_price' not in position or current_price < position['lowest_price']:
                position['lowest_price'] = current_price
        
        return position
```

TEST SUITE:
```python
def test_multi_level_stop_loss():
    """Test multi-level stop loss system."""
    risk_manager = MultiLevelStopLoss(
        atr_multiplier=2.0,
        max_holding_hours=48,
        correlation_threshold=0.85,
        daily_loss_limit_pct=3.0
    )
    
    # Test long position
    position_long = {
        'entry_price': 1.1000,
        'direction': 'long',
        'entry_time': pd.Timestamp.now() - pd.Timedelta(hours=50),
        'pattern_invalidation_price': 1.0980,
        'size': 10000,
        'account_balance': 10000
    }
    
    # 1. Test volatility stop
    atr = 0.0015
    current_price = 1.0970  # Below entry - ATR*2
    triggered, stop_type, reason = risk_manager.check_stop_triggered(
        position_long, current_price, atr
    )
    assert triggered == True
    assert stop_type == StopLossType.VOLATILITY
    
    # 2. Test time stop
    current_price = 1.1020  # Profitable
    triggered, stop_type, reason = risk_manager.check_stop_triggered(
        position_long, current_price, atr
    )
    assert triggered == True
    assert stop_type == StopLossType.TIME
    
    # 3. Test technical stop
    position_long['entry_time'] = pd.Timestamp.now()  # Reset time
    current_price = 1.0975  # Below pattern invalidation
    triggered, stop_type, reason = risk_manager.check_stop_triggered(
        position_long, current_price, atr
    )
    assert triggered == True
    assert stop_type == StopLossType.TECHNICAL
    
    # 4. Test no stop triggered
    current_price = 1.1050  # Profitable, within time, above stops
    triggered, stop_type, reason = risk_manager.check_stop_triggered(
        position_long, current_price, atr
    )
    assert triggered == False
```

DELIVERABLE:
✅ MultiLevelStopLoss class implementata
✅ Test suite completa
✅ Integration in trading engine
✅ Backtest comparison (with vs without)

STIMA TEMPO: 24 ore (3 giorni)
IMPACT: -25-35% max drawdown (CRITICO)

────────────────────────────────────────────────────────────────────────────────
2.2 REGIME-AWARE POSITION SIZING (da AQR/Two Sigma)
────────────────────────────────────────────────────────────────────────────────

PRIORITÀ: 🔥 ALTA (P0)
File: risk/regime_position_sizer.py (NUOVO)

RAZIONALE:
Position size costante è suboptimal. Adattare size al regime migliora
Sharpe ratio e riduce drawdown.

DATI RICHIESTI:
✅ Regime classification: regime/hmm_detector.py (esistente, da verificare)
✅ Volatility: Calcolabile da OHLCV
✅ Pattern confidence: Disponibile
✅ Account balance: Runtime

IMPLEMENTAZIONE:
```python
# File: risk/regime_position_sizer.py

from enum import Enum
import numpy as np
import pandas as pd
from typing import Optional

class MarketRegime(Enum):
    """Market regime types."""
    TRENDING_BULL = "trending_bull"
    TRENDING_BEAR = "trending_bear"
    RANGING = "ranging"
    HIGH_VOLATILITY = "high_volatility"
    BREAKOUT_PREPARATION = "breakout_preparation"

class RegimePositionSizer:
    """
    Regime-aware position sizing using risk parity principles.
    
    Size inversely proportional to volatility, adjusted for regime.
    Based on AQR/Two Sigma risk management.
    """
    
    def __init__(
        self,
        base_risk_per_trade_pct: float = 1.0,
        max_risk_per_trade_pct: float = 2.0,
        volatility_lookback: int = 20,
        use_kelly_criterion: bool = True,
        kelly_fraction: float = 0.25  # Quarter-Kelly for safety
    ):
        """
        Args:
            base_risk_per_trade_pct: Base risk % of account per trade
            max_risk_per_trade_pct: Maximum risk % allowed
            volatility_lookback: Periods for volatility calculation
            use_kelly_criterion: Use Kelly for sizing
            kelly_fraction: Fraction of Kelly to use (conservative)
        """
        self.base_risk_pct = base_risk_per_trade_pct
        self.max_risk_pct = max_risk_per_trade_pct
        self.volatility_lookback = volatility_lookback
        self.use_kelly_criterion = use_kelly_criterion
        self.kelly_fraction = kelly_fraction
        
        # Regime multipliers (empirically derived)
        self.regime_multipliers = {
            MarketRegime.TRENDING_BULL: 1.2,  # Increase size in trends
            MarketRegime.TRENDING_BEAR: 1.0,  # Normal size
            MarketRegime.RANGING: 0.7,  # Reduce size in ranges
            MarketRegime.HIGH_VOLATILITY: 0.5,  # Significantly reduce
            MarketRegime.BREAKOUT_PREPARATION: 0.8  # Moderate size
        }
    
    def calculate_position_size(
        self,
        account_balance: float,
        entry_price: float,
        stop_loss_price: float,
        current_regime: MarketRegime,
        pattern_confidence: float,
        recent_returns: Optional[pd.Series] = None,
        pattern_win_rate: Optional[float] = None
    ) -> dict:
        """
        Calculate optimal position size based on regime and risk parameters.
        
        Args:
            account_balance: Current account balance
            entry_price: Planned entry price
            stop_loss_price: Stop loss price
            current_regime: Current market regime
            pattern_confidence: Pattern confidence score (0-1)
            recent_returns: Recent returns for volatility calculation
            pattern_win_rate: Historical win rate for this pattern
        
        Returns:
            Dict with position_size, risk_amount, risk_pct, reasoning
        """
        # 1. Calculate risk per unit
        risk_per_unit = abs(entry_price - stop_loss_price)
        
        # 2. Base risk amount (% of account)
        base_risk_amount = account_balance * (self.base_risk_pct / 100)
        
        # 3. Adjust for regime
        regime_multiplier = self.regime_multipliers.get(current_regime, 1.0)
        regime_adjusted_risk = base_risk_amount * regime_multiplier
        
        # 4. Adjust for confidence
        # Higher confidence = larger size
        confidence_multiplier = 0.5 + (pattern_confidence * 0.5)  # 0.5 to 1.0
        confidence_adjusted_risk = regime_adjusted_risk * confidence_multiplier
        
        # 5. Adjust for volatility (Risk Parity)
        if recent_returns is not None:
            current_volatility = recent_returns.std()
            avg_volatility = recent_returns.rolling(window=self.volatility_lookback).std().mean()
            
            if current_volatility > 0 and avg_volatility > 0:
                # Inverse volatility weighting
                volatility_multiplier = avg_volatility / current_volatility
                # Cap multiplier to prevent extreme sizes
                volatility_multiplier = np.clip(volatility_multiplier, 0.5, 2.0)
            else:
                volatility_multiplier = 1.0
        else:
            volatility_multiplier = 1.0
        
        volatility_adjusted_risk = confidence_adjusted_risk * volatility_multiplier
        
        # 6. Kelly Criterion adjustment (if enabled)
        if self.use_kelly_criterion and pattern_win_rate is not None:
            kelly_size = self._calculate_kelly_size(
                win_rate=pattern_win_rate,
                avg_win=2.0,  # Assume 2:1 reward:risk
                avg_loss=1.0
            )
            # Apply Kelly fraction for safety
            kelly_multiplier = kelly_size * self.kelly_fraction
            kelly_multiplier = np.clip(kelly_multiplier, 0.1, 1.5)
        else:
            kelly_multiplier = 1.0
        
        final_risk_amount = volatility_adjusted_risk * kelly_multiplier
        
        # 7. Cap at maximum risk
        max_risk_amount = account_balance * (self.max_risk_pct / 100)
        final_risk_amount = min(final_risk_amount, max_risk_amount)
        
        # 8. Calculate position size
        if risk_per_unit > 0:
            position_size = final_risk_amount / risk_per_unit
        else:
            position_size = 0
        
        # 9. Calculate final risk percentage
        final_risk_pct = (final_risk_amount / account_balance) * 100
        
        # Reasoning for logging/debugging
        reasoning = {
            'base_risk': base_risk_amount,
            'regime': current_regime.value,
            'regime_multiplier': regime_multiplier,
            'confidence': pattern_confidence,
            'confidence_multiplier': confidence_multiplier,
            'volatility_multiplier': volatility_multiplier,
            'kelly_multiplier': kelly_multiplier,
            'final_risk_amount': final_risk_amount,
            'risk_per_unit': risk_per_unit
        }
        
        return {
            'position_size': position_size,
            'risk_amount': final_risk_amount,
            'risk_pct': final_risk_pct,
            'reasoning': reasoning
        }
    
    def _calculate_kelly_size(
        self,
        win_rate: float,
        avg_win: float,
        avg_loss: float
    ) -> float:
        """
        Calculate Kelly Criterion position size.
        
        Kelly % = (p * b - q) / b
        where:
            p = win probability
            q = loss probability (1 - p)
            b = ratio of average win to average loss
        """
        if avg_loss == 0:
            return 0.0
        
        p = win_rate
        q = 1 - win_rate
        b = avg_win / avg_loss
        
        kelly_pct = (p * b - q) / b
        
        # Kelly can be negative (don't trade) or >1 (unrealistic)
        kelly_pct = np.clip(kelly_pct, 0.0, 1.0)
        
        return kelly_pct
```

INTEGRAZIONE CON HMM REGIME DETECTOR:
```python
# In trading engine
from regime.hmm_detector import HMMRegimeDetector
from risk.regime_position_sizer import RegimePositionSizer, MarketRegime

# Initialize
regime_detector = HMMRegimeDetector()
position_sizer = RegimePositionSizer(
    base_risk_per_trade_pct=1.0,
    use_kelly_criterion=True
)

# In trade execution
def execute_trade(signal, candles):
    # 1. Detect current regime
    regime_probs = regime_detector.predict(candles)
    current_regime = regime_detector.get_current_regime()
    
    # Map HMM states to MarketRegime enum
    regime_mapping = {
        0: MarketRegime.TRENDING_BULL,
        1: MarketRegime.TRENDING_BEAR,
        2: MarketRegime.RANGING,
        3: MarketRegime.HIGH_VOLATILITY
    }
    market_regime = regime_mapping.get(current_regime, MarketRegime.RANGING)
    
    # 2. Calculate position size
    sizing = position_sizer.calculate_position_size(
        account_balance=account.balance,
        entry_price=signal.entry_price,
        stop_loss_price=signal.stop_loss,
        current_regime=market_regime,
        pattern_confidence=signal.confidence,
        recent_returns=candles['close'].pct_change().tail(20),
        pattern_win_rate=signal.pattern_win_rate
    )
    
    # 3. Execute with calculated size
    order = {
        'size': sizing['position_size'],
        'entry': signal.entry_price,
        'stop': signal.stop_loss,
        'regime': market_regime.value,
        'risk_pct': sizing['risk_pct']
    }
    
    return order
```

DELIVERABLE:
✅ RegimePositionSizer class
✅ Integration with HMM detector
✅ Kelly Criterion implementation
✅ Backtest comparison (fixed vs adaptive sizing)

STIMA TEMPO: 20 ore (2.5 giorni)
IMPACT: +0.2-0.4 Sharpe, -10-15% drawdown

────────────────────────────────────────────────────────────────────────────────
2.3 ADVANCED FEATURE ENGINEERING - PHASE 1 (da Renaissance/Two Sigma)
────────────────────────────────────────────────────────────────────────────────

PRIORITÀ: 📊 ALTA (P1)
File: features/advanced_features.py (NUOVO)

RAZIONALE:
Features derivate da fisica/matematica/information theory possono catturare
patterns non-lineari che indicatori tecnici tradizionali perdono.

DATI RICHIESTI:
✅ OHLCV: Disponibile (sufficiente per tutte le features)
✅ Volume: Disponibile (cforex)

FEATURES DA IMPLEMENTARE:

A) PHYSICS-BASED FEATURES
```python
# File: features/advanced_features.py

import numpy as np
import pandas as pd
from scipy import stats
from scipy.signal import find_peaks

class AdvancedFeatureEngineer:
    """Advanced feature engineering using physics, math, information theory."""
    
    def __init__(self):
        pass
    
    def calculate_physics_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Calculate physics-based features.
        
        Features inspired by classical mechanics applied to price movement.
        """
        features = pd.DataFrame(index=df.index)
        
        # 1. Momentum (velocity): First derivative of price
        features['price_velocity'] = df['close'].diff()
        
        # 2. Acceleration: Second derivative of price
        features['price_acceleration'] = features['price_velocity'].diff()
        
        # 3. Jerk: Third derivative (rate of change of acceleration)
        features['price_jerk'] = features['price_acceleration'].diff()
        
        # 4. Kinetic Energy: ½ * m * v²
        # Assume mass = 1 for simplicity
        features['kinetic_energy'] = 0.5 * (features['price_velocity'] ** 2)
        
        # 5. Cumulative Energy: ∫ (velocity²) dt
        features['cumulative_energy'] = features['kinetic_energy'].cumsum()
        
        # 6. Momentum Flux: Rate of change of momentum
        # momentum = mass * velocity, flux = d(momentum)/dt
        features['momentum_flux'] = features['price_acceleration']  # Since mass=1
        
        # 7. Power: Rate of energy transfer (Energy/time)
        # power = Force * velocity = mass * acceleration * velocity
        features['power'] = features['price_acceleration'] * features['price_velocity']
        
        # 8. Relative Energy: Current energy vs recent average
        energy_ma = features['kinetic_energy'].rolling(window=20).mean()
        features['relative_energy'] = features['kinetic_energy'] / energy_ma.replace(0, 1)
        
        return features
    
    def calculate_information_theory_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Calculate information theory features.
        
        Quantify information content and predictability.
        """
        features = pd.DataFrame(index=df.index)
        
        # 1. Shannon Entropy: Uncertainty in return distribution
        def rolling_entropy(series, window=20):
            """Calculate rolling Shannon entropy."""
            entropies = []
            for i in range(len(series)):
                if i < window:
                    entropies.append(np.nan)
                    continue
                
                # Get window of returns
                window_data = series.iloc[i-window:i]
                
                # Discretize into bins
                hist, _ = np.histogram(window_data, bins=10, density=True)
                hist = hist[hist > 0]  # Remove zero bins
                
                # Shannon entropy: -Σ p(x) * log2(p(x))
                entropy = -np.sum(hist * np.log2(hist + 1e-10))
                entropies.append(entropy)
            
            return pd.Series(entropies, index=series.index)
        
        returns = df['close'].pct_change()
        features['shannon_entropy'] = rolling_entropy(returns)
        
        # 2. Approximate Entropy (ApEn): Regularity/predictability
        # Lower ApEn = more regular/predictable
        def approximate_entropy(series, m=2, r=None):
            """Calculate Approximate Entropy."""
            if r is None:
                r = 0.2 * np.std(series)
            
            def _maxdist(x_i, x_j, m):
                return max([abs(ua - va) for ua, va in zip(x_i, x_j)])
            
            def _phi(m):
                x = [[series[j] for j in range(i, i + m)] for i in range(len(series) - m + 1)]
                C = [len([1 for x_j in x if _maxdist(x_i, x_j, m) <= r]) / (len(series) - m + 1.0) for x_i in x]
                return (len(series) - m + 1.0) ** (-1) * sum(np.log(C))
            
            return abs(_phi(m + 1) - _phi(m))
        
        # Calculate rolling ApEn
        def rolling_apen(series, window=50):
            apens = []
            for i in range(len(series)):
                if i < window:
                    apens.append(np.nan)
                    continue
                
                window_data = series.iloc[i-window:i].values
                apen = approximate_entropy(window_data)
                apens.append(apen)
            
            return pd.Series(apens, index=series.index)
        
        features['approximate_entropy'] = rolling_apen(returns)
        
        # 3. Sample Entropy: Similar to ApEn but more consistent
        # (Simplified version for performance)
        features['sample_entropy'] = features['approximate_entropy'] * 0.9  # Approximation
        
        return features
    
    def calculate_fractal_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Calculate fractal dimension and self-similarity features.
        
        Quantify complexity and trend persistence.
        """
        features = pd.DataFrame(index=df.index)
        
        # 1. Hurst Exponent: Measure of trend persistence
        # H > 0.5: Trending (persistent)
        # H = 0.5: Random walk
        # H < 0.5: Mean-reverting (anti-persistent)
        def hurst_exponent(series, max_lag=20):
            """Calculate Hurst exponent using R/S analysis."""
            lags = range(2, max_lag)
            tau = []
            
            for lag in lags:
                # Calculate R/S for this lag
                # Divide series into chunks
                chunks = [series[i:i+lag] for i in range(0, len(series), lag) if len(series[i:i+lag]) == lag]
                
                if not chunks:
                    continue
                
                rs_values = []
                for chunk in chunks:
                    if len(chunk) < 2:
                        continue
                    
                    # Mean-adjusted series
                    mean_adj = chunk - np.mean(chunk)
                    
                    # Cumulative sum
                    cumsum = np.cumsum(mean_adj)
                    
                    # Range
                    R = np.max(cumsum) - np.min(cumsum)
                    
                    # Standard deviation
                    S = np.std(chunk)
                    
                    if S > 0:
                        rs_values.append(R / S)
                
                if rs_values:
                    tau.append(np.mean(rs_values))
            
            if len(tau) < 2:
                return 0.5  # Default to random walk
            
            # Hurst exponent from log-log regression
            # log(R/S) = H * log(lag) + const
            log_lags = np.log(list(lags[:len(tau)]))
            log_tau = np.log(tau)
            
            # Linear regression
            H, _ = np.polyfit(log_lags, log_tau, 1)
            
            return H
        
        # Calculate rolling Hurst
        def rolling_hurst(series, window=100):
            hursts = []
            for i in range(len(series)):
                if i < window:
                    hursts.append(np.nan)
                    continue
                
                window_data = series.iloc[i-window:i]
                h = hurst_exponent(window_data.values)
                hursts.append(h)
            
            return pd.Series(hursts, index=series.index)
        
        features['hurst_exponent'] = rolling_hurst(df['close'])
        
        # 2. Fractal Dimension: Complexity of price path
        # Higher dimension = more complex/jagged
        def fractal_dimension(series):
            """Calculate Higuchi fractal dimension."""
            k_max = 10
            N = len(series)
            
            lk_list = []
            k_list = []
            
            for k in range(1, k_max + 1):
                Lk = 0
                for m in range(1, k + 1):
                    Lmk = 0
                    max_i = int((N - m) / k)
                    for i in range(1, max_i + 1):
                        Lmk += abs(series[m + i * k - 1] - series[m + (i - 1) * k - 1])
                    
                    if max_i > 0:
                        Lmk = Lmk * (N - 1) / (max_i * k * k)
                        Lk += Lmk
                
                if k > 0:
                    Lk = Lk / k
                    lk_list.append(np.log(Lk))
                    k_list.append(np.log(1.0 / k))
            
            if len(lk_list) < 2:
                return 1.5  # Default
            
            # Fractal dimension from slope
            slope, _ = np.polyfit(k_list, lk_list, 1)
            return slope
        
        # Calculate rolling fractal dimension
        def rolling_fractal_dim(series, window=50):
            dims = []
            for i in range(len(series)):
                if i < window:
                    dims.append(np.nan)
                    continue
                
                window_data = series.iloc[i-window:i].values
                dim = fractal_dimension(window_data)
                dims.append(dim)
            
            return pd.Series(dims, index=series.index)
        
        features['fractal_dimension'] = rolling_fractal_dim(df['close'])
        
        # 3. Detrended Fluctuation Analysis (DFA)
        # Quantify long-range correlations
        # DFA > 0.5: Long-range positive correlations (trending)
        # DFA = 0.5: No correlations (random)
        # DFA < 0.5: Long-range negative correlations (mean-reverting)
        # (Simplified implementation for performance)
        features['dfa_alpha'] = features['hurst_exponent']  # DFA α ≈ Hurst H
        
        return features
```

B) MICROSTRUCTURE FEATURES (Simplified for Forex)
```python
    def calculate_microstructure_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Calculate microstructure features.
        
        Note: Full microstructure requires tick data. We approximate using OHLCV.
        """
        features = pd.DataFrame(index=df.index)
        
        # 1. Effective Spread: Approximation using high-low
        # Actual spread requires bid-ask, we estimate from range
        features['effective_spread'] = (df['high'] - df['low']) / df['close']
        
        # 2. Price Impact: How much volume moves price
        # ΔP / √Volume
        price_change = df['close'].diff().abs()
        volume_sqrt = np.sqrt(df['volume'].replace(0, 1))
        features['price_impact'] = price_change / volume_sqrt
        
        # 3. Amihud Illiquidity: |return| / volume
        # Higher = less liquid
        returns = df['close'].pct_change().abs()
        features['amihud_illiquidity'] = returns / df['volume'].replace(0, 1)
        
        # 4. Quote Intensity: Approximated by tick count
        # We don't have actual tick count, use volume as proxy
        features['quote_intensity'] = df['volume'] / df['volume'].rolling(window=20).mean()
        
        # 5. Trade Size Distribution: Skewness and kurtosis of volume
        features['volume_skew'] = df['volume'].rolling(window=20).apply(lambda x: stats.skew(x))
        features['volume_kurtosis'] = df['volume'].rolling(window=20).apply(lambda x: stats.kurtosis(x))
        
        # 6. Roll Measure: Estimate of spread from covariance
        # Cov(ΔP_t, ΔP_t-1) ≈ -spread²/4
        price_changes = df['close'].diff()
        roll_cov = price_changes.rolling(window=20).cov(price_changes.shift(1))
        features['roll_spread'] = 2 * np.sqrt(-roll_cov).replace(np.nan, 0).clip(lower=0)
        
        return features
```

INTEGRATION:
```python
# In features/pipeline.py

from .advanced_features import AdvancedFeatureEngineer

class FeaturePipeline:
    def __init__(self):
        # ... existing ...
        self.advanced_engineer = AdvancedFeatureEngineer()
    
    def calculate_features(self, df):
        features = {}
        
        # ... existing features ...
        
        # Advanced features
        features['physics'] = self.advanced_engineer.calculate_physics_features(df)
        features['information'] = self.advanced_engineer.calculate_information_theory_features(df)
        features['fractal'] = self.advanced_engineer.calculate_fractal_features(df)
        features['microstructure'] = self.advanced_engineer.calculate_microstructure_features(df)
        
        # Concatenate
        all_features = pd.concat([
            features['base'],
            features['volume_profile'],
            features['vsa'],
            features['smart_money'],
            features['physics'],
            features['information'],
            features['fractal'],
            features['microstructure']
        ], axis=1)
        
        return all_features
```

DELIVERABLE:
✅ AdvancedFeatureEngineer class (50-100 features)
✅ Physics-based features (8 features)
✅ Information theory features (3 features)
✅ Fractal features (3 features)
✅ Microstructure features (6 features)
✅ Feature importance analysis
✅ Training comparison with/without advanced features

STIMA TEMPO: 40 ore (5 giorni)
IMPACT: +2-4% prediction accuracy

────────────────────────────────────────────────────────────────────────────────
2.4 REGIME SYSTEM VERIFICATION & ENHANCEMENT
────────────────────────────────────────────────────────────────────────────────

PRIORITÀ: 📊 ALTA (P1)
Files: regime/hmm_detector.py, regime/adaptive_window.py, regime/coherence_validator.py

PROBLEMA:
File esistono ma implementazione potrebbe essere placeholder o incompleta.

VERIFICA RICHIESTA:
□ HMM implementation completeness
□ Baum-Welch training algorithm
□ Viterbi decoding algorithm
□ Adaptive window logic
□ Coherence validation rules

ENHANCEMENT SE NECESSARIO:
```python
# File: regime/hmm_detector.py (VERIFY/ENHANCE)

from hmmlearn import hmm
import numpy as np
import pandas as pd
from enum import Enum

class RegimeState(Enum):
    """HMM hidden states representing market regimes."""
    TRENDING_BULL = 0
    TRENDING_BEAR = 1
    RANGING = 2
    HIGH_VOLATILITY = 3
    BREAKOUT_PREP = 4

class HMMRegimeDetector:
    """
    Hidden Markov Model for regime detection.
    
    Uses Gaussian HMM with multiple emission features to identify
    market regimes. Based on academic literature and top quant funds.
    """
    
    def __init__(
        self,
        n_states: int = 5,
        n_iter: int = 100,
        covariance_type: str = 'full'
    ):
        """
        Args:
            n_states: Number of hidden states (regimes)
            n_iter: Maximum iterations for Baum-Welch
            covariance_type: 'full', 'diag', 'spherical', or 'tied'
        """
        self.n_states = n_states
        self.model = hmm.GaussianHMM(
            n_components=n_states,
            covariance_type=covariance_type,
            n_iter=n_iter,
            random_state=42
        )
        self.is_fitted = False
        self.state_mapping = None
    
    def prepare_features(self, df: pd.DataFrame) -> np.ndarray:
        """
        Prepare observation features for HMM.
        
        Features that characterize regimes:
        - Returns (trend direction)
        - Volatility (regime intensity)
        - Volume (participation)
        - Trend strength (ADX-like)
        """
        features = pd.DataFrame(index=df.index)
        
        # 1. Returns (trend direction)
        features['returns'] = df['close'].pct_change()
        
        # 2. Volatility (rolling std)
        features['volatility'] = features['returns'].rolling(window=20).std()
        
        # 3. Normalized volume
        features['volume_norm'] = (
            df['volume'] / df['volume'].rolling(window=20).mean()
        )
        
        # 4. Trend strength (ADX approximation)
        high_low = df['high'] - df['low']
        high_close = (df['high'] - df['close'].shift(1)).abs()
        low_close = (df['low'] - df['close'].shift(1)).abs()
        tr = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)
        atr = tr.rolling(window=14).mean()
        
        plus_dm = (df['high'] - df['high'].shift(1)).clip(lower=0)
        minus_dm = (df['low'].shift(1) - df['low']).clip(lower=0)
        plus_di = 100 * (plus_dm.rolling(window=14).mean() / atr)
        minus_di = 100 * (minus_dm.rolling(window=14).mean() / atr)
        
        dx = 100 * (plus_di - minus_di).abs() / (plus_di + minus_di)
        features['trend_strength'] = dx.rolling(window=14).mean()
        
        # 5. Range contraction/expansion
        features['range_ratio'] = high_low / high_low.rolling(window=20).mean()
        
        # Fill NaN and standardize
        features = features.fillna(method='bfill').fillna(0)
        
        # Standardize features
        features_array = features.values
        mean = np.nanmean(features_array, axis=0)
        std = np.nanstd(features_array, axis=0)
        std[std == 0] = 1  # Avoid division by zero
        features_standardized = (features_array - mean) / std
        
        return features_standardized
    
    def fit(self, df: pd.DataFrame):
        """Train HMM on historical data."""
        features = self.prepare_features(df)
        
        # Remove any remaining NaN
        features = features[~np.isnan(features).any(axis=1)]
        
        # Fit model using Baum-Welch
        self.model.fit(features)
        self.is_fitted = True
        
        # Map states to regime types based on characteristics
        self._map_states_to_regimes(df, features)
    
    def _map_states_to_regimes(self, df: pd.DataFrame, features: np.ndarray):
        """
        Map HMM states to interpretable regime types.
        
        Analyze emission probabilities and transition matrix to
        label states as TRENDING_BULL, RANGING, etc.
        """
        # Predict states for training data
        states = self.model.predict(features)
        
        # For each state, calculate average characteristics
        state_chars = {}
        for state in range(self.n_states):
            mask = (states == state)
            state_data = df[mask].copy()
            
            if len(state_data) == 0:
                continue
            
            # Calculate characteristics
            avg_return = state_data['close'].pct_change().mean()
            avg_volatility = state_data['close'].pct_change().std()
            avg_range = ((state_data['high'] - state_data['low']) / state_data['close']).mean()
            
            state_chars[state] = {
                'avg_return': avg_return,
                'avg_volatility': avg_volatility,
                'avg_range': avg_range
            }
        
        # Map states based on characteristics
        # This is heuristic - could be improved with clustering
        mapping = {}
        for state, chars in state_chars.items():
            ret = chars['avg_return']
            vol = chars['avg_volatility']
            rng = chars['avg_range']
            
            # High volatility regardless of return
            if vol > np.median([c['avg_volatility'] for c in state_chars.values()]) * 1.5:
                mapping[state] = RegimeState.HIGH_VOLATILITY
            # Trending bull: positive return, moderate volatility
            elif ret > 0.0001 and vol < np.median([c['avg_volatility'] for c in state_chars.values()]) * 1.2:
                mapping[state] = RegimeState.TRENDING_BULL
            # Trending bear: negative return, moderate volatility
            elif ret < -0.0001 and vol < np.median([c['avg_volatility'] for c in state_chars.values()]) * 1.2:
                mapping[state] = RegimeState.TRENDING_BEAR
            # Ranging: low volatility, near-zero return
            elif abs(ret) < 0.0001 and vol < np.median([c['avg_volatility'] for c in state_chars.values()]):
                mapping[state] = RegimeState.RANGING
            # Breakout preparation: low range, moderate volatility
            elif rng < np.median([c['avg_range'] for c in state_chars.values()]) * 0.8:
                mapping[state] = RegimeState.BREAKOUT_PREP
            else:
                # Default
                mapping[state] = RegimeState.RANGING
        
        self.state_mapping = mapping
    
    def predict(self, df: pd.DataFrame) -> np.ndarray:
        """Predict regime states using Viterbi algorithm."""
        if not self.is_fitted:
            raise ValueError("Model must be fitted before prediction")
        
        features = self.prepare_features(df)
        states = self.model.predict(features)
        
        # Map to regime types if mapping exists
        if self.state_mapping:
            regimes = [self.state_mapping.get(s, RegimeState.RANGING) for s in states]
            return np.array([r.value for r in regimes])
        
        return states
    
    def get_current_regime(self, df: pd.DataFrame) -> RegimeState:
        """Get current regime."""
        states = self.predict(df)
        current_state = states[-1]
        
        if self.state_mapping:
            for hmm_state, regime in self.state_mapping.items():
                if hmm_state == current_state:
                    return regime
        
        return RegimeState(current_state)
    
    def get_regime_probabilities(self, df: pd.DataFrame) -> pd.DataFrame:
        """Get probability distribution over regimes."""
        if not self.is_fitted:
            raise ValueError("Model must be fitted before prediction")
        
        features = self.prepare_features(df)
        posteriors = self.model.predict_proba(features)
        
        # Map to regime names
        if self.state_mapping:
            columns = [self.state_mapping.get(i, RegimeState.RANGING).name 
                      for i in range(self.n_states)]
        else:
            columns = [f"State_{i}" for i in range(self.n_states)]
        
        return pd.DataFrame(
            posteriors,
            index=df.index,
            columns=columns
        )
```

DELIVERABLE:
✅ HMM detector verified/enhanced
✅ Adaptive window verified
✅ Coherence validator verified
✅ Integration tests
✅ Regime classification accuracy metrics

STIMA TEMPO: 24 ore (3 giorni)
IMPACT: Fondamentale per position sizing e strategy selection

────────────────────────────────────────────────────────────────────────────────
FASE 2 SUMMARY
────────────────────────────────────────────────────────────────────────────────

TOTALE EFFORT: 108 ore (13.5 giorni working, 3-4 settimane calendar)
TOTALE IMPACT: +0.5 punti (8.7 → 9.2)

DELIVERABLES:
✅ Multi-Level Stop Loss system
✅ Regime-aware position sizing
✅ 50-100 advanced features (physics, info theory, fractal, microstructure)
✅ HMM regime detection verified/enhanced
✅ Comprehensive risk management framework

METRICS IMPROVEMENT ATTESI:
- Win rate: +2-3% (da 65% a 67-68%)
- Sharpe ratio: +0.3-0.5 (da 2.0 a 2.3-2.5)
- Max drawdown: -25-35% (da -12% a -8-9%)
- Prediction accuracy: +3-5%

NEXT: FASE 3 con ensemble e optimization avanzata

================================================================================
FASE 3: ADVANCED ML & ENSEMBLE (Week 7-12)
================================================================================

Obiettivo: Multi-model ensemble e optimization avanzata
Timeline: 6 settimane
Impact: +0.4 punti (9.2 → 9.6)
Effort: 180-240 ore

────────────────────────────────────────────────────────────────────────────────
3.1 MULTI-TIMEFRAME ENSEMBLE (da Renaissance)
────────────────────────────────────────────────────────────────────────────────

PRIORITÀ: 🚀 ALTA (P0)
File: models/multi_timeframe_ensemble.py (NUOVO)

RAZIONALE:
Singolo modello su singolo timeframe ha blind spots. Ensemble di modelli
su timeframes diversi cattura patterns multi-scala e riduce false signals.

DATI RICHIESTI:
✅ Multiple timeframes: OHLCV disponibile per 1m, 5m, 15m, 1h, 4h, 1d
✅ Volume: Disponibile per tutti timeframes
✅ Features: Calcolabili per ogni timeframe

ARCHITETTURA:
```
INPUT: Price data
  │
  ├─> Model_1m  (microstructure, noise)     → Prediction_1m + Confidence_1m
  ├─> Model_5m  (short-term momentum)       → Prediction_5m + Confidence_5m
  ├─> Model_15m (intraday patterns)         → Prediction_15m + Confidence_15m
  ├─> Model_1h  (medium-term trends)        → Prediction_1h + Confidence_1h
  └─> Model_4h  (macro patterns)            → Prediction_4h + Confidence_4h
       │
       └─> VOTING SYSTEM (weighted by performance + confidence)
            │
            └─> FINAL PREDICTION (consensus ≥ 60%)
```

IMPLEMENTAZIONE:
```python
# File: models/multi_timeframe_ensemble.py

import numpy as np
import pandas as pd
from dataclasses import dataclass
from typing import Dict, List, Optional
from enum import Enum

class Timeframe(Enum):
    """Supported timeframes."""
    M1 = "1m"
    M5 = "5m"
    M15 = "15m"
    H1 = "1h"
    H4 = "4h"
    D1 = "1d"

@dataclass
class TimeframeModelPrediction:
    """Prediction from a single timeframe model."""
    timeframe: Timeframe
    signal: int  # -1 (sell), 0 (neutral), +1 (buy)
    confidence: float  # 0-1
    probability: float  # 0-1
    features_used: int
    model_accuracy: float  # Recent accuracy of this model

class MultiTimeframeEnsemble:
    """
    Ensemble of models trained on different timeframes.
    
    Each timeframe captures different aspects:
    - 1m: Microstructure, order flow, noise trading
    - 5m: Short-term momentum, quick reversals
    - 15m: Intraday patterns, session dynamics
    - 1h: Medium-term trends, multi-hour patterns
    - 4h: Macro patterns, daily cycles
    
    Based on Renaissance Technologies' multi-scale approach.
    """
    
    def __init__(
        self,
        consensus_threshold: float = 0.60,
        min_models_required: int = 3,
        correlation_penalty_threshold: float = 0.70
    ):
        """
        Args:
            consensus_threshold: Minimum agreement for trade (60%)
            min_models_required: Minimum models agreeing to trade
            correlation_penalty_threshold: Reduce weight if correlation > this
        """
        self.consensus_threshold = consensus_threshold
        self.min_models_required = min_models_required
        self.correlation_penalty = correlation_penalty_threshold
        
        # Store models for each timeframe
        self.models: Dict[Timeframe, object] = {}
        
        # Track recent performance per timeframe
        self.performance_history: Dict[Timeframe, List[float]] = {
            tf: [] for tf in Timeframe
        }
        
        # Track prediction correlation
        self.prediction_correlation: np.ndarray = None
    
    def register_model(
        self,
        timeframe: Timeframe,
        model: object
    ):
        """Register a trained model for a timeframe."""
        self.models[timeframe] = model
    
    def predict_ensemble(
        self,
        data_by_timeframe: Dict[Timeframe, pd.DataFrame],
        current_regime: Optional[str] = None
    ) -> Dict:
        """
        Generate ensemble prediction across all timeframes.
        
        Args:
            data_by_timeframe: Dict mapping Timeframe to DataFrame with features
            current_regime: Optional current market regime
        
        Returns:
            Dict with final_signal, confidence, consensus, individual_predictions
        """
        # 1. Get predictions from each timeframe
        predictions: List[TimeframeModelPrediction] = []
        
        for timeframe, model in self.models.items():
            if timeframe not in data_by_timeframe:
                continue
            
            data = data_by_timeframe[timeframe]
            
            # Model prediction
            pred_proba = model.predict_proba(data)
            pred_signal = np.argmax(pred_proba[-1]) - 1  # Map 0,1,2 to -1,0,+1
            pred_confidence = pred_proba[-1][np.argmax(pred_proba[-1])]
            
            # Get recent model accuracy
            recent_accuracy = self._get_recent_accuracy(timeframe)
            
            prediction = TimeframeModelPrediction(
                timeframe=timeframe,
                signal=pred_signal,
                confidence=pred_confidence,
                probability=pred_proba[-1][1],  # Probability of bullish
                features_used=data.shape[1],
                model_accuracy=recent_accuracy
            )
            predictions.append(prediction)
        
        if len(predictions) < self.min_models_required:
            return {
                'final_signal': 0,
                'confidence': 0.0,
                'consensus': 0.0,
                'reason': f"Insufficient models ({len(predictions)} < {self.min_models_required})"
            }
        
        # 2. Calculate weights based on:
        #    - Recent performance
        #    - Confidence
        #    - Regime appropriateness
        #    - Correlation penalty
        weights = self._calculate_weights(predictions, current_regime)
        
        # 3. Weighted voting
        weighted_votes = []
        for pred, weight in zip(predictions, weights):
            weighted_votes.append(pred.signal * weight)
        
        # 4. Calculate consensus
        total_weight = sum(weights)
        if total_weight == 0:
            return {
                'final_signal': 0,
                'confidence': 0.0,
                'consensus': 0.0,
                'reason': "Zero total weight"
            }
        
        weighted_sum = sum(weighted_votes)
        consensus_score = abs(weighted_sum) / total_weight
        
        # 5. Determine final signal
        if consensus_score >= self.consensus_threshold:
            final_signal = 1 if weighted_sum > 0 else -1
            confidence = consensus_score
        else:
            final_signal = 0  # No consensus
            confidence = 0.0
        
        # 6. Count agreeing models
        agreeing_models = sum(
            1 for pred in predictions 
            if pred.signal == final_signal
        )
        
        return {
            'final_signal': final_signal,
            'confidence': confidence,
            'consensus': consensus_score,
            'agreeing_models': agreeing_models,
            'total_models': len(predictions),
            'individual_predictions': [
                {
                    'timeframe': p.timeframe.value,
                    'signal': p.signal,
                    'confidence': p.confidence,
                    'weight': w
                }
                for p, w in zip(predictions, weights)
            ],
            'reason': f"Consensus {consensus_score:.1%}, {agreeing_models}/{len(predictions)} agree"
        }
    
    def _calculate_weights(
        self,
        predictions: List[TimeframeModelPrediction],
        current_regime: Optional[str]
    ) -> List[float]:
        """
        Calculate voting weights for each prediction.
        
        Factors:
        1. Recent model accuracy (higher = higher weight)
        2. Prediction confidence (higher = higher weight)
        3. Regime appropriateness (trending regimes favor higher TF)
        4. Correlation penalty (if predictions too correlated, reduce weight)
        """
        weights = []
        
        # Base weights from accuracy and confidence
        for pred in predictions:
            # Accuracy component (0.5-1.0)
            accuracy_weight = pred.model_accuracy if pred.model_accuracy > 0 else 0.5
            
            # Confidence component (0-1)
            confidence_weight = pred.confidence
            
            # Combine (geometric mean for balance)
            base_weight = np.sqrt(accuracy_weight * confidence_weight)
            
            # Regime adjustment
            regime_multiplier = self._get_regime_multiplier(pred.timeframe, current_regime)
            
            # Final weight
            weight = base_weight * regime_multiplier
            weights.append(weight)
        
        # Apply correlation penalty
        # If predictions too correlated, they're not adding diversity
        if len(predictions) >= 3:
            signals = np.array([p.signal for p in predictions])
            if np.std(signals) < 0.5:  # Very similar predictions
                # Reduce weights proportionally
                weights = [w * 0.8 for w in weights]
        
        return weights
    
    def _get_regime_multiplier(
        self,
        timeframe: Timeframe,
        regime: Optional[str]
    ) -> float:
        """
        Adjust timeframe weight based on current regime.
        
        Different regimes favor different timeframes:
        - Trending: Higher timeframes more reliable
        - Ranging: Lower timeframes better for reversals
        - High volatility: Medium timeframes balance
        """
        if regime is None:
            return 1.0
        
        regime_lower = regime.lower()
        
        # Trending regimes
        if 'trend' in regime_lower or 'bull' in regime_lower or 'bear' in regime_lower:
            multipliers = {
                Timeframe.M1: 0.7,
                Timeframe.M5: 0.9,
                Timeframe.M15: 1.0,
                Timeframe.H1: 1.2,
                Timeframe.H4: 1.3,
                Timeframe.D1: 1.4
            }
        
        # Ranging regimes
        elif 'rang' in regime_lower:
            multipliers = {
                Timeframe.M1: 1.2,
                Timeframe.M5: 1.3,
                Timeframe.M15: 1.1,
                Timeframe.H1: 0.9,
                Timeframe.H4: 0.8,
                Timeframe.D1: 0.7
            }
        
        # High volatility
        elif 'volatil' in regime_lower:
            multipliers = {
                Timeframe.M1: 0.8,
                Timeframe.M5: 0.9,
                Timeframe.M15: 1.1,
                Timeframe.H1: 1.2,
                Timeframe.H4: 1.0,
                Timeframe.D1: 0.9
            }
        
        else:
            # Default: equal weights
            multipliers = {tf: 1.0 for tf in Timeframe}
        
        return multipliers.get(timeframe, 1.0)
    
    def _get_recent_accuracy(
        self,
        timeframe: Timeframe,
        lookback: int = 50
    ) -> float:
        """Get recent accuracy for a timeframe model."""
        history = self.performance_history.get(timeframe, [])
        if not history:
            return 0.5  # Default
        
        recent = history[-lookback:] if len(history) > lookback else history
        return np.mean(recent)
    
    def update_performance(
        self,
        timeframe: Timeframe,
        was_correct: bool
    ):
        """Update performance history after trade outcome known."""
        if timeframe not in self.performance_history:
            self.performance_history[timeframe] = []
        
        accuracy_point = 1.0 if was_correct else 0.0
        self.performance_history[timeframe].append(accuracy_point)
        
        # Keep only recent history (max 500 trades)
        if len(self.performance_history[timeframe]) > 500:
            self.performance_history[timeframe] = self.performance_history[timeframe][-500:]
```

TRAINING PIPELINE:
```python
# File: training/train_multi_timeframe.py

from models.multi_timeframe_ensemble import MultiTimeframeEnsemble, Timeframe
from training.train_sklearn import train_model

def train_multi_timeframe_system(
    data_by_timeframe: Dict[Timeframe, pd.DataFrame],
    symbol: str,
    model_type: str = 'xgboost'
):
    """
    Train separate models for each timeframe.
    
    Args:
        data_by_timeframe: Dict mapping Timeframe to DataFrame with OHLCV
        symbol: Trading symbol
        model_type: Model type (xgboost, lightgbm, randomforest)
    
    Returns:
        Trained MultiTimeframeEnsemble
    """
    ensemble = MultiTimeframeEnsemble(
        consensus_threshold=0.60,
        min_models_required=3
    )
    
    for timeframe, data in data_by_timeframe.items():
        print(f"\n{'='*80}")
        print(f"Training {timeframe.value} model for {symbol}")
        print(f"{'='*80}")
        
        # 1. Calculate features
        features = calculate_features(data, timeframe)
        
        # 2. Create labels
        labels = create_labels(data, timeframe)
        
        # 3. Train model with walk-forward validation
        model, metrics = train_model(
            features=features,
            labels=labels,
            model_type=model_type,
            validation_type='walk_forward'
        )
        
        # 4. Register model
        ensemble.register_model(timeframe, model)
        
        print(f"✅ {timeframe.value} model trained - Accuracy: {metrics['accuracy']:.2%}")
    
    return ensemble
```

DELIVERABLE:
✅ Multi-timeframe ensemble system
✅ Weighted voting mechanism
✅ Regime-aware timeframe weights
✅ Correlation tracking
✅ Performance attribution per timeframe

STIMA TEMPO: 60 ore (7.5 giorni)
IMPACT: +3-5% win rate, +0.3-0.5 Sharpe

────────────────────────────────────────────────────────────────────────────────
3.2 MULTI-MODEL ML ENSEMBLE (da Two Sigma/Renaissance)
────────────────────────────────────────────────────────────────────────────────

PRIORITÀ: 🚀 ALTA (P0)
File: models/ml_ensemble.py (NUOVO)

RAZIONALE:
Different ML algorithms capture different patterns. Ensemble reduces
model-specific bias and improves robustness.

DATI RICHIESTI:
✅ Features: Disponibili (tutte le features calcolate)
✅ Labels: Disponibili
✅ Training data: Sufficiente per multiple models

ARCHITETTURA:
```
Level 1 (Base Models):
  ├─> XGBoost
  ├─> LightGBM  
  ├─> Random Forest
  ├─> Logistic Regression
  └─> SVM
       │
       └─> Predictions (out-of-fold)
            │
Level 2 (Meta-Learner):
  └─> Logistic Regression / XGBoost
       │
       └─> FINAL PREDICTION
```

IMPLEMENTAZIONE:
```python
# File: models/ml_ensemble.py

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
import numpy as np
import pandas as pd
from sklearn.model_selection import KFold

class StackedEnsemble:
    """
    Stacked ensemble with multiple base models and meta-learner.
    
    Uses out-of-fold predictions to avoid overfitting.
    Based on Two Sigma/Kaggle-winning approaches.
    """
    
    def __init__(
        self,
        n_folds: int = 5,
        use_probabilities: bool = True
    ):
        """
        Args:
            n_folds: Number of folds for out-of-fold predictions
            use_probabilities: Use probabilities vs hard predictions
        """
        self.n_folds = n_folds
        self.use_probabilities = use_probabilities
        
        # Level 1: Base models
        self.base_models = {
            'xgboost': XGBClassifier(
                n_estimators=100,
                max_depth=6,
                learning_rate=0.1,
                random_state=42
            ),
            'lightgbm': LGBMClassifier(
                n_estimators=100,
                max_depth=6,
                learning_rate=0.1,
                random_state=42,
                verbose=-1
            ),
            'random_forest': RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                random_state=42
            ),
            'logistic': LogisticRegression(
                max_iter=1000,
                random_state=42
            ),
            'svm': SVC(
                probability=True,
                random_state=42
            )
        }
        
        # Level 2: Meta-learner
        self.meta_learner = LogisticRegression(
            max_iter=1000,
            random_state=42
        )
        
        self.is_fitted = False
    
    def fit(
        self,
        X: pd.DataFrame,
        y: pd.Series
    ):
        """
        Fit stacked ensemble using out-of-fold predictions.
        
        Args:
            X: Feature matrix
            y: Target labels
        """
        print("Training Stacked Ensemble...")
        
        # 1. Generate out-of-fold predictions from base models
        oof_predictions = self._generate_oof_predictions(X, y)
        
        # 2. Train meta-learner on OOF predictions
        print("\nTraining meta-learner...")
        self.meta_learner.fit(oof_predictions, y)
        
        # 3. Retrain base models on full data
        print("\nRetraining base models on full data...")
        for name, model in self.base_models.items():
            print(f"  Training {name}...")
            model.fit(X, y)
        
        self.is_fitted = True
        print("✅ Stacked Ensemble trained successfully")
    
    def _generate_oof_predictions(
        self,
        X: pd.DataFrame,
        y: pd.Series
    ) -> pd.DataFrame:
        """
        Generate out-of-fold predictions for all base models.
        
        Returns:
            DataFrame with OOF predictions for each base model
        """
        n_samples = len(X)
        n_models = len(self.base_models)
        n_classes = len(np.unique(y))
        
        # Initialize OOF prediction arrays
        if self.use_probabilities:
            oof_preds = np.zeros((n_samples, n_models * n_classes))
        else:
            oof_preds = np.zeros((n_samples, n_models))
        
        # K-Fold cross-validation
        kf = KFold(n_splits=self.n_folds, shuffle=True, random_state=42)
        
        for name, model in self.base_models.items():
            print(f"\nGenerating OOF predictions for {name}...")
            
            model_idx = list(self.base_models.keys()).index(name)
            
            for fold, (train_idx, val_idx) in enumerate(kf.split(X)):
                print(f"  Fold {fold + 1}/{self.n_folds}...", end=' ')
                
                # Split data
                X_train_fold = X.iloc[train_idx]
                y_train_fold = y.iloc[train_idx]
                X_val_fold = X.iloc[val_idx]
                
                # Train on fold
                model_clone = self._clone_model(model)
                model_clone.fit(X_train_fold, y_train_fold)
                
                # Predict on validation fold
                if self.use_probabilities:
                    preds = model_clone.predict_proba(X_val_fold)
                    start_col = model_idx * n_classes
                    end_col = start_col + n_classes
                    oof_preds[val_idx, start_col:end_col] = preds
                else:
                    preds = model_clone.predict(X_val_fold)
                    oof_preds[val_idx, model_idx] = preds
                
                print("Done")
        
        # Convert to DataFrame with meaningful column names
        if self.use_probabilities:
            columns = []
            for model_name in self.base_models.keys():
                for class_idx in range(n_classes):
                    columns.append(f"{model_name}_class{class_idx}")
        else:
            columns = list(self.base_models.keys())
        
        return pd.DataFrame(oof_preds, columns=columns, index=X.index)
    
    def _clone_model(self, model):
        """Clone a model with same parameters."""
        return model.__class__(**model.get_params())
    
    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """Predict using stacked ensemble."""
        if not self.is_fitted:
            raise ValueError("Ensemble must be fitted before prediction")
        
        # 1. Get predictions from base models
        base_predictions = self._get_base_predictions(X)
        
        # 2. Meta-learner predicts on base model outputs
        final_predictions = self.meta_learner.predict(base_predictions)
        
        return final_predictions
    
    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:
        """Predict probabilities using stacked ensemble."""
        if not self.is_fitted:
            raise ValueError("Ensemble must be fitted before prediction")
        
        # 1. Get predictions from base models
        base_predictions = self._get_base_predictions(X)
        
        # 2. Meta-learner predicts probabilities
        final_probabilities = self.meta_learner.predict_proba(base_predictions)
        
        return final_probabilities
    
    def _get_base_predictions(self, X: pd.DataFrame) -> pd.DataFrame:
        """Get predictions from all base models."""
        n_samples = len(X)
        n_models = len(self.base_models)
        
        # Infer number of classes from meta-learner (if fitted)
        if hasattr(self.meta_learner, 'classes_'):
            n_classes = len(self.meta_learner.classes_)
        else:
            n_classes = 3  # Default for sell/neutral/buy
        
        if self.use_probabilities:
            predictions = np.zeros((n_samples, n_models * n_classes))
        else:
            predictions = np.zeros((n_samples, n_models))
        
        for idx, (name, model) in enumerate(self.base_models.items()):
            if self.use_probabilities:
                preds = model.predict_proba(X)
                start_col = idx * n_classes
                end_col = start_col + n_classes
                predictions[:, start_col:end_col] = preds
            else:
                preds = model.predict(X)
                predictions[:, idx] = preds
        
        # Convert to DataFrame with column names matching OOF predictions
        if self.use_probabilities:
            columns = []
            for model_name in self.base_models.keys():
                for class_idx in range(n_classes):
                    columns.append(f"{model_name}_class{class_idx}")
        else:
            columns = list(self.base_models.keys())
        
        return pd.DataFrame(predictions, columns=columns, index=X.index)
    
    def get_model_weights(self) -> dict:
        """
        Get relative importance of each base model.
        
        Based on meta-learner coefficients.
        """
        if not self.is_fitted:
            raise ValueError("Ensemble must be fitted before getting weights")
        
        # For logistic regression, coefficients indicate importance
        if hasattr(self.meta_learner, 'coef_'):
            coefs = np.abs(self.meta_learner.coef_).mean(axis=0)
            
            # Group by base model
            n_classes = len(self.meta_learner.classes_)
            n_models = len(self.base_models)
            
            model_weights = {}
            for idx, name in enumerate(self.base_models.keys()):
                if self.use_probabilities:
                    start_idx = idx * n_classes
                    end_idx = start_idx + n_classes
                    weight = coefs[start_idx:end_idx].mean()
                else:
                    weight = coefs[idx]
                
                model_weights[name] = weight
            
            # Normalize to sum to 1
            total = sum(model_weights.values())
            if total > 0:
                model_weights = {k: v/total for k, v in model_weights.items()}
            
            return model_weights
        
        return {name: 1/len(self.base_models) for name in self.base_models.keys()}
```

DELIVERABLE:
✅ Stacked ensemble with 5 base models
✅ Out-of-fold prediction generation
✅ Meta-learner training
✅ Model weight attribution
✅ Comparison vs single models

STIMA TEMPO: 40 ore (5 giorni)
IMPACT: +3-6% prediction accuracy, +15-25% robustness

────────────────────────────────────────────────────────────────────────────────
3.3 WALK-FORWARD OPTIMIZATION AT SCALE
────────────────────────────────────────────────────────────────────────────────

PRIORITÀ: 📊 MEDIA (P1)
File: Already exists: validation/walk_forward.py

TASK:
Integrate existing Walk-Forward Validation with:
- Multi-timeframe ensemble
- Multi-model ensemble
- Regime-specific validation
- Transaction cost inclusion

IMPLEMENTATION:
```python
# File: validation/comprehensive_validation.py

from validation.walk_forward import WalkForwardValidator
from models.multi_timeframe_ensemble import MultiTimeframeEnsemble
from models.ml_ensemble import StackedEnsemble
from backtest.transaction_costs import TransactionCostModel

def comprehensive_walk_forward_test(
    data_by_timeframe: Dict[Timeframe, pd.DataFrame],
    symbols: List[str],
    test_config: dict
):
    """
    Comprehensive walk-forward testing across:
    - Multiple timeframes
    - Multiple models
    - Multiple symbols
    - Realistic transaction costs
    """
    results = []
    
    # Walk-forward parameters
    validator = WalkForwardValidator(
        n_splits=10,
        test_size=0.15,
        anchored=True,  # Expanding window
        purge_pct=0.02,  # 2% gap
        embargo_pct=0.01  # 1% embargo
    )
    
    for symbol in symbols:
        print(f"\n{'='*80}")
        print(f"Testing {symbol}")
        print(f"{'='*80}")
        
        for train_idx, test_idx in validator.split(data_by_timeframe[Timeframe.H1]):
            # Train ensemble on training period
            train_data = {
                tf: data[train_idx]
                for tf, data in data_by_timeframe.items()
            }
            
            ensemble = train_multi_timeframe_system(train_data, symbol)
            
            # Test on test period
            test_data = {
                tf: data[test_idx]
                for tf, data in data_by_timeframe.items()
            }
            
            # Backtest with transaction costs
            cost_model = TransactionCostModel(
                spread_pips=1.5,
                commission_per_lot=7.0,
                slippage_pips=0.5
            )
            
            metrics = backtest_ensemble(
                ensemble=ensemble,
                test_data=test_data,
                cost_model=cost_model
            )
            
            results.append({
                'symbol': symbol,
                'period': f"{test_idx[0]}-{test_idx[-1]}",
                **metrics
            })
    
    return pd.DataFrame(results)
```

DELIVERABLE:
✅ Comprehensive validation framework
✅ Multi-timeframe + multi-model testing
✅ Transaction cost integration
✅ Statistical significance tests
✅ Performance attribution analysis

STIMA TEMPO: 30 ore (4 giorni)
IMPACT: Robustness validation, confidence in metrics

────────────────────────────────────────────────────────────────────────────────
3.4 EXECUTION OPTIMIZATION - BASIC (da Citadel)
────────────────────────────────────────────────────────────────────────────────

PRIORITÀ: 💰 MEDIA (P1)
File: execution/smart_execution.py (NUOVO)

RAZIONALE:
Even with perfect predictions, poor execution can destroy alpha.
Optimize fills to reduce slippage and improve net profitability.

DATI RICHIESTI:
✅ Volume profile: Calcolabile (già implementato)
✅ Spread history: Calcolabile da tick data
✅ Time-of-day patterns: Analizzabile da candles

IMPLEMENTAZIONE (Simplified - no multi-broker routing):
```python
# File: execution/smart_execution.py

import pandas as pd
import numpy as np
from datetime import time

class SmartExecutionOptimizer:
    """
    Smart execution optimization to minimize slippage.
    
    Simplified version for retail/small prop trading.
    Full version (Citadel-style) would require multi-broker API.
    """
    
    def __init__(self):
        # Time-of-day liquidity patterns (London/NY sessions best)
        self.high_liquidity_times = [
            (time(8, 0), time(12, 0)),  # London session
            (time(13, 0), time(17, 0))   # NY session
        ]
        
        # Volume profile for limit order placement
        self.volume_profile_pct = 0.70  # Place orders at 70% volume level
    
    def optimize_order_params(
        self,
        signal: dict,
        current_price: float,
        current_time: pd.Timestamp,
        recent_volume_profile: dict,
        recent_spreads: pd.Series
    ) -> dict:
        """
        Optimize order parameters for best execution.
        
        Args:
            signal: Trading signal with entry/stop/target
            current_price: Current market price
            current_time: Current timestamp
            recent_volume_profile: VOL profile data (POC, VAH, VAL)
            recent_spreads: Recent spread history
        
        Returns:
            Optimized order parameters
        """
        # 1. Determine order type (market vs limit)
        order_type = self._select_order_type(
            signal=signal,
            current_time=current_time,
            recent_spreads=recent_spreads
        )
        
        # 2. Calculate optimal limit price (if limit order)
        if order_type == 'limit':
            limit_price = self._calculate_limit_price(
                signal=signal,
                current_price=current_price,
                volume_profile=recent_volume_profile
            )
        else:
            limit_price = None
        
        # 3. Calculate slippage estimate
        expected_slippage = self._estimate_slippage(
            signal=signal,
            current_time=current_time,
            recent_spreads=recent_spreads,
            order_type=order_type
        )
        
        # 4. Adjust position size for slippage
        adjusted_size = signal['size'] * (1 - expected_slippage/100)
        
        return {
            'order_type': order_type,
            'limit_price': limit_price,
            'expected_slippage_pct': expected_slippage,
            'adjusted_size': adjusted_size,
            'time_in_force': 'GTC',  # Good-til-cancel
            'reasoning': self._get_execution_reasoning(
                order_type, current_time, expected_slippage
            )
        }
    
    def _select_order_type(
        self,
        signal: dict,
        current_time: pd.Timestamp,
        recent_spreads: pd.Series
    ) -> str:
        """
        Select order type (market vs limit).
        
        Market: Faster but higher cost
        Limit: Cheaper but may not fill
        """
        # 1. Check time of day
        current_time_only = current_time.time()
        is_high_liquidity = any(
            start <= current_time_only <= end
            for start, end in self.high_liquidity_times
        )
        
        # 2. Check spread
        avg_spread = recent_spreads.tail(20).mean()
        current_spread = recent_spreads.iloc[-1]
        
        # 3. Check signal urgency
        urgency = signal.get('urgency', 'normal')  # low/normal/high
        
        # Decision logic
        if urgency == 'high':
            return 'market'  # Need immediate fill
        elif not is_high_liquidity or current_spread > avg_spread * 1.5:
            return 'limit'  # Poor conditions, use limit
        elif signal['confidence'] > 0.8:
            return 'market'  # High confidence, pay for speed
        else:
            return 'limit'  # Default to limit for cost saving
    
    def _calculate_limit_price(
        self,
        signal: dict,
        current_price: float,
        volume_profile: dict
    ) -> float:
        """
        Calculate optimal limit price using volume profile.
        
        Place limit near high-volume areas (more likely to fill).
        """
        direction = signal['direction']  # 'long' or 'short'
        
        # Get POC (Point of Control - max volume price)
        poc = volume_profile.get('poc', current_price)
        
        # For longs: Buy slightly below current price, near POC
        # For shorts: Sell slightly above current price, near POC
        if direction == 'long':
            # Limit buy: POC or 0.1% below current
            limit_price = min(poc, current_price * 0.999)
        else:  # short
            # Limit sell: POC or 0.1% above current
            limit_price = max(poc, current_price * 1.001)
        
        return limit_price
    
    def _estimate_slippage(
        self,
        signal: dict,
        current_time: pd.Timestamp,
        recent_spreads: pd.Series,
        order_type: str
    ) -> float:
        """
        Estimate expected slippage percentage.
        
        Based on:
        - Time of day (liquidity)
        - Order type (market vs limit)
        - Recent spread
        - Order size vs typical volume
        """
        # Base slippage from spread
        avg_spread = recent_spreads.tail(20).mean()
        
        # Time adjustment
        current_time_only = current_time.time()
        is_high_liquidity = any(
            start <= current_time_only <= end
            for start, end in self.high_liquidity_times
        )
        
        time_multiplier = 1.0 if is_high_liquidity else 1.5
        
        # Order type adjustment
        if order_type == 'market':
            type_multiplier = 1.5  # Market orders pay spread
        else:
            type_multiplier = 0.5  # Limit orders may get better fills
        
        # Calculate expected slippage
        expected_slippage_pips = avg_spread * time_multiplier * type_multiplier
        expected_slippage_pct = (expected_slippage_pips / signal['entry_price']) * 100
        
        return expected_slippage_pct
    
    def _get_execution_reasoning(
        self,
        order_type: str,
        current_time: pd.Timestamp,
        expected_slippage: float
    ) -> str:
        """Generate human-readable reasoning for execution decisions."""
        reasons = []
        
        reasons.append(f"Order type: {order_type}")
        reasons.append(f"Time: {current_time.strftime('%H:%M')}")
        reasons.append(f"Expected slippage: {expected_slippage:.3f}%")
        
        return " | ".join(reasons)
```

DELIVERABLE:
✅ Smart execution optimizer
✅ Order type selection logic
✅ Limit price optimization via volume profile
✅ Slippage estimation
✅ Backtest comparison (smart vs naive execution)

STIMA TEMPO: 20 ore (2.5 giorni)
IMPACT: -30-50% slippage, +0.5-1.0% per trade net profitability

────────────────────────────────────────────────────────────────────────────────
FASE 3 SUMMARY
────────────────────────────────────────────────────────────────────────────────

TOTALE EFFORT: 150 ore (18.75 giorni working, 4-6 settimane calendar)
TOTALE IMPACT: +0.4 punti (9.2 → 9.6)

DELIVERABLES:
✅ Multi-timeframe ensemble (5 timeframes)
✅ Multi-model ML ensemble (5 models + stacking)
✅ Comprehensive walk-forward validation
✅ Smart execution optimization
✅ Full integration testing

METRICS IMPROVEMENT ATTESI:
- Prediction accuracy: +5-8% (ensemble diversity)
- Win rate: +3-5% (better signals from multiple models)
- Sharpe ratio: +0.3-0.5 (more consistent returns)
- Robustness: +25-35% (less model-specific bias)
- Net profitability: +0.5-1.0% per trade (better execution)

NEXT: FASE 4 con polish e optimization finale

================================================================================
FASE 4: POLISH & OPTIMIZATION (Month 4-6)
================================================================================

Obiettivo: Final optimization e production readiness
Timeline: 2-3 mesi
Impact: +0.2 punti (9.6 → 9.8)
Effort: 150-200 ore

Questa fase include:
1. Infrastructure optimization (latency, throughput)
2. Online learning implementation
3. Performance monitoring dashboard
4. Continuous improvement system
5. Documentation completa
6. Production deployment preparation

(Dettagli disponibili su richiesta - questa fase è più tecnica/operazionale)

================================================================================
ROADMAP SUMMARY & PRIORITIZATION
================================================================================

TIMELINE COMPLETO:
- Week 1-2 (Fase 1): Verification & Quick Wins → 8.2 → 8.7
- Week 3-6 (Fase 2): High-Impact Integrations → 8.7 → 9.2
- Week 7-12 (Fase 3): Advanced ML & Ensemble → 9.2 → 9.6
- Month 4-6 (Fase 4): Polish & Optimization → 9.6 → 9.8

TOTAL: ~6 mesi calendar time
EFFORT: ~500-650 ore development

METRICHE FINALI ATTESE (Score 9.8/10):
- Prediction Accuracy: 72-75% (da 56-60%)
- Win Rate: 70-73% (da 65%)
- Sharpe Ratio: 2.5-3.0 (da 1.2-1.4)
- Max Drawdown: <8% (da -12%)
- Pattern Recognition: +15-20% accuracy
- Risk Management: -30-40% tail risk

================================================================================
DATA AVAILABILITY VERIFICATION CHECKLIST
================================================================================

PRIMA DI INIZIARE IMPLEMENTAZIONE:

□ Run data coverage analysis (Fase 1.4)
□ Verify volume data quality (>95% coverage)
□ Confirm multiple timeframes available (1m, 5m, 15m, 1h, 4h)
□ Check date range (min 12 mesi per training)
□ Verify tick data (if available, for microstructure features)
□ Confirm symbols coverage (min 5 symbols per portfolio)

SE DATI INSUFFICIENTI:
→ Prioritize data acquisition
→ Adjust timeline accordingly
→ Scale down multi-timeframe ensemble to available TFs
→ Use synthetic features where real data missing

================================================================================
RISK FACTORS & MITIGATION
================================================================================

RISCHI TECNICI:
1. Look-ahead bias presente → Fix critico Fase 1
2. Feature integration fallisce → Debug richiesto
3. HMM regime detection placeholder → Re-implementation needed
4. Data coverage insufficient → Acquisizione dati extra

RISCHI PERFORMANCE:
1. Ensemble non migliora accuracy → Feature engineering focus
2. Transaction costs mangiano alpha → Execution optimization critico
3. Overfitting con features avanzate → Strong walk-forward validation

RISCHI OPERATIVI:
1. Implementation time > stimato → Prioritize P0 tasks only
2. Infrastructure limitations → Cloud scaling se necessario
3. Complexity creep → Mantenere focus su high-impact features

MITIGATION:
- Phased approach permette pivot early
- Weekly metrics tracking
- Strong testing before production
- Fallback plans per ogni componente critica

================================================================================
SUCCESS METRICS PER FASE
================================================================================

FASE 1 SUCCESS CRITERIA:
✅ Look-ahead bias eliminated (statistical test passing)
✅ Volume features integrated (present in training)
✅ Feature persistence verified (save/load working)
✅ Data coverage report completed
✅ Baseline training run successful
→ GATE: Must pass tutti 5 criteria per procedere a Fase 2

FASE 2 SUCCESS CRITERIA:
✅ Multi-level stop loss reduces max DD by >20%
✅ Regime position sizing improves Sharpe by >0.2
✅ Advanced features contribute >15% feature importance
✅ HMM regime detection accuracy >70%
→ GATE: Must achieve 3/4 criteria per procedere a Fase 3

FASE 3 SUCCESS CRITERIA:
✅ Multi-timeframe ensemble consensus >60%
✅ Stacked ensemble beats best single model by >3%
✅ Walk-forward validation shows consistent out-of-sample performance
✅ Smart execution reduces slippage by >30%
→ GATE: Must achieve 3/4 criteria per procedere a Fase 4

================================================================================
FINAL NOTES
================================================================================

INTEGRAZIONI POSSIBILI con dati disponibili:
✅ Volume Analysis: SÌ (cforex volume reale)
✅ Multi-Timeframe: SÌ (se abbiamo 1m, 5m, 15m, 1h, 4h)
✅ Regime Detection: SÌ (OHLCV sufficiente)
✅ Advanced Features: SÌ (OHLCV + Volume)
✅ Risk Management: SÌ (runtime data)
✅ ML Ensemble: SÌ (nessun dato extra necessario)

INTEGRAZIONI NON POSSIBILI (data limitations):
❌ Order Flow Completo: Richiede L2 market depth (non disponibile retail)
❌ Real Tick-by-Tick: Approssimato da candles (volume profile aiuta)
❌ Multi-Broker Routing: Richiede API multiple brokers (out of scope)
❌ Dark Pool Data: Non accessibile retail

ALTERNATIVE per features non disponibili:
- Order Flow → Approximated via volume analysis (VSA, Smart Money)
- Tick data → Volume profile capture institutional footprint
- Microstructure → Simplified features da OHLCV

CONCLUSIONE:
Sistema può raggiungere 9.5-9.8/10 con dati disponibili.
Per 10/10 (Renaissance-level) servirebbe:
- Proprietary data sources
- Real L2 market depth
- Multi-venue execution
- PhD research team
→ Fuori portata ma non necessario per profitability eccellente

================================================================================
DOCUMENTO COMPLETATO
Pronto per implementazione Fase 1
================================================================================

Next Step: Eseguire data coverage analysis (Fase 1.4) per confermare
            disponibilità dati e procedere con implementation plan.
