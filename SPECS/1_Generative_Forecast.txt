# ForexGPT - Generative Forecast System: Issues, Bugs & Optimizations

Version: 1.0
Date: 2025-10-13
Status: Comprehensive Analysis

================================================================================
CRITICAL ISSUES
================================================================================

## ISSUE-001: DUPLICATED CODE ACROSS TRAINING MODULES
Severity: HIGH
Category: Code Quality, Maintainability
Impact: Technical Debt, Bug Risk, Maintenance Overhead

**Problem**:
Multiple training scripts contain identical or near-identical functions, creating:
- Maintenance nightmare (bug fixes must be applied N times)
- Inconsistency risk (fixes applied to some but not others)
- Code bloat (~500+ lines duplicated)

**Duplicated Functions**:

1. fetch_candles_from_db() - EXACT DUPLICATE
   Locations:
   - src/forex_diffusion/training/train_sklearn.py (line 35)
   - src/forex_diffusion/training/train_sklearn_btalib.py (line 31)
   
   Code: 57 lines, 100% identical
   
   Used by:
   - validation/multi_horizon.py
   - ui/pattern_training_tab.py
   - training/train.py
   - training/training_pipeline/training_orchestrator.py
   - features/incremental_updater.py
   - backtest/kernc_integration.py

2. _coerce_indicator_tfs() - EXACT DUPLICATE
   Locations:
   - src/forex_diffusion/training/train_sklearn.py (line 119)
   - src/forex_diffusion/training/train_sklearn_btalib.py (line 408)
   
   Code: 31 lines, 100% identical

3. _realized_vol_feature() - EXACT DUPLICATE
   Locations:
   - src/forex_diffusion/training/train_sklearn.py (line 150)
   - src/forex_diffusion/training/train_sklearn_btalib.py (line 137)
   
   Code: 10 lines, 100% identical

4. Feature Engineering Functions - SIMILAR DUPLICATES
   - _ensure_dt_index()
   - _timeframe_to_timedelta()
   - _relative_ohlc()
   - _temporal_feats()
   - _resample()
   
   Scattered across train_sklearn.py, train_sklearn_btalib.py, forecast_worker.py

**Recommendation**:
1. Create src/forex_diffusion/data/data_loader.py with:
   - fetch_candles_from_db()
   
2. Create src/forex_diffusion/features/feature_utils.py with:
   - _coerce_indicator_tfs()
   - _ensure_dt_index()
   - _timeframe_to_timedelta()
   - _resample()
   
3. Create src/forex_diffusion/features/feature_engineering.py with:
   - _relative_ohlc()
   - _temporal_feats()
   - _realized_vol_feature()
   
4. Update all imports across codebase

**Estimated Effort**: 4 hours
**Risk**: Low (pure refactoring, existing tests validate behavior)

--------------------------------------------------------------------------------

## ISSUE-002: INCONSISTENT IMPORTS AND CIRCULAR DEPENDENCY RISK
Severity: MEDIUM
Category: Code Structure, Maintainability
Impact: Import Errors, Refactoring Difficulty

**Problem**:
Multiple files import from train_sklearn.py, creating tight coupling:
- If train_sklearn.py is refactored, 10+ files break
- Circular import risk (training module imports from features, features imports from training)
- Inconsistent try/except import patterns

**Affected Files**:
- validation/multi_horizon.py → imports fetch_candles_from_db
- ui/workers/forecast_worker.py → imports 5+ functions
- ui/pattern_training_tab.py → imports fetch_candles_from_db
- training/train_sklearn_btalib.py → imports from train_sklearn
- training/train.py → imports fetch_candles_from_db, _coerce_indicator_tfs
- training/inproc.py → imports entire train_sklearn module
- features/incremental_updater.py → imports 5+ functions
- backtest/kernc_integration.py → imports fetch_candles_from_db

**Import Patterns**:
```python
# Pattern 1: Try/except with fallback
try:
    from forex_diffusion.training import train_sklearn as trainer_mod
except Exception:
    from . import train_sklearn as trainer_mod

# Pattern 2: Direct import
from forex_diffusion.training.train_sklearn import fetch_candles_from_db

# Pattern 3: Relative import with try/except
try:
    from ..training.train_sklearn import _relative_ohlc, _temporal_feats
except ImportError:
    from forex_diffusion.training.train_sklearn import _relative_ohlc
```

**Recommendation**:
1. After consolidating duplicates (ISSUE-001), update all imports to use new modules:
   ```python
   from forex_diffusion.data.data_loader import fetch_candles_from_db
   from forex_diffusion.features.feature_engineering import (
       relative_ohlc, temporal_feats, realized_vol_feature
   )
   from forex_diffusion.features.feature_utils import (
       coerce_indicator_tfs, ensure_dt_index, timeframe_to_timedelta
   )
   ```

2. Establish import conventions in AGENTS.md:
   - Prefer absolute imports from package root
   - No circular imports between training and features
   - Use try/except only for optional dependencies (not core modules)

**Estimated Effort**: 2 hours (after ISSUE-001 resolved)
**Risk**: Low (automated find/replace with verification)

--------------------------------------------------------------------------------

## ISSUE-003: UNCLEAR TRAINING SCRIPT SELECTION
Severity: MEDIUM
Category: Documentation, User Experience
Impact: Confusion, Suboptimal Model Choice

**Problem**:
Users face 7 training scripts with overlapping functionality:
1. train.py - VAE + Diffusion (PyTorch Lightning)
2. train_sklearn.py - Traditional ML (Ridge, RF, etc.)
3. train_sklearn_btalib.py - Enhanced ML with 80+ indicators
4. train_sssd.py - SSSD model (experimental)
5. train_optimized.py - NVIDIA-optimized training
6. inproc.py - In-process wrapper (GUI only)
7. loop.py - PyTorch Lightning module (internal)

**Current Documentation**:
- README.md mentions "train models" but doesn't specify which script
- Training.md (Italian) exists but incomplete
- No decision tree or comparison matrix

**Missing Information**:
- When to use train_sklearn.py vs train_sklearn_btalib.py?
- How to enable NVIDIA optimizations?
- Which script supports multi-horizon?
- Performance comparison (speed, accuracy)?

**Recommendation**:
1. Create Training Decision Matrix in README.md:
   ```
   Use Case                      | Script                    | GPU Needed?
   ------------------------------|---------------------------|------------
   Quick baseline (< 5 min)      | train_sklearn.py          | No
   Advanced features (80+ ind.)  | train_sklearn_btalib.py   | No
   Probabilistic forecasting     | train.py                  | Recommended
   Multi-horizon (experimental)  | train_sssd.py             | Yes
   Production GPU training       | train_optimized.py        | Yes
   GUI training                  | (automatic via UI)        | No
   ```

2. Add "Which Training Script?" section to Documentation/1_Generative_Forecast.md

3. Deprecate or merge redundant scripts:
   - Merge train_sklearn.py into train_sklearn_btalib.py (make 80+ indicators optional)
   - Make train_optimized.py a flag in train.py (--use_nvidia_opts)

**Estimated Effort**: 2 hours (documentation)
**Risk**: Low

--------------------------------------------------------------------------------

## ISSUE-004: LOOK-AHEAD BIAS RISK IN MULTIPLE SCRIPTS
Severity: HIGH (DATA QUALITY)
Category: Statistical Validity, Model Integrity
Impact: Inflated Accuracy, Production Failure

**Problem**:
Only train.py implements verified look-ahead bias prevention. Other scripts may leak future information:

**Scripts with Protection**:
✅ train.py: _standardize_train_val() with KS test verification

**Scripts WITHOUT Verification**:
❌ train_sklearn.py: _standardize_train_val() exists but NO KS test
❌ train_sklearn_btalib.py: No explicit standardization function visible
❌ inproc.py: Calls trainer_mod._standardize_train_val() (delegates to train_sklearn.py)
❌ train_sssd.py: SSSDDataModule - standardization unclear

**Risk Scenario**:
```python
# WRONG (look-ahead bias):
mu = np.mean(X)  # Uses entire dataset
sigma = np.std(X)
X_train = (X[:train_size] - mu) / sigma  # Train sees val stats
X_val = (X[train_size:] - mu) / sigma

# CORRECT:
mu = np.mean(X[:train_size])  # Only training set
sigma = np.std(X[:train_size])
X_train = (X[:train_size] - mu) / sigma
X_val = (X[train_size:] - mu) / sigma
```

**Evidence in train.py** (GOOD):
```python
# Line 106-142: Implements KS test verification
if scaler_metadata.get('ks_test_p_value') is not None:
    logger.info(f"[Scaler] KS test p-value: {scaler_metadata['ks_test_p_value']:.4f}")

if p_value is not None and p_value > 0.8:
    logger.warning(
        f"⚠️ POTENTIAL LOOK-AHEAD BIAS DETECTED!\n"
        f"Train/Val distributions suspiciously similar (KS p-value={p_value:.3f})."
    )
```

**Recommendation**:
1. **URGENT**: Audit train_sklearn.py and train_sklearn_btalib.py for look-ahead bias
2. Backport KS test verification from train.py to all scripts
3. Add unit tests for standardization (verify p-value < 0.8)
4. Document standardization protocol in AGENTS.md
5. Create forex_diffusion/utils/standardization.py with verified implementation

**Estimated Effort**: 6 hours
**Risk**: MEDIUM (changes affect all models, requires re-validation)

--------------------------------------------------------------------------------

## ISSUE-005: MISSING FEATURE ENGINEERING CENTRALIZATION
Severity: MEDIUM
Category: Architecture, Code Quality
Impact: Inconsistency, Duplication, Maintenance

**Problem**:
Feature engineering logic scattered across:
- training/train_sklearn.py (_indicators function, 300+ lines)
- training/train_sklearn_btalib.py (BTALibIndicators class)
- features/incremental_updater.py (imports from training)
- ui/workers/forecast_worker.py (imports from training)

**Inconsistencies**:
- train_sklearn.py uses ta library for indicators
- train_sklearn_btalib.py uses bta-lib
- No shared indicator computation logic
- Incremental updater must replicate training logic

**Recommendation**:
1. Create src/forex_diffusion/features/indicator_pipeline.py:
   ```python
   class IndicatorPipeline:
       def __init__(self, indicator_config, timeframe_config):
           self.config = indicator_config
           self.tfs = timeframe_config
       
       def compute(self, df: pd.DataFrame) -> pd.DataFrame:
           """Compute all indicators per config"""
           features = []
           for indicator, timeframes in self.tfs.items():
               for tf in timeframes:
                   feat = self._compute_indicator(df, indicator, tf)
                   features.append(feat)
           return pd.concat(features, axis=1)
   ```

2. Unify indicator computation:
   - Use bta-lib as default (80+ indicators)
   - Fallback to ta library if bta-lib unavailable
   - Consistent API across all scripts

3. Update all training scripts to use IndicatorPipeline

**Estimated Effort**: 8 hours
**Risk**: MEDIUM (requires comprehensive testing)

--------------------------------------------------------------------------------

## ISSUE-006: TRAINING ORCHESTRATOR DATABASE COUPLING
Severity: LOW
Category: Architecture
Impact: Testing Difficulty, Flexibility

**Problem**:
TrainingOrchestrator tightly coupled to SQLAlchemy database:
- training_pipeline/training_orchestrator.py requires DB session
- training_pipeline/database.py defines ORM models
- Cannot easily test or use in-memory

**Current Design**:
```python
class TrainingOrchestrator:
    def train_models_grid(self, queue_id: int):
        with session_scope() as session:  # DB required
            queue = get_training_queue_by_id(session, queue_id)
            ...
```

**Recommendation**:
1. Abstract storage backend:
   ```python
   class TrainingStorage(ABC):
       @abstractmethod
       def create_queue(self, configs): pass
       @abstractmethod
       def get_queue(self, queue_id): pass
       @abstractmethod
       def update_queue_status(self, queue_id, status): pass
   
   class SQLAlchemyStorage(TrainingStorage):
       # Current implementation
   
   class InMemoryStorage(TrainingStorage):
       # For testing
   ```

2. Inject storage into orchestrator:
   ```python
   orchestrator = TrainingOrchestrator(storage=SQLAlchemyStorage())
   ```

**Estimated Effort**: 4 hours
**Risk**: LOW (optional improvement)

================================================================================
BUGS
================================================================================

## BUG-001: POTENTIAL DIVISION BY ZERO IN STANDARDIZATION
Severity: MEDIUM
Category: Runtime Error
Impact: Training Crash

**Location**:
- train_sklearn.py: Line ~160 (inferred from pattern)
- train.py: Line 111 (FIXED with sigma[sigma == 0] = 1.0)

**Problem**:
If a feature has zero variance (constant value), standardization divides by zero:
```python
sigma = train.std(axis=(0, 2), keepdims=True)
train_norm = (train - mu) / sigma  # ZeroDivisionError if sigma=0
```

**Status**:
- train.py: FIXED (line 111)
- train_sklearn.py: NEEDS VERIFICATION
- train_sklearn_btalib.py: NEEDS VERIFICATION

**Recommendation**:
1. Verify all scripts replace sigma=0 with sigma=1.0:
   ```python
   sigma[sigma == 0] = 1.0  # Or np.finfo(float).eps
   ```

2. Add logging warning when constant features detected:
   ```python
   if np.any(sigma == 0):
       logger.warning(f"Constant features detected: {feature_names[sigma == 0]}")
   ```

**Estimated Effort**: 1 hour
**Risk**: LOW

--------------------------------------------------------------------------------

## BUG-002: MISSING ERROR HANDLING IN FETCH_CANDLES_FROM_DB
Severity: LOW
Category: Error Handling
Impact: Cryptic Error Messages

**Location**:
- train_sklearn.py: fetch_candles_from_db() (line 35)
- train_sklearn_btalib.py: fetch_candles_from_db() (line 31)

**Problem**:
Generic exceptions catch all errors without context:
```python
except Exception as e:
    raise RuntimeError(f"Failed to instantiate MarketDataService: {e}")
```

**Issues**:
- Doesn't distinguish between:
  - MarketDataService not found (import error)
  - Database connection failure
  - Invalid symbol/timeframe
  - No data available
- Stack trace lost

**Recommendation**:
```python
try:
    ms = MarketDataService()
except ImportError as e:
    raise RuntimeError(f"MarketDataService not available: {e}") from e
except ConnectionError as e:
    raise RuntimeError(f"Database connection failed: {e}") from e
except Exception as e:
    raise RuntimeError(f"Unexpected error in MarketDataService: {e}") from e
```

**Estimated Effort**: 1 hour
**Risk**: LOW

--------------------------------------------------------------------------------

## BUG-003: INDICATOR CACHE NOT INVALIDATED ON CONFIG CHANGE
Severity: LOW
Category: Caching Logic
Impact: Stale Data

**Location**:
- train_sklearn.py: _indicators() function (line ~200-500)

**Problem**:
Timeframe data cached in timeframe_cache dict during indicator computation:
```python
timeframe_cache: Dict[str, pd.DataFrame] = {base_tf: df.copy()}

# Pre-fetch all timeframes
for tf in unique_tfs:
    if tf != base_tf:
        timeframe_cache[tf] = fetch_candles_from_db(symbol, tf, days_history)
```

**Issue**:
If called multiple times in same process with different symbols, cache persists:
- First call: EUR/USD with ["1m", "5m"] → cache["5m"] = EUR/USD data
- Second call: GBP/USD with ["1m", "5m"] → cache["5m"] STILL EUR/USD data!

**Impact**:
Only affects in-process training (GUI via inproc.py), not CLI (new process each time)

**Recommendation**:
1. Use symbol-aware cache key:
   ```python
   cache_key = f"{symbol}_{tf}_{days_history}"
   timeframe_cache[cache_key] = ...
   ```

2. Clear cache after each training run:
   ```python
   timeframe_cache.clear()
   ```

3. Use LRU cache with size limit:
   ```python
   from functools import lru_cache
   
   @lru_cache(maxsize=32)
   def _fetch_timeframe(symbol, tf, days_history):
       return fetch_candles_from_db(symbol, tf, days_history)
   ```

**Estimated Effort**: 1 hour
**Risk**: LOW

================================================================================
OPTIMIZATIONS
================================================================================

## OPT-001: REDUCE TIMEFRAME FETCHING OVERHEAD
Severity: MEDIUM
Category: Performance
Impact: 30-50% Training Speedup

**Problem**:
Current indicator computation fetches same timeframe multiple times:
- ATR_5m_14, RSI_5m_14, MACD_5m_12_26_9 → 3x fetch("5m")

**Current** (train_sklearn.py line ~220):
```python
for name, params in ind_cfg.items():
    for tf in indicator_tfs[name]:
        tmp = fetch_candles_from_db(symbol, tf, days_history)  # DUPLICATE FETCH
        # compute indicator
```

**Optimized** (line ~230):
```python
# Pre-fetch all unique timeframes ONCE (already implemented!)
timeframe_cache = {}
for tf in unique_tfs:
    timeframe_cache[tf] = fetch_candles_from_db(symbol, tf, days_history)

# Use cached data
for name, params in ind_cfg.items():
    for tf in indicator_tfs[name]:
        tmp = timeframe_cache[tf]  # CACHE HIT
```

**Status**: ALREADY IMPLEMENTED in train_sklearn.py (line ~215-230)

**Recommendation**:
✅ Verify this optimization exists in ALL scripts:
- train_sklearn.py: ✅ IMPLEMENTED
- train_sklearn_btalib.py: ❌ NEEDS VERIFICATION
- inproc.py: ✅ Uses train_sklearn (inherits optimization)

**Estimated Effort**: 1 hour (verification + apply to missing scripts)
**Risk**: LOW

--------------------------------------------------------------------------------

## OPT-002: PARALLELIZE INDICATOR COMPUTATION
Severity: MEDIUM
Category: Performance
Impact: 2-4x Speedup (Multi-Core)

**Problem**:
Indicators computed sequentially, but are independent:
```python
for indicator, timeframe in product(indicators, timeframes):
    feature = compute_indicator(indicator, timeframe)  # Sequential
```

**Recommendation**:
```python
from concurrent.futures import ThreadPoolExecutor, as_completed

def compute_indicator_tf(indicator, timeframe, df):
    # Compute single indicator
    return compute_indicator(df, indicator, timeframe)

# Parallel computation
with ThreadPoolExecutor(max_workers=4) as executor:
    futures = []
    for indicator, tf in product(indicators, timeframes):
        future = executor.submit(compute_indicator_tf, indicator, tf, df)
        futures.append((indicator, tf, future))
    
    features = []
    for indicator, tf, future in futures:
        result = future.result()
        features.append(result)
```

**Constraints**:
- GIL limits benefit for pure Python (but TA-Lib, BTA-Lib release GIL)
- Memory usage increases (all timeframes in RAM simultaneously)
- Best for >= 10 indicators × timeframes

**Estimated Effort**: 4 hours
**Risk**: MEDIUM (threading bugs, memory pressure)

--------------------------------------------------------------------------------

## OPT-003: LAZY LOADING FOR INFERENCE
Severity: LOW
Category: Performance, Memory
Impact: Faster Inference Startup

**Problem**:
Inference loads entire history for feature computation:
```python
candles = fetch_candles_from_db(symbol, tf, days_history=90)
# Only last N bars needed for indicators (N = max(indicator_window))
```

**Recommendation**:
```python
# Calculate required bars
max_window = max(atr_n, rsi_n, bb_n, hurst_window, ...)
required_bars = max_window + warmup_bars + 10  # Buffer

# Fetch only required recent bars
candles = fetch_candles_from_db_recent(symbol, tf, n_bars=required_bars)
```

**Benefit**:
- Reduced memory (90 days × 1440 bars/day = 129,600 bars → 200 bars)
- Faster query (no full table scan)
- Faster feature computation

**Estimated Effort**: 2 hours
**Risk**: LOW

--------------------------------------------------------------------------------

## OPT-004: COMPILE VAE ENCODER/DECODER WITH TORCH.COMPILE
Severity: LOW
Category: Performance
Impact: 20-40% Speedup (Already Implemented!)

**Location**:
- models/vae.py: _compile_if_available() (line ~165)

**Status**: ✅ ALREADY IMPLEMENTED

**Code**:
```python
def _compile_if_available(self):
    if not torch.cuda.is_available():
        return
    
    try:
        if hasattr(torch, 'compile'):
            self.encoder = torch.compile(self.encoder, mode="reduce-overhead")
            self.decoder = torch.compile(self.decoder, mode="reduce-overhead")
            logger.info("VAE compiled with torch.compile (20-40% speedup)")
    except Exception as e:
        logger.warning(f"torch.compile failed: {e}")
```

**Recommendation**:
✅ No action needed (already optimal)

--------------------------------------------------------------------------------

## OPT-005: USE NVIDIA DALI FOR DATA LOADING
Severity: LOW
Category: Performance (GPU Training)
Impact: 2-3x Data Loading Speedup

**Problem**:
PyTorch DataLoader uses CPU for augmentation/transformation:
- Bottleneck for small models on fast GPUs
- GPU idle while waiting for batches

**Recommendation**:
Implement DALI data loader in train.py:
```python
from nvidia.dali.pipeline import Pipeline
from nvidia.dali.plugin.pytorch import DALIGenericIterator

class CandlePipeline(Pipeline):
    def __init__(self, patches, targets, batch_size, num_threads):
        super().__init__(batch_size, num_threads, device_id=0)
        self.patches_input = ops.ExternalSource()
        self.targets_input = ops.ExternalSource()
    
    def define_graph(self):
        patches = self.patches_input()
        targets = self.targets_input()
        # GPU-side normalization
        patches = ops.Normalize(patches, mean=mu, std=sigma, device="gpu")
        return patches, targets
```

**Constraints**:
- Linux/WSL only (no Windows support)
- Added complexity
- Only beneficial for large datasets (> 100k samples)

**Status**: Configured in pyproject.toml but not implemented

**Estimated Effort**: 8 hours
**Risk**: MEDIUM

================================================================================
UNUSED/DEAD CODE
================================================================================

## DEAD-001: UNUSED TRAINING SCRIPTS
Severity: LOW
Category: Code Cleanup
Impact: Confusion, Clutter

**Potentially Unused**:
1. train_optimized.py (duplicates train.py with --use_nvidia_opts flag)
2. train_sklearn_btalib.py (could merge into train_sklearn.py with --use_btalib flag)

**Recommendation**:
1. Verify usage via git log and grep:
   ```bash
   git log --all --grep="train_optimized" --since="6 months ago"
   grep -r "train_optimized" .
   ```

2. If unused:
   - Move to deprecated/ folder
   - Add deprecation notice in README
   - Remove after 1 version cycle

**Estimated Effort**: 1 hour
**Risk**: LOW

--------------------------------------------------------------------------------

## DEAD-002: UNUSED IMPORTS IN TRAIN.PY
Severity: LOW
Category: Code Cleanup
Impact: Clutter, Confusion

**Location**:
- train.py: Line 1-20

**Unused Imports**:
```python
from typing import Iterable  # Unused
```

**Recommendation**:
Run automated import cleanup:
```bash
autoflake --remove-all-unused-imports --in-place train.py
```

**Estimated Effort**: 15 minutes
**Risk**: NONE

================================================================================
PROCEDURAL ERRORS
================================================================================

## PROC-001: MISSING TRAIN/VAL/TEST SPLIT
Severity: MEDIUM
Category: Statistical Validity
Impact: Overfitting Risk

**Problem**:
Current workflow:
```
Data → Train (80%) / Validation (20%) → Model
      ↓
   Test on validation set → Report metrics
```

**Issue**: No held-out test set!
- Validation set used for early stopping AND final metrics
- Model implicitly optimized on validation set
- True generalization unknown

**Recommendation**:
```
Data → Train (60%) / Validation (20%) / Test (20%) → Model
      ↓                ↓                   ↓
   Training      Early Stopping      Final Metrics
```

**Implementation**:
```python
train_size = int(n * 0.6)
val_size = int(n * 0.2)
test_size = n - train_size - val_size

X_train = X[:train_size]
X_val = X[train_size:train_size+val_size]
X_test = X[train_size+val_size:]

# Train with early stopping on val
# Report metrics on test
```

**Estimated Effort**: 2 hours
**Risk**: LOW

--------------------------------------------------------------------------------

## PROC-002: NO HYPERPARAMETER SEARCH IN GUI
Severity: LOW
Category: User Experience
Impact: Suboptimal Models

**Problem**:
GUI (TrainingTab) offers genetic algorithm but:
- Not enabled by default
- No explanation of when to use
- No comparison with manual tuning

**Recommendation**:
1. Add "Optimize Hyperparameters?" checkbox
2. Show popup explaining GA:
   - "Genetic Algorithm will test 50-500 combinations"
   - "Expected time: 30-60 minutes"
   - "Recommended for first-time training"

3. Add "Quick Start" mode:
   - Pre-filled sensible defaults
   - No optimization (fast baseline)
   - Show estimated time: "~5 minutes"

**Estimated Effort**: 4 hours
**Risk**: LOW

--------------------------------------------------------------------------------

## PROC-003: MISSING ARTIFACT CLEANUP POLICY
Severity: LOW
Category: Storage Management
Impact: Disk Space

**Problem**:
artifacts/ folder grows indefinitely:
- Every training run saves model + metadata
- No automatic cleanup
- Can reach 10s of GB

**Recommendation**:
1. Implement max_saved policy (already in config!):
   ```python
   # In ModelFileManager
   if len(saved_models) > config.max_saved:
       oldest = sorted(saved_models, key=lambda m: m.timestamp)[0]
       oldest.delete()
   ```

2. Add CLI command:
   ```bash
   python -m forex_diffusion.cli.artifacts clean --keep-best 10
   ```

3. Add GUI warning:
   "artifacts/ folder is 5.2 GB. Clean up old models?"

**Estimated Effort**: 2 hours
**Risk**: LOW

================================================================================
INDENTATION & STYLE ISSUES
================================================================================

## STYLE-001: INCONSISTENT INDENTATION
Severity: LOW
Category: Code Style
Impact: Readability

**Location**:
- Multiple files use mix of tabs and spaces (unlikely but check)

**Recommendation**:
```bash
# Check for mixed indentation
find src -name "*.py" -exec grep -l $'\t' {} \;

# Auto-fix with black
black src/forex_diffusion/training/*.py
```

**Estimated Effort**: 15 minutes
**Risk**: NONE

--------------------------------------------------------------------------------

## STYLE-002: MISSING TYPE HINTS
Severity: LOW
Category: Code Quality
Impact: IDE Support, Maintainability

**Problem**:
Many functions lack type hints:
```python
def _build_features(candles, args):  # No hints
    ...
```

**Recommendation**:
```python
def _build_features(
    candles: pd.DataFrame,
    args: argparse.Namespace
) -> Tuple[np.ndarray, np.ndarray, Dict[str, Any]]:
    ...
```

**Estimated Effort**: 4 hours (gradual improvement)
**Risk**: NONE

================================================================================
SUMMARY STATISTICS
================================================================================

Total Issues Found: 20
- Critical: 5
- High: 1 (look-ahead bias)
- Medium: 8
- Low: 6

Severity Distribution:
- CRITICAL (Code Duplication): 1
- HIGH (Data Quality): 1
- MEDIUM (Architecture/Performance): 8
- LOW (Cleanup/Style): 10

Estimated Total Effort: 60 hours
- Quick wins (<2 hours): 8 issues
- Medium effort (2-8 hours): 7 issues
- Large effort (>8 hours): 1 issue

Recommended Priority (Top 5):
1. ISSUE-004: Look-ahead bias verification (6 hours, HIGH severity)
2. ISSUE-001: Code duplication consolidation (4 hours, HIGH impact)
3. ISSUE-002: Import consistency (2 hours, MEDIUM impact)
4. BUG-001: Division by zero verification (1 hour, MEDIUM severity)
5. OPT-001: Verify timeframe caching (1 hour, MEDIUM impact)

================================================================================
ACTIONABLE NEXT STEPS
================================================================================

PHASE 1: Critical Fixes (12 hours)
1. Verify and fix look-ahead bias (ISSUE-004)
2. Consolidate duplicated code (ISSUE-001)
3. Fix division by zero bugs (BUG-001)
4. Implement train/val/test split (PROC-001)

PHASE 2: Architecture Improvements (16 hours)
1. Refactor imports (ISSUE-002)
2. Centralize feature engineering (ISSUE-005)
3. Create training decision matrix (ISSUE-003)
4. Add hyperparameter search to GUI (PROC-002)

PHASE 3: Performance Optimizations (12 hours)
1. Verify timeframe caching (OPT-001)
2. Parallelize indicators (OPT-002)
3. Lazy loading for inference (OPT-003)
4. Implement DALI (optional) (OPT-005)

PHASE 4: Cleanup (8 hours)
1. Remove dead code (DEAD-001, DEAD-002)
2. Fix style issues (STYLE-001, STYLE-002)
3. Improve error handling (BUG-002, BUG-003)
4. Artifact cleanup policy (PROC-003)

Total Estimated: 48 hours (6 days @ 8 hours/day)

================================================================================
END OF DOCUMENT
================================================================================

Generated: 2025-10-13
Analyzer: Factory AI
Version: 1.0
