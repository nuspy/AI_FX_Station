"model": {
  "label": "Model:",
  "tooltip": "Model Algorithm Selection\n\n1) WHAT IT IS:\nThe machine learning algorithm used to train the predictive model.\nEach algorithm has different strengths, weaknesses, speed, and accuracy characteristics.\n\nAvailable algorithms:\n- ridge: Ridge Regression (L2 regularization)\n- lasso: Lasso Regression (L1 regularization)\n- elasticnet: Elastic Net (L1 + L2 combined)\n- rf: Random Forest (ensemble of decision trees)\n- lightning: LightGBM (gradient boosting)\n- diffusion-ddpm: Denoising Diffusion Probabilistic Model\n- diffusion-ddim: Denoising Diffusion Implicit Model\n- sssd: Structured State Space Diffusion\n\n2) HOW AND WHEN TO USE:\n\nWhen to use LINEAR models (ridge, lasso, elasticnet):\n- Beginner-friendly (simple, interpretable)\n- Very fast training (seconds to minutes)\n- Limited data (<10K samples)\n- When you want feature importance scores\n- Production deployment (fast inference)\n- Stable, reliable baseline\n\nWhen to use TREE-BASED models (rf, lightning):\n- Non-linear relationships in data\n- Moderate to large datasets (>10K samples)\n- Feature interactions important\n- Robust to outliers and missing data\n- When you have 10-60 minutes for training\n- Production systems (good speed + accuracy balance)\n\nWhen to use DIFFUSION models (ddpm, ddim, sssd):\n- Research and experimentation\n- Maximum accuracy potential\n- Large datasets (>50K samples)\n- GPU available (required for reasonable speed)\n- When training time is not critical (hours)\n- Cutting-edge forecasting\n\nHow to choose:\n- Start with ridge (fastest baseline)\n- If ridge works, try lightning (usually 2-5% better)\n- If lightning insufficient, try diffusion (research-level)\n- Never start with complex models (hard to debug)\n\n3) WHY TO USE IT:\n- Different algorithms capture different patterns\n- Trade-off between speed, accuracy, complexity\n- Some algorithms better for specific market conditions\n- Model selection is CRITICAL for performance\n- Start simple, increase complexity only if needed\n\n4) EFFECTS:\n\n4.1) RIDGE REGRESSION:\n\nBehavior:\n- Linear model with L2 penalty (shrinks coefficients)\n- Assumes linear relationships between features and target\n- Training time: 5-30 seconds (7-90 days)\n- Inference: <1ms per prediction\n\nAdvantages:\n- EXTREMELY FAST (fastest algorithm available)\n- Very stable (no overfitting with proper regularization)\n- Interpretable (can see feature weights)\n- Good baseline (beat 50% of sophisticated models)\n- Low memory usage\n- Deterministic (same result every time)\n- Perfect for rapid prototyping\n\nDisadvantages:\n- LINEAR ONLY (can't model complex non-linear patterns)\n- Limited accuracy ceiling (~54-58% on most forex data)\n- Assumes feature scaling (requires preprocessing)\n- Poor with feature interactions (A×B not captured)\n\nWhen to use:\n- DEFAULT for beginners (learn with this first)\n- Quick experiments and iteration\n- Production systems (fast + reliable)\n- When training time critical (<1 minute)\n- Baseline comparison (always train ridge first)\n\nTypical results:\n- Accuracy: 54-58% (decent edge)\n- Sharpe: 1.2-1.5\n- Training time: 10-30 seconds\n- Inference: <1ms\n- Memory: <100MB\n\n4.2) LASSO REGRESSION:\n\nBehavior:\n- Linear model with L1 penalty (sets weak features to zero)\n- Automatic feature selection (sparse model)\n- Training time: 10-60 seconds (slightly slower than ridge)\n\nAdvantages:\n- FEATURE SELECTION (automatically drops useless features)\n- Interpretable (fewer non-zero coefficients)\n- Fast training and inference\n- Prevents overfitting via sparsity\n- Good when many irrelevant features\n\nDisadvantages:\n- Still linear only\n- Can be unstable with correlated features\n- May drop relevant features if highly correlated\n- Accuracy similar to ridge (54-58%)\n\nWhen to use:\n- When you have MANY features (>100)\n- Feature selection needed (want sparse model)\n- Correlated features to remove\n- Interpretability critical (need to explain model)\n\nTypical results:\n- Accuracy: 54-58%\n- Sharpe: 1.2-1.5\n- Training time: 15-60 seconds\n- Features used: 30-60% of input features (rest zeroed)\n\n4.3) ELASTIC NET:\n\nBehavior:\n- Combines L1 (lasso) + L2 (ridge) penalties\n- Best of both worlds (feature selection + stability)\n- Training time: 15-90 seconds\n\nAdvantages:\n- BALANCED (feature selection + stability)\n- Handles correlated features better than lasso\n- More robust than pure lasso or ridge\n- Good default for linear models\n\nDisadvantages:\n- Still linear only\n- Two hyperparameters to tune (L1 + L2 ratios)\n- Slower than ridge\n- Marginal improvement over ridge/lasso\n\nWhen to use:\n- When lasso too aggressive (drops too many features)\n- When ridge too conservative (keeps all features)\n- Correlated features + need stability\n- Alternative to ridge/lasso (try all 3, pick best)\n\nTypical results:\n- Accuracy: 54-58%\n- Sharpe: 1.2-1.5\n- Training time: 20-90 seconds\n\n4.4) RANDOM FOREST (rf):\n\nBehavior:\n- Ensemble of decision trees (100-500 trees)\n- Non-linear, captures feature interactions\n- Training time: 2-15 minutes (7-90 days)\n- Inference: 5-20ms per prediction\n\nAdvantages:\n- NON-LINEAR (captures complex patterns)\n- Feature interactions (A×B, A×B×C automatically)\n- Robust to outliers and missing data\n- Feature importance scores (interpretable)\n- No feature scaling needed\n- Usually 2-4% better accuracy than ridge\n- Handles categorical features well\n\nDisadvantages:\n- SLOW TRAINING (10-100× slower than ridge)\n- Slow inference (5-20ms vs <1ms)\n- High memory usage (GB for large forests)\n- Can overfit with too many trees\n- Not fully deterministic (random splits)\n\nWhen to use:\n- When ridge/lasso accuracy insufficient\n- Non-linear patterns suspected\n- Feature interactions important\n- When you have 5-20 minutes for training\n- Production systems (good speed/accuracy balance)\n\nTypical results:\n- Accuracy: 56-62% (2-4% better than ridge)\n- Sharpe: 1.4-1.8\n- Training time: 3-15 minutes\n- Inference: 5-20ms\n- Memory: 500MB-2GB\n\n4.5) LIGHTGBM (lightning):\n\nBehavior:\n- Gradient boosting (sequential trees correct errors)\n- State-of-the-art tree-based algorithm\n- Training time: 1-10 minutes (faster than RF)\n- Inference: 2-10ms per prediction\n\nAdvantages:\n- BEST TREE-BASED ACCURACY (usually top performer)\n- Faster than Random Forest (3-5× speedup)\n- Handles large datasets efficiently (>100K samples)\n- Feature interactions captured\n- Built-in regularization (less overfitting)\n- GPU acceleration available\n- RECOMMENDED for production (best speed/accuracy)\n\nDisadvantages:\n- More hyperparameters to tune\n- Can overfit if not regularized properly\n- Requires careful tuning (learning rate, depth, etc.)\n- Less interpretable than linear models\n\nWhen to use:\n- RECOMMENDED for serious trading systems\n- When you want best accuracy without diffusion complexity\n- Large datasets (>50K samples)\n- When you have 5-15 minutes for training\n- Production deployment (fast + accurate)\n\nTypical results:\n- Accuracy: 58-64% (BEST non-diffusion)\n- Sharpe: 1.5-2.0\n- Training time: 2-10 minutes\n- Inference: 2-10ms\n- Memory: 300MB-1GB\n\n4.6) DIFFUSION-DDPM:\n\nBehavior:\n- Denoising Diffusion Probabilistic Model\n- Generative model adapted for forecasting\n- Training time: 30 minutes to 8 hours (GPU required)\n- Inference: 100-500ms per prediction\n\nAdvantages:\n- MAXIMUM ACCURACY POTENTIAL (state-of-the-art)\n- Captures extremely complex patterns\n- Probabilistic predictions (uncertainty quantification)\n- Research-grade forecasting\n- Can model distribution of outcomes (not just point estimate)\n\nDisadvantages:\n- VERY SLOW TRAINING (hours with GPU, days with CPU)\n- VERY SLOW INFERENCE (100-500ms vs <10ms for others)\n- Requires large dataset (>50K samples minimum)\n- GPU required (not practical on CPU)\n- Complex hyperparameter tuning\n- Can be unstable (divergence during training)\n- Overkill for most use cases\n\nWhen to use:\n- Research projects and experimentation\n- When you have GPU and large dataset\n- Maximum accuracy needed (competitions)\n- When training time not critical (overnight)\n- NEVER for real-time trading (too slow inference)\n\nTypical results:\n- Accuracy: 60-68% (BEST possible, with perfect tuning)\n- Sharpe: 1.6-2.2 (if successful)\n- Training time: 1-8 hours (GPU)\n- Inference: 100-500ms (too slow for 1m/5m trading)\n- Memory: 2-8GB\n\n4.7) DIFFUSION-DDIM:\n\nBehavior:\n- Denoising Diffusion Implicit Model (faster than DDPM)\n- Same architecture, faster sampling\n- Training time: 30 minutes to 6 hours (GPU)\n- Inference: 50-200ms per prediction (2-5× faster than DDPM)\n\nAdvantages:\n- Faster inference than DDPM (still very accurate)\n- Deterministic sampling (reproducible)\n- Similar accuracy to DDPM\n- Preferred over DDPM for production\n\nDisadvantages:\n- Still slow training (hours)\n- Still slow inference (50-200ms)\n- All DDPM disadvantages apply\n\nWhen to use:\n- When DDPM too slow for inference\n- Research with time constraints\n- Prefer this over DDPM (faster, deterministic)\n\nTypical results:\n- Accuracy: 60-68%\n- Training time: 1-6 hours (GPU)\n- Inference: 50-200ms\n\n4.8) SSSD (Structured State Space Diffusion):\n\nBehavior:\n- Advanced diffusion with state space models\n- Captures temporal dependencies better\n- Training time: 1-10 hours (GPU)\n- Inference: 100-400ms\n\nAdvantages:\n- BEST for time series (specialized for sequential data)\n- Long-range dependencies captured\n- Cutting-edge research\n\nDisadvantages:\n- Most complex algorithm\n- Longest training time\n- Experimental (may be unstable)\n- Requires expertise to tune\n\nWhen to use:\n- PhD-level research\n- Maximum accuracy, don't care about time\n- Experimental forecasting\n\nTypical results:\n- Accuracy: 60-70% (best in ideal conditions)\n- Training time: 2-10 hours (GPU)\n- Inference: 100-400ms\n\n5) TYPICAL RANGE / DEFAULT VALUES:\n\n- BEGINNER DEFAULT: ridge (learn here)\n- INTERMEDIATE DEFAULT: lightning (best balance)\n- ADVANCED DEFAULT: lightning or rf (production)\n- RESEARCH DEFAULT: diffusion-ddim (if GPU available)\n\nUsage distribution:\n- 40% use ridge (beginners + fast iteration)\n- 30% use lightning (serious traders)\n- 20% use rf (intermediate users)\n- 10% use diffusion models (researchers)\n\nRecommendation progression:\n1. Start with ridge (1-2 weeks, learn basics)\n2. Graduate to lightning (production system)\n3. Try rf if lightning insufficient\n4. Only try diffusion if you have GPU + time + expertise\n\n6) ADDITIONAL NOTES / BEST PRACTICES:\n\nModel selection strategy:\n1. Always train ridge FIRST (baseline in 30 seconds)\n2. If ridge >55% accuracy: Try lightning (usually +2-4%)\n3. If lightning >58% accuracy: Stick with it (good enough)\n4. If lightning <58%: Problem is features, not algorithm (add more features)\n5. Only try diffusion if you have exhausted feature engineering\n\nSpeed vs Accuracy trade-off:\n- Ridge: 1× speed, 1× accuracy (baseline)\n- Lightning: 10× slower, +2-4% accuracy\n- RF: 20× slower, +2-3% accuracy\n- Diffusion: 100-1000× slower, +4-8% accuracy (if successful)\n\nWhen accuracy stuck:\n- If all models plateau at 52-54%: Market not predictable with current features\n- Solution: Add more/better features, NOT more complex model\n- Complex model + bad features = overfitting\n- Simple model + good features = success\n\nProduction deployment:\n- RECOMMENDED: lightning (best balance)\n- Alternative: ridge (if speed critical, <1ms inference)\n- AVOID: diffusion (too slow for real-time, 100-500ms)\n\nGPU acceleration:\n- Ridge/Lasso/ElasticNet: No GPU benefit (CPU fine)\n- RF: Minimal GPU benefit (CPU fine)\n- Lightning: 2-3× speedup with GPU (optional)\n- Diffusion: GPU REQUIRED (10-50× speedup vs CPU)\n\nEnsemble strategy (ADVANCED):\n- Train ridge + lightning + rf\n- Average predictions (weighted: 0.2 ridge + 0.5 lightning + 0.3 rf)\n- Usually +1-2% accuracy vs single best model\n- Cost: 3× training time\n\nInteraction with other parameters:\n- Complex models (rf, lightning, diffusion) + few features (<20) = WASTE (use ridge)\n- Simple models (ridge) + many features (>100) = OK (but lasso better for feature selection)\n- Diffusion + small dataset (<10K samples) = DISASTER (severe overfitting)\n- Lightning + genetic optimization = VERY SLOW (hours to days)\n\nHow ForexGPT trains models:\n- Loads features from database\n- Splits train/test (80/20)\n- Fits model on training data\n- Evaluates on test data\n- Saves model to artifacts/{model_name}.pkl\n- Metadata includes: algorithm, hyperparameters, performance metrics"
}
