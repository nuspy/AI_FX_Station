{
  "_meta": {
    "language": "en_US",
    "language_name": "English (United States)",
    "version": "1.0.0",
    "last_updated": "2025-01-08",
    "translators": ["ForexGPT Team"]
  },
  
  "_comment_structure": "Keys follow dot notation: tab.widget.element.type (label|tooltip|placeholder|error)",
  
  "app": {
    "title": "ForexGPT - AI-Powered Forex Trading Platform",
    "version": "v2.0.0"
  },
  
  "common": {
    "ok": "OK",
    "cancel": "Cancel",
    "apply": "Apply",
    "close": "Close",
    "save": "Save",
    "load": "Load",
    "delete": "Delete",
    "edit": "Edit",
    "add": "Add",
    "remove": "Remove",
    "browse": "Browse...",
    "reset": "Reset",
    "default": "Default",
    "enabled": "Enabled",
    "disabled": "Disabled",
    "on": "ON",
    "off": "OFF",
    "yes": "Yes",
    "no": "No",
    "error": "Error",
    "warning": "Warning",
    "info": "Information",
    "success": "Success"
  },
  
  "chart": {
    "tab_name": "Chart",
    
    "market_watch": {
      "title": "Market Watch",
      "spread_label": "Spread:",
      "pips_label": "pips",
      "change_label": "Change:"
    },
    
    "vix": {
      "title": "Volatility",
      "label": "VIX:",
      "classification": {
        "complacency": "Complacency",
        "normal": "Normal",
        "concern": "Concern",
        "fear": "Fear"
      }
    },
    
    "order_books": {
      "title": "Order Books",
      "bid_label": "Bid",
      "ask_label": "Ask",
      "spread_label": "Spread:",
      "imbalance_label": "Imbalance:"
    },
    
    "order_flow": {
      "title": "Order Flow Imbalance",
      "bid_volume": "Bid:",
      "ask_volume": "Ask:"
    },
    
    "timeframe": {
      "label": "Timeframe:",
      "tooltip": "Chart Timeframe Selector\n\n1) WHAT IT IS:\nSelect the time period represented by each candlestick on the chart.\n1m = 1-minute candles, 1d = daily candles, etc.\n\nEach timeframe represents a different granularity of price action:\n- Higher timeframes (4h, 1d) = bigger picture, less noise\n- Lower timeframes (1m, 5m) = detailed view, more noise\n\n2) HOW AND WHEN TO USE:\n\nWhen to use different timeframes:\n- Scalping: 1m, 5m (trades lasting 5-60 minutes)\n- Day trading: 15m, 30m, 1h (trades within the same day)\n- Swing trading: 4h, 1d (trades lasting days to weeks)\n- Position trading: 1d, 1w (trades lasting weeks to months)\n\nHow to use (Multi-timeframe analysis):\n1. Start with higher timeframe (1d) to identify major trend\n2. Drop to medium timeframe (1h) to find entry zones\n3. Use lower timeframe (15m) for precise entry timing\n\nRelevance by strategy:\n- Scalping: CRITICAL - must use 1m/5m for entries\n- Day trading: IMPORTANT - 15m/1h is your decision timeframe\n- Swing trading: MODERATE - 4h/1d for entries, lower TF for precision\n- Position trading: LOW - 1d/1w sufficient, lower TF unnecessary\n\n3) WHY TO USE IT:\n- See the same market at different time scales\n- Align your strategy with appropriate granularity\n- Avoid trading against the higher timeframe trend (e.g., don't short on 15m if 1d is strong uptrend)\n- Multi-timeframe confluence increases probability of success\n- Match timeframe to your trading style and available time\n\n4) EFFECTS:\n\n4.1) LOW TIMEFRAMES (1m, 5m):\n\nBehavior:\n- Each candle forms very quickly (1-5 minutes)\n- Extremely detailed price action visible\n- Every small move captured\n- High frequency of signals\n\nAdvantages:\n- Maximum precision for scalping entries\n- Catch micro-patterns and quick reversals\n- Perfect for intraday momentum trading\n- See exact price levels where orders execute\n\nDisadvantages:\n- Very HIGH NOISE - many false signals\n- Requires constant monitoring (can't step away)\n- Spread costs significant on small moves\n- Whipsaws and stop hunts common\n- Stressful and time-consuming\n\nWhen to use:\n- Scalping strategies (5-15 minute holds)\n- Precise entry timing within larger setup\n- Day trading during high volatility sessions\n- When you can actively monitor for hours\n\n4.2) MEDIUM TIMEFRAMES (15m, 30m, 1h):\n\nBehavior:\n- Balanced view of price action\n- Filters out most micro-noise\n- Patterns form over hours\n- Moderate frequency of signals\n\nAdvantages:\n- OPTIMAL balance noise vs signal\n- Reliable patterns (3+ candles = hours of confirmation)\n- Good for part-time traders (check every hour)\n- Widely followed by institutions (support/resistance respected)\n- Lifestyle-friendly (not glued to screen)\n\nDisadvantages:\n- Slower reaction to news events (compared to 1m/5m)\n- Larger stop losses needed (more pips)\n- Fewer opportunities (1-2 setups per day vs 10+ on 1m)\n- May miss very fast intraday moves\n\nWhen to use:\n- Day trading (trades lasting 1-8 hours)\n- Short-term swing trading (overnight to 3 days)\n- PRIMARY timeframe for most retail traders\n- When checking chart 1-3 times per hour is feasible\n\n4.3) HIGH TIMEFRAMES (4h, 1d, 1w):\n\nBehavior:\n- Big picture view of market\n- Very smooth trends, minimal noise\n- Patterns develop over days/weeks\n- Low frequency of signals\n\nAdvantages:\n- MAXIMUM RELIABILITY - patterns proven over days\n- Major levels highly respected by institutions\n- Check ONCE per day (or even weekly)\n- Perfect for busy people with day jobs\n- Low stress, no need for constant monitoring\n\nDisadvantages:\n- VERY SLOW to react (4 hours per candle on 4h chart!)\n- Wide stop losses required (50-200 pips)\n- Very few opportunities (1-2 setups per month)\n- Requires large account (wide stops = more capital)\n- Boring for active traders\n\nWhen to use:\n- Swing trading (holds lasting 3-30 days)\n- Position trading (weeks to months)\n- Trend identification for lower timeframe entries\n- When you can only check markets once per day\n- When you want lifestyle freedom\n\n5) TYPICAL RANGE / DEFAULT VALUES:\n\n- Minimum: 1m (highest granularity available)\n- BEGINNER DEFAULT: 1h (best balance for learning)\n- SCALPER DEFAULT: 5m (detailed but not too noisy)\n- DAY TRADER DEFAULT: 15m or 1h\n- SWING TRADER DEFAULT: 4h or 1d\n- POSITION TRADER DEFAULT: 1d or 1w\n- Maximum: 1M (monthly, rarely used)\n\nUsage distribution:\n- Beginners: 80% should start with 1h timeframe\n- Intermediate: 40% use 15m-1h, 40% use 4h-1d, 20% use 1m-5m\n- Advanced: Spread across all timeframes based on strategy\n\nRecommendation by experience:\n- Beginner: Start with 1h ONLY for first 6 months (master one timeframe)\n- Intermediate: Add 4h (trend filter) and 15m (entry timing)\n- Advanced: Full multi-timeframe analysis (1d → 4h → 1h → 15m)\n\n6) ADDITIONAL NOTES / BEST PRACTICES:\n\nMulti-timeframe workflow (THE RIGHT WAY):\n1. Start on Daily (1d): What's the major trend? Up, down, or range?\n2. Drop to 4H: Confirm trend, identify swing structure\n3. Drop to 1H: Find entry setups aligned with 4H/Daily trend\n4. Drop to 15m: Precise entry point (breakout, pullback, etc.)\n\nCommon mistakes to AVOID:\n- Trading lower TF against higher TF trend (fighting the trend = lose)\n- Using only 1 timeframe (missing big picture)\n- Too many timeframes (analysis paralysis, conflicting signals)\n- Switching timeframes randomly (lack of consistency)\n\nOptimal timeframe combinations:\n- Scalping: 5m (primary) + 15m (trend filter)\n- Day trading: 15m (primary) + 1h (trend) + 5m (entry)\n- Swing trading: 4h (primary) + 1d (trend) + 1h (entry)\n- Position: 1d (primary) + 1w (trend) + 4h (entry)\n\nHow ForexGPT uses timeframes:\n- Indicators calculated on ALL timeframes simultaneously\n- Multi-timeframe ensemble model combines predictions from 6 timeframes\n- Regime detection uses 4h and 1d for macro trend\n- Signals generated on 1h, refined on 15m for entry\n\nKeyboard shortcuts:\n- Ctrl+Up: Next higher timeframe\n- Ctrl+Down: Next lower timeframe\n- Ctrl+1 through Ctrl+9: Jump to specific timeframes"
    },
    
    "symbol": {
      "label": "Symbol:",
      "tooltip": "Symbol Selector\n\n1) WHAT IT IS:\nChoose which currency pair (or financial instrument) to display and analyze.\nSymbols formatted as BASE/QUOTE (e.g., EUR/USD = Euro vs US Dollar).\n\nExample: EUR/USD = 1.0850 means 1 Euro = 1.0850 US Dollars\n\n2) HOW AND WHEN TO USE:\n\nWhen to trade different symbols:\n- EUR/USD: Best during London/NY sessions (13:00-22:00 GMT)\n- USD/JPY: Best during Tokyo/NY sessions (00:00-09:00, 13:00-22:00 GMT)\n- GBP/USD: Best during London session (07:00-16:00 GMT)\n- AUD/USD: Best during Sydney/Asian session (22:00-08:00 GMT)\n\nHow to use:\n- Click dropdown to see all available symbols\n- Type first letters to quick-search (e.g., \"EU\" finds EUR/USD)\n- Select symbol to load chart and data\n- Right-click for advanced options (add to watchlist, etc.)\n\nRelevance by experience:\n- Beginner: Trade ONLY EUR/USD for first 6-12 months (master ONE pair)\n- Intermediate: Add 1-2 more majors (GBP/USD, USD/JPY)\n- Advanced: 5-8 pairs including crosses, correlation-aware\n\n3) WHY TO USE IT:\n- Access different markets and opportunities\n- Diversification (don't put all trades in one pair)\n- Correlation analysis (EUR/USD and GBP/USD move together)\n- Find the pair that's moving (opportunity scanning)\n- Each pair has unique characteristics (volatility, spread, behavior)\n\n4) EFFECTS:\n\n4.1) MAJOR PAIRS (Tight spreads, high liquidity):\n\nEUR/USD (Euro vs US Dollar):\n- Most traded pair globally (~30% of FX volume)\n- Spread: 0.0-1.0 pips (tightest available)\n- Volatility: MEDIUM (50-100 pips/day)\n- Best for: Beginners, scalping, technical analysis\n\nAdvantages:\n- Tightest spreads (lowest cost)\n- Highest liquidity (instant fills, no slippage)\n- Smooth trends, respects technical levels\n- Most educational resources available\n\nDisadvantages:\n- Can be slow during Asian session\n- News-driven (ECB and Fed announcements cause spikes)\n\nWhen to trade:\n- London/NY overlap (13:00-17:00 GMT) - BEST\n- European session (07:00-16:00 GMT) - Good\n- Asian session (22:00-08:00 GMT) - Slow\n\nGBP/USD (British Pound vs US Dollar - \"The Cable\"):\n- 2nd most popular major\n- Spread: 1-3 pips\n- Volatility: HIGH (100-200 pips/day)\n- Best for: Experienced traders (wild swings)\n\nAdvantages:\n- High volatility = big profit potential\n- Strong trends when it moves\n- Active during London session\n\nDisadvantages:\n- Sharp reversals (can whipsaw stops)\n- Brexit news causes extreme volatility\n- Higher spread than EUR/USD\n- Requires wider stops (50-100 pips)\n\nWhen to trade:\n- London session (07:00-16:00 GMT) - BEST\n- Avoid during major UK news (can spike 50+ pips instantly)\n\n4.2) MINOR PAIRS (Moderate spreads, good liquidity):\n\nEUR/GBP (Euro vs British Pound):\n- European cross (no USD)\n- Spread: 1.5-3 pips\n- Volatility: LOW (30-60 pips/day)\n- Best for: Range trading\n\nAdvantages:\n- Mean-reverting (tends to stay in range)\n- Predictable behavior\n- Less affected by USD news\n\nDisadvantages:\n- Rarely trends (boring for trend traders)\n- Lower volatility = smaller profit potential\n\nWhen to use:\n- When both EUR and GBP have clear directional bias\n- Range trading strategies (buy support, sell resistance)\n\n4.3) EXOTIC PAIRS (Wide spreads, low liquidity - CAUTION):\n\nEUR/TRY, USD/MXN, USD/ZAR:\n- Emerging market currencies\n- Spread: 10-100+ pips (!)\n- Volatility: EXTREME\n- Best for: Advanced traders ONLY\n\nAdvantages:\n- NONE for most traders (avoid these)\n\nDisadvantages:\n- Extremely wide spreads (cost prohibitive)\n- Low liquidity (slippage, gaps, poor fills)\n- High risk of flash crashes\n- Broker manipulation more common\n- Overnight gaps of 100-500 pips possible\n\nWhen to use:\n- Almost NEVER (unless you're expert in emerging markets)\n- Requires massive stop losses (500-1000 pips)\n\n5) TYPICAL RANGE / DEFAULT VALUES:\n\n- DEFAULT for beginners: EUR/USD (start here, master it)\n- DEFAULT for scalpers: EUR/USD or GBP/USD (tight spreads or high volatility)\n- DEFAULT for day traders: EUR/USD, GBP/USD, USD/JPY (choose 2-3)\n- DEFAULT for swing traders: Any major pair based on setup\n\nNumber of pairs to trade:\n- Beginner: 1 pair only (EUR/USD recommended)\n- Intermediate: 2-4 major pairs\n- Advanced: 5-8 pairs including crosses\n\nUsage distribution:\n- 60% of traders primarily trade EUR/USD\n- 20% primarily GBP/USD\n- 10% USD/JPY\n- 10% other pairs\n\n6) ADDITIONAL NOTES / BEST PRACTICES:\n\nPair correlations (IMPORTANT for risk management):\n- EUR/USD and USD/CHF: -95% correlation (nearly perfect inverse)\n- EUR/USD and GBP/USD: +80% correlation (move together)\n- AUD/USD and NZD/USD: +85% correlation (both commodity currencies)\n\nDON'T trade correlated pairs simultaneously:\n- EUR/USD long + USD/CHF long = positions cancel out (waste spread)\n- EUR/USD long + GBP/USD long = DOUBLE exposure (2× risk if wrong)\n\nDO trade for diversification:\n- EUR/USD long + USD/JPY short = uncorrelated (true diversification)\n\nSession optimization:\n- Asian (22:00-08:00 GMT): USD/JPY, AUD/USD, NZD/USD\n- European (07:00-16:00 GMT): EUR/USD, GBP/USD, EUR/GBP\n- NY (13:00-22:00 GMT): All USD pairs\n- OVERLAP (13:00-17:00 GMT): BEST time, all majors active\n\nHow ForexGPT handles symbols:\n- Each symbol has independent trained models\n- Correlation matrix updated real-time\n- Portfolio risk calculated across all positions\n- Alert when opening correlated positions (>80% correlation)\n\nKeyboard shortcuts:\n- Ctrl+S: Open symbol selector\n- Type letters: Quick search (\"GBP\" finds GBP/USD)\n- Ctrl+→/←: Cycle through watchlist"
    }
  },
  
  "training": {
    "tab_name": "Training",
    
    "model_name": {
      "label": "Model Name:",
      "placeholder": "Auto-generate from features",
      "tooltip": "Model Name (Optional Custom Name)\n\n1) WHAT IT IS:\nCustom name for the trained model file that will be saved.\nIf left empty, name is auto-generated from features (e.g., \"EUR_USD_1h_ridge_ATR_RSI_MACD_20250108\").\n\nExample: \"my_eurusd_scalping_model_v2\" or \"production_swing_gbpusd\"\n\n2) HOW AND WHEN TO USE:\n\nWhen to set custom name:\n- Production models you'll use for real trading\n- A/B testing (e.g., \"model_A_conservative\" vs \"model_B_aggressive\")\n- Versioning (\"v1\", \"v2\", \"v3\" as you iterate)\n- Organizing multiple strategies\n\nWhen to leave empty (auto-generate):\n- Quick experiments and testing\n- Don't care about the name\n- Want descriptive name based on features\n\nHow to use:\n- Type any name (letters, numbers, underscore, dash allowed)\n- Avoid special characters (/, \\, *, ?, <, >, |)\n- Keep under 50 characters for readability\n\n3) WHY TO USE IT:\n- Easy identification of models later\n- Organize models by strategy/version\n- Professional workflow (\"production_v3\" vs random auto-name)\n- A/B testing and comparison\n- Team collaboration (everyone knows which model is which)\n\n4) EFFECTS:\n\n4.1) EMPTY (Auto-generate name):\n\nBehavior:\n- System generates name from: symbol_timeframe_algorithm_features_date\n- Example: \"EURUSD_1h_ridge_ATR_RSI_MACD_Bollinger_20250108_143022\"\n- Very long but descriptive name\n\nAdvantages:\n- FULLY descriptive (you know exactly what's in the model)\n- No thinking required\n- Never conflicts (timestamp unique)\n- Good for experiments\n\nDisadvantages:\n- Long and ugly names\n- Hard to remember\n- Not professional for production\n\nWhen to use:\n- Quick tests and experiments\n- Throwaway models\n- Learning phase\n\n4.2) CUSTOM NAME SET:\n\nBehavior:\n- Model saved with your custom name\n- Example: \"production_eurusd_v3\"\n- Metadata still contains all features info\n\nAdvantages:\n- Clean, professional names\n- Easy to remember and reference\n- Good for production deployment\n- Version control\n- Team-friendly\n\nDisadvantages:\n- Need to track what features are in each version\n- Risk of name conflicts (if not careful with versioning)\n- Requires discipline (good naming convention)\n\nWhen to use:\n- Production models\n- Models you'll keep and use long-term\n- A/B testing\n- Professional workflow\n\n5) TYPICAL RANGE / DEFAULT VALUES:\n\n- DEFAULT: Empty (auto-generate)\n- Most users: 70% leave empty, 30% use custom names\n- Production use: 90% use custom names\n\nRecommended naming convention:\n- Format: \"[environment]_[symbol]_[strategy]_v[version]\"\n- Examples:\n  * \"prod_eurusd_scalp_v3\"\n  * \"test_gbpusd_swing_v1\"\n  * \"paper_usdjpy_trend_v2\"\n\n6) ADDITIONAL NOTES / BEST PRACTICES:\n\nNaming conventions for teams:\n- Include environment: prod, test, dev, paper\n- Include symbol: eurusd, gbpusd (no slash)\n- Include strategy type: scalp, swing, trend\n- Include version: v1, v2, v3\n- Optional: Include date: 20250108\n\nExamples:\n- \"prod_eurusd_scalp_v3_20250108\" (production scalping model v3)\n- \"test_gbpusd_swing_v1\" (test swing trading model)\n- \"research_multi_symbol_ensemble_v2\" (research multi-pair model)\n\nDO NOT use:\n- Spaces (use underscore instead)\n- Special characters (/, \\, *, ?, etc.)\n- Very long names (>50 chars hard to read)\n\nVersion control:\n- Always increment version when retraining\n- Keep old versions for rollback\n- Document what changed in each version\n\nHow ForexGPT uses model names:\n- Saved to database (model_metadata table)\n- File saved as: artifacts/{model_name}.pkl\n- Searchable in model selection dropdowns\n- Displayed in backtest results"
    },
    
    "symbol": {
      "label": "Symbol:",
      "tooltip": "[SAME DETAILED TOOLTIP AS CHART.SYMBOL - SEE ABOVE FOR FULL 6-SECTION STRUCTURE]"
    },
    
    "timeframe": {
      "label": "Base TF:",
      "tooltip": "[SAME AS CHART.TIMEFRAME.TOOLTIP - Training uses same timeframe concept]"
    },
    
    "days": {
      "label": "Days:",
      "tooltip": "Days History - Training Dataset Size\n\n1) WHAT IT IS:\nNumber of historical days to include in the training dataset.\nDetermines how much past data the model will learn from.\n\nExample: 7 days on 1m timeframe = ~10,080 samples (7 × 24 × 60)\n         30 days on 1h timeframe = ~720 samples (30 × 24)\n\nFormula: Total samples ≈ days × (1440 / timeframe_minutes)\n\n2) HOW AND WHEN TO USE:\n\nWhen to use MORE days (90-365+):\n- Production models (need best generalization)\n- When you have time for long training (hours)\n- Capturing seasonal patterns and cycles\n- Stable markets with long-term patterns\n- Models you'll use for real money\n\nWhen to use FEWER days (1-30):\n- Quick experiments and prototyping\n- Testing new features or algorithms\n- Fast iteration during research\n- Very recent market regime (only want latest behavior)\n- Limited computational resources\n\nHow to choose:\n- Rule of thumb: At least 1000 samples per feature\n- Example: 50 features × 20 samples = 1000 min samples\n- On 1m TF: 1000 samples = ~17 hours ≈ 1 day minimum\n- On 1h TF: 1000 samples = ~42 days minimum\n\nRelevance by use case:\n- Scalping (1m-5m): 7-30 days sufficient (capture recent patterns)\n- Day trading (15m-1h): 30-90 days recommended\n- Swing trading (4h-1d): 90-365 days optimal\n- Research/production: 180-730 days (1-2 years)\n\n3) WHY TO USE IT:\n- Control train/test balance (more data = better generalization)\n- Capture enough market conditions (trending, ranging, volatile, calm)\n- Seasonal patterns (need ≥365 days for yearly cycles)\n- Prevent overfitting (too little data = model memorizes noise)\n- Statistical significance (more samples = more reliable metrics)\n\n4) EFFECTS:\n\n4.1) VERY LOW (1-3 days):\n\nBehavior:\n- Only most recent 1-3 days used for training\n- Very small dataset (1,440-4,320 samples on 1m)\n- Training completes in seconds\n\nAdvantages:\n- EXTREMELY FAST training (seconds)\n- Tests new ideas instantly\n- Only recent market behavior (if regime just changed)\n- Low memory usage\n- Quick iteration cycle\n\nDisadvantages:\n- HIGH OVERFITTING RISK (model memorizes specific recent patterns)\n- No seasonal/cyclical patterns captured\n- Unstable metrics (small sample = high variance)\n- Poor generalization (fails on unseen conditions)\n- Not suitable for production\n\nWhen to use:\n- Quick prototyping only\n- Testing if code works\n- Debugging new features\n- NEVER for production trading\n\nTypical results:\n- Training accuracy: 70-90% (looks great!)\n- Out-of-sample accuracy: 45-55% (disaster - worse than random)\n- Model fails within hours/days of deployment\n\n4.2) LOW (7-30 days):\n\nBehavior:\n- 1-4 weeks of historical data\n- Moderate dataset size (10K-43K samples on 1m)\n- Training time: 30 seconds to 5 minutes (Ridge/Lasso)\n\nAdvantages:\n- FAST training (minutes for most algorithms)\n- Captures recent 1-month market regime\n- Sufficient for initial experiments\n- Good for models you'll retrain frequently (daily/weekly)\n- Balances speed and quality\n\nDisadvantages:\n- Limited market conditions captured (may miss volatility extremes)\n- No long-term patterns (seasonal, cycles)\n- Still some overfitting risk\n- Walk-forward validation limited (not enough out-of-sample data)\n\nWhen to use:\n- DEFAULT for beginners (good starting point)\n- Algorithms you'll retrain frequently\n- When training time matters (minutes not hours)\n- Markets with fast regime changes (crypto)\n\nTypical results:\n- Training accuracy: 60-70%\n- Out-of-sample accuracy: 52-58% (decent edge)\n- Model useful for 1-4 weeks before retraining needed\n\n4.3) MEDIUM (30-90 days):\n\nBehavior:\n- 1-3 months of historical data\n- Large dataset (43K-130K samples on 1m)\n- Training time: 2-15 minutes (Ridge/Lasso), 10-60 minutes (RF/XGBoost)\n\nAdvantages:\n- GOOD GENERALIZATION (multiple market conditions)\n- Captures 3-month trends and corrections\n- Sufficient walk-forward validation data\n- Production-quality for short-term strategies\n- Balances quality and training time\n\nDisadvantages:\n- Longer training time (5-60 minutes)\n- May include outdated regime (if market changed)\n- Still no full seasonal cycles\n\nWhen to use:\n- RECOMMENDED for day trading models\n- Production models for strategies <1 month hold\n- When you have 10-60 minutes for training\n- Stable markets (forex majors)\n\nTypical results:\n- Training accuracy: 58-65%\n- Out-of-sample accuracy: 54-60% (solid edge)\n- Model useful for 1-3 months\n\n4.4) HIGH (90-180 days):\n\nBehavior:\n- 3-6 months of historical data\n- Very large dataset (130K-260K samples on 1m)\n- Training time: 5-30 minutes (Ridge/Lasso), 30 minutes-2 hours (RF/XGBoost/LightGBM)\n\nAdvantages:\n- EXCELLENT generalization\n- Captures multiple trend cycles\n- Robust to regime changes\n- High-quality walk-forward validation (3-6 month out-of-sample)\n- Near-seasonal coverage (approaching 1 year)\n\nDisadvantages:\n- SLOW training (30 min to 2 hours)\n- May dilute recent patterns if market changed recently\n- High memory usage (GB RAM for complex models)\n\nWhen to use:\n- Production models for serious trading\n- Swing trading strategies (multi-day holds)\n- When training overnight is acceptable\n- Capturing quarterly patterns\n\nTypical results:\n- Training accuracy: 56-62%\n- Out-of-sample accuracy: 54-59% (reliable edge)\n- Model useful for 3-6 months\n\n4.5) VERY HIGH (365-730+ days):\n\nBehavior:\n- 1-2 years of historical data\n- Massive dataset (525K-1M+ samples on 1m)\n- Training time: 15 minutes-1 hour (Ridge/Lasso), 1-8 hours (RF/XGBoost), DAYS (neural networks)\n\nAdvantages:\n- MAXIMUM generalization\n- Full seasonal cycles captured (12-month patterns)\n- All market conditions (bull, bear, sideways, volatile, calm)\n- Institutional-quality backtesting\n- Most robust to regime changes\n\nDisadvantages:\n- VERY SLOW training (hours to days)\n- May include irrelevant old data (market structure changed)\n- Requires powerful computer (high RAM, GPU recommended)\n- Potential concept drift (2-year-old patterns may not apply now)\n\nWhen to use:\n- Research and PhD-level work\n- Position trading (multi-month holds)\n- When you need maximum confidence\n- Training models you'll use for years\n- Institutional/fund deployment\n\nTypical results:\n- Training accuracy: 54-58%\n- Out-of-sample accuracy: 53-57% (conservative but reliable)\n- Model useful for 6-12 months (very stable)\n\n5) TYPICAL RANGE / DEFAULT VALUES:\n\n- Minimum: 1 day (only for testing code, never production)\n- BEGINNER DEFAULT: 7 days (fast iteration, learn quickly)\n- INTERMEDIATE DEFAULT: 30 days (balanced)\n- ADVANCED DEFAULT: 90 days (production quality)\n- PROFESSIONAL DEFAULT: 180-365 days (institutional quality)\n- Maximum: 3650 days (10 years, rarely useful - concept drift)\n\nDistribution by experience:\n- Beginners: 80% use 7 days, 15% use 30 days, 5% use 90+\n- Intermediate: 30% use 7-30 days, 50% use 30-90 days, 20% use 90+\n- Advanced: 10% use <30 days, 40% use 30-90 days, 50% use 90-365 days\n\nDistribution by timeframe:\n- 1m TF: 7-30 days typical (enough samples, recent patterns)\n- 5m-15m TF: 14-60 days typical\n- 1h TF: 60-180 days typical (need more calendar days for same sample count)\n- 4h-1d TF: 180-730 days typical (each sample is 4h or 1 day!)\n\nRule of thumb:\n- Aim for 5,000-50,000 samples total\n- Calculate: samples = days × (1440 / timeframe_minutes)\n- Example 1m: 7 days = 10,080 samples ✓\n- Example 1h: 7 days = 168 samples ✗ (too few!)\n- Example 1h: 90 days = 2,160 samples ✓\n\n6) ADDITIONAL NOTES / BEST PRACTICES:\n\nCalculating required days:\n- Minimum samples needed: 1000 samples per feature\n- Example: 50 features = 50,000 samples minimum\n- On 1m: 50,000 / 1440 ≈ 35 days minimum\n- On 1h: 50,000 / 24 ≈ 2,083 days (5.7 years!) - reduce features!\n\nConcept drift consideration:\n- Markets change over time (2008 crisis vs 2020 COVID vs 2023 AI era)\n- Very old data may hurt more than help\n- Recommendation: Use 1-2 years maximum for most models\n- Exception: If you're specifically modeling long-term cycles\n\nMemory requirements:\n- 1 day 1m data ≈ 1-5 MB (raw OHLCV)\n- 1 day 1m data with 100 features ≈ 50-200 MB (in memory)\n- 365 days with 100 features ≈ 18-73 GB RAM (!)\n- Solution: Use fewer features or higher timeframes\n\nTraining time estimates (Ridge regression baseline):\n- 7 days: 10 seconds\n- 30 days: 30 seconds\n- 90 days: 2 minutes\n- 180 days: 5 minutes\n- 365 days: 15 minutes\n\n(Random Forest 10× slower, XGBoost 5× slower, Neural Networks 100× slower)\n\nRetraining frequency:\n- 7 days model: Retrain weekly\n- 30 days model: Retrain monthly\n- 90 days model: Retrain quarterly\n- 365 days model: Retrain 1-2× per year\n\nInteraction with other parameters:\n- More days + more features = VERY SLOW (exponential time)\n- More days + walk-forward validation = need even MORE days\n  (Example: 90 days with 30-day out-of-sample = need 120 days total)\n- More days + genetic optimization = EXTREMELY SLOW (days × generations × population)\n\nHow ForexGPT uses days parameter:\n- Queries database: SELECT * FROM bars WHERE ts_utc >= (now - days)\n- Splits into train/test (default 80/20)\n- Walk-forward validation uses rolling windows within this period\n- Artifacts saved with training period in metadata"
    },
    
    "horizon": {
      "label": "Horizon:",
      "tooltip": "Prediction Horizon - How Far Ahead to Predict\n\n1) WHAT IT IS:\nNumber of bars (candlesticks) into the future that the model predicts.\nHorizon = 1: Predict next bar only\nHorizon = 4: Predict 4 bars ahead\nHorizon = 12: Predict 12 bars ahead\n\nExample on 15m timeframe:\n- Horizon 1 = 15 minutes ahead\n- Horizon 4 = 1 hour ahead (4 × 15m)\n- Horizon 12 = 3 hours ahead (12 × 15m)\n\nExample on 1h timeframe:\n- Horizon 1 = 1 hour ahead\n- Horizon 4 = 4 hours ahead\n- Horizon 24 = 1 day ahead (24 × 1h)\n\n2) HOW AND WHEN TO USE:\n\nWhen to use SHORT horizon (1-2 bars):\n- Scalping strategies (need immediate prediction)\n- High-frequency trading (1m-5m timeframes)\n- Maximum accuracy (easier to predict near future)\n- When you can monitor and adjust quickly\n- Tight stop losses (need fast signals)\n\nWhen to use MEDIUM horizon (3-6 bars):\n- Day trading (15m-1h timeframes)\n- Balanced accuracy and forward visibility\n- Gives time to analyze and enter position\n- Most common use case (sweet spot)\n\nWhen to use LONG horizon (12-48+ bars):\n- Swing trading (4h-1d timeframes)\n- Position trading (want multi-day forecast)\n- When you can't monitor continuously\n- Avoid noise and micromanagement\n- Wider stop losses acceptable\n\nHow to choose:\n- Match to your trading style timeframe:\n  * Scalping (1m): horizon 1-2 (1-2 minutes ahead)\n  * Day trading (15m): horizon 4-8 (1-2 hours ahead)\n  * Swing trading (1h): horizon 12-24 (12h-1 day ahead)\n  * Position (4h-1d): horizon 6-12 (1-3 days ahead)\n\nRelevance by experience:\n- Beginner: Start with horizon 4 (gives time to think)\n- Intermediate: 2-6 (optimize for your style)\n- Advanced: Multi-horizon ensemble (combine 1, 4, 12)\n\n3) WHY TO USE IT:\n- Control prediction timeframe (match your hold period)\n- Trade-off accuracy vs forward visibility\n- Avoid overtrading (longer horizon = fewer signals)\n- Match model predictions to execution speed\n- Different strategies need different lookahead\n\n4) EFFECTS:\n\n4.1) VERY SHORT (Horizon 1):\n\nBehavior:\n- Model predicts ONLY next bar\n- Immediate signal (no lookahead)\n- Maximum frequency of predictions (every bar)\n\nAdvantages:\n- HIGHEST ACCURACY (near future easiest to predict)\n- Immediate actionable signals\n- Catches micro-trends and reversals\n- Perfect for scalping\n- Lowest prediction error (MAE/RMSE)\n\nDisadvantages:\n- VERY HIGH NOISE (every tiny wiggle generates signal)\n- Overtrading risk (100+ signals per day)\n- No forward visibility (blind beyond next bar)\n- Whipsaw risk (direction changes quickly)\n- Requires constant monitoring\n- High transaction costs (frequent entries/exits)\n\nWhen to use:\n- Scalping on 1m-5m timeframes\n- Automated systems (can execute instantly)\n- When you want absolute maximum accuracy\n- High-frequency strategies\n\nTypical results:\n- Prediction accuracy: 58-65% (best possible)\n- Sharpe ratio: 1.2-1.6 (many trades, small wins)\n- Win rate: 54-60%\n- Signals per day: 100-500 (on 1m TF)\n\n4.2) SHORT (Horizon 2-4):\n\nBehavior:\n- Model predicts 2-4 bars ahead\n- On 15m TF: 30 minutes to 1 hour lookahead\n- On 1h TF: 2-4 hours lookahead\n- Moderate signal frequency\n\nAdvantages:\n- HIGH ACCURACY (still close future)\n- Filters out some noise (vs horizon 1)\n- Gives time to analyze and enter\n- Good balance accuracy vs visibility\n- RECOMMENDED DEFAULT for most users\n- Catches intraday trends\n\nDisadvantages:\n- Still requires regular monitoring (hourly)\n- Some overtrading possible\n- Moderate transaction costs\n\nWhen to use:\n- Day trading on 15m-1h timeframes\n- DEFAULT for beginners (sweet spot)\n- When you check markets every 1-2 hours\n- Intraday strategies\n\nTypical results:\n- Prediction accuracy: 54-60% (very good)\n- Sharpe ratio: 1.4-1.8 (optimal risk/reward)\n- Win rate: 52-58%\n- Signals per day: 20-50 (on 15m TF)\n\n4.3) MEDIUM (Horizon 6-12):\n\nBehavior:\n- Model predicts 6-12 bars ahead\n- On 1h TF: 6-12 hours lookahead\n- On 4h TF: 1-2 days lookahead\n- Lower signal frequency\n\nAdvantages:\n- FILTERS NOISE WELL (only significant moves predicted)\n- Forward visibility (can plan entries/exits)\n- Catches swing trends (multi-hour to multi-day)\n- Fewer false signals\n- Lower transaction costs (fewer trades)\n- Can check markets 1-2× per day\n\nDisadvantages:\n- LOWER ACCURACY (far future harder to predict)\n- Prediction error increases (MAE/RMSE higher)\n- May miss short-term reversals\n- Slower to react to regime changes\n\nWhen to use:\n- Swing trading on 1h-4h timeframes\n- When you can't monitor constantly\n- Want fewer, higher-quality signals\n- Overnight holds acceptable\n\nTypical results:\n- Prediction accuracy: 51-56% (good edge)\n- Sharpe ratio: 1.3-1.7\n- Win rate: 50-55%\n- Signals per day: 5-15 (on 1h TF)\n\n4.4) LONG (Horizon 24-48):\n\nBehavior:\n- Model predicts 1-2 days ahead (on 1h TF)\n- Or 1-2 weeks ahead (on 4h TF)\n- Very low signal frequency\n\nAdvantages:\n- MAXIMUM NOISE FILTERING (only major trends)\n- Very forward-looking (plan multi-day trades)\n- Minimal overtrading\n- Very low transaction costs\n- Check markets once per day (lifestyle-friendly)\n- Captures large trend moves\n\nDisadvantages:\n- LOWEST ACCURACY (distant future very uncertain)\n- High prediction error\n- Misses many short-term opportunities\n- Slow to adapt to sudden regime changes\n- Requires wide stop losses (gives room for noise)\n\nWhen to use:\n- Position trading on 4h-1d timeframes\n- When you have day job (can't monitor)\n- Want lifestyle freedom (not glued to screen)\n- Patient trader (comfortable holding days/weeks)\n\nTypical results:\n- Prediction accuracy: 50-54% (small edge)\n- Sharpe ratio: 1.1-1.5\n- Win rate: 48-53%\n- Signals per week: 3-10 (on 4h TF)\n\n5) TYPICAL RANGE / DEFAULT VALUES:\n\n- Minimum: 1 bar (immediate next prediction)\n- BEGINNER DEFAULT: 4 bars (good balance)\n- SCALPER DEFAULT: 1-2 bars\n- DAY TRADER DEFAULT: 4-6 bars\n- SWING TRADER DEFAULT: 12-24 bars\n- POSITION TRADER DEFAULT: 24-48 bars\n- Maximum: 96+ bars (4 days on 1h, rarely accurate)\n\nDistribution by timeframe:\n- 1m TF: horizon 1-4 typical (1-4 minutes ahead)\n- 5m TF: horizon 2-6 typical (10-30 minutes ahead)\n- 15m TF: horizon 4-8 typical (1-2 hours ahead)\n- 1h TF: horizon 4-12 typical (4-12 hours ahead)\n- 4h TF: horizon 6-12 typical (1-2 days ahead)\n- 1d TF: horizon 3-7 typical (3-7 days ahead)\n\nDistribution by experience:\n- Beginners: 70% use horizon 4, 20% use 1-2, 10% use 8+\n- Intermediate: 40% use 2-4, 40% use 4-8, 20% use 12+\n- Advanced: 20% use 1-2, 40% use 4-8, 40% use multi-horizon ensemble\n\nAccuracy degradation:\n- Horizon 1: ~60% accuracy\n- Horizon 4: ~56% accuracy (lose 4%)\n- Horizon 12: ~53% accuracy (lose 7%)\n- Horizon 24: ~51% accuracy (lose 9%)\n- Horizon 48: ~50% accuracy (no edge!)\n\n6) ADDITIONAL NOTES / BEST PRACTICES:\n\nThe accuracy-visibility trade-off:\n- Shorter horizon = higher accuracy, less visibility\n- Longer horizon = lower accuracy, more visibility\n- Sweet spot: horizon where accuracy still >55% but gives enough time to act\n- For most: horizon 4-6 bars optimal\n\nCalculating real-world time:\n- Real time = horizon × timeframe_minutes\n- Example: horizon 4 on 15m TF = 4 × 15 = 60 minutes (1 hour)\n- Example: horizon 12 on 1h TF = 12 × 1 = 12 hours\n\nMatching horizon to execution:\n- If you need 5 minutes to analyze and enter: horizon ≥ 1 (on 5m TF)\n- If you need 1 hour to analyze: horizon ≥ 4 (on 15m TF)\n- If you check markets once per day: horizon ≥ 24 (on 1h TF)\n\nMulti-horizon ensemble (ADVANCED):\n- Train 3 models: horizon 1, 4, 12\n- Combine predictions (weighted average or voting)\n- Horizon 1: 50% weight (accuracy)\n- Horizon 4: 30% weight (balance)\n- Horizon 12: 20% weight (trend confirmation)\n- Result: Best of all worlds (accuracy + visibility)\n\nInteraction with other parameters:\n- Longer horizon + more features = better (captures complex patterns)\n- Longer horizon + more days = CRITICAL (need enough data to learn long-term patterns)\n- Horizon 24 with only 7 days data = DISASTER (not enough examples of 24-bar sequences)\n- Rule: days × (1440/TF) / horizon ≥ 200 examples minimum\n  Example: horizon 12, 1h TF → need 12 × 200 / 24 = 100 days minimum\n\nStop loss sizing:\n- Horizon 1: tight stops (5-10 pips)\n- Horizon 4: medium stops (10-20 pips)\n- Horizon 12: wide stops (20-40 pips)\n- Horizon 24: very wide stops (40-80 pips)\n- Longer horizon needs room for noise between now and target\n\nHow ForexGPT uses horizon:\n- Target variable: y = close[t+horizon] - close[t] (price change)\n- Or: y = sign(close[t+horizon] - close[t]) (direction)\n- Model learns: f(features[t]) → prediction[t+horizon]\n- At inference: Given current features, predict horizon bars ahead\n- Trading engine: Uses prediction to decide entry NOW for exit in horizon bars"
    },
    
    "model": {
      "label": "Model:",
      "tooltip": "[SEE temp_model_tooltip.txt - Model Algorithm Selection with 8 algorithms documented (ridge, lasso, elasticnet, rf, lightning, diffusion-ddpm, diffusion-ddim, sssd) - 4000+ words covering speed/accuracy trade-offs]"
    },
    
    "encoder": {
      "label": "Encoder:",
      "tooltip": "Feature Encoder - Preprocessing and Dimensionality Reduction\n\n1) WHAT IT IS:\nTransformation applied to features before training the model.\nReduces dimensionality, removes redundancy, and improves training.\n\nAvailable encoders:\n- none: No encoding (use raw features)\n- pca: Principal Component Analysis (linear dimensionality reduction)\n- autoencoder: Neural network encoder (non-linear dimensionality reduction)\n- vae: Variational Autoencoder (probabilistic encoder)\n- latents: Latent space from pre-trained diffusion model\n\n2) HOW AND WHEN TO USE:\n\nWhen to use NONE (no encoding):\n- Beginner-friendly (simplest, most interpretable)\n- Few features (<50)\n- Linear models (ridge, lasso, elasticnet)\n- When features already well-engineered\n- Fast iteration and debugging\n\nWhen to use PCA:\n- Many correlated features (>50)\n- Want faster training (reduced dimensions)\n- Need to visualize feature space (2D/3D projection)\n- Linear relationships sufficient\n- Production systems (fast + simple)\n\nWhen to use AUTOENCODER:\n- Complex non-linear relationships\n- Many features (>100)\n- Using tree-based or neural models\n- When accuracy critical (extra 1-3%)\n- Have time for 2-stage training\n\nWhen to use VAE:\n- Research and experimentation\n- Want uncertainty quantification\n- Generative modeling needs\n- Similar to autoencoder but probabilistic\n\nWhen to use LATENTS:\n- Using diffusion models\n- Want pre-trained representations\n- Maximum accuracy potential\n- Research-level work\n\n3) WHY TO USE IT:\n- Reduce dimensionality (100 features → 20 components)\n- Remove feature redundancy and multicollinearity\n- Speed up training (fewer dimensions = faster)\n- Improve generalization (noise filtering)\n- Enable complex models on limited data\n- Extract meaningful representations\n\n4) EFFECTS:\n\n4.1) NONE (No Encoding):\n\nBehavior:\n- Features used as-is, no transformation\n- Model trains on raw feature matrix\n- No preprocessing overhead\n\nAdvantages:\n- SIMPLEST approach (no complexity)\n- FASTEST setup (no encoding time)\n- Fully interpretable (see original features)\n- Good for linear models\n- Easy debugging (no hidden transformations)\n- No additional hyperparameters\n\nDisadvantages:\n- Multicollinearity issues (correlated features hurt performance)\n- Redundant features waste computation\n- May be too high-dimensional (slow training)\n- No noise filtering\n- Linear models limited by feature quality\n\nWhen to use:\n- DEFAULT for beginners\n- Quick experiments (<50 features)\n- Linear models (ridge, lasso)\n- Well-engineered features\n- When interpretability critical\n\nTypical results:\n- Training time: 1× baseline\n- Accuracy: baseline (54-58% for ridge)\n- Memory: 1× (all features stored)\n\n4.2) PCA (Principal Component Analysis):\n\nBehavior:\n- Linear transformation to orthogonal components\n- Keeps top N components (e.g., 20 from 100 features)\n- Components ordered by explained variance\n- Encoding time: 1-10 seconds\n\nAdvantages:\n- REMOVES MULTICOLLINEARITY (orthogonal components)\n- FASTER TRAINING (fewer dimensions)\n- Interpretable (variance explained per component)\n- Deterministic (reproducible)\n- Very fast encoding (<1 second)\n- Filters noise (low-variance components dropped)\n- Reduces memory usage\n\nDisadvantages:\n- LINEAR ONLY (can't capture non-linear relationships)\n- Loses interpretability (components = mixtures of features)\n- May lose information (if too few components)\n- Assumes Gaussian distributions\n- Requires feature scaling (sensitive to scale)\n\nWhen to use:\n- Many correlated features (>50)\n- Linear or tree-based models\n- Want faster training (50-100 features → 10-30 components)\n- Production systems (fast encoding)\n\nTypical results:\n- Encoding time: 1-10 seconds\n- Training time: 0.3-0.5× (30-50% faster)\n- Accuracy: +0-2% (slight improvement from noise filtering)\n- Memory: 0.2-0.5× (fewer dimensions)\n- Components: Keep 80-95% variance (20-40 components typical)\n\nExample:\n- 100 features → 20 PCA components\n- Training time: 60s → 20s (3× faster)\n- Accuracy: 56% → 57% (+1%)\n\n4.3) AUTOENCODER (Neural Network):\n\nBehavior:\n- Non-linear neural network encoder\n- Learns compressed representation (bottleneck layer)\n- 2-stage training: (1) train autoencoder, (2) train model on encoded features\n- Encoding time: 30 seconds to 5 minutes\n\nAdvantages:\n- NON-LINEAR (captures complex relationships PCA can't)\n- Better compression (20 latents vs 40 PCA components)\n- Learns task-relevant features\n- Usually +1-3% accuracy vs PCA\n- Flexible architecture (tune bottleneck size)\n\nDisadvantages:\n- SLOW ENCODING (30s-5min to train autoencoder)\n- 2-stage process (train encoder, then train model)\n- Non-deterministic (random initialization)\n- Harder to tune (learning rate, architecture)\n- Not interpretable (black box)\n- Overkill for simple models\n\nWhen to use:\n- Many features (>100)\n- Complex non-linear relationships\n- Using tree-based or neural models (not ridge/lasso)\n- When accuracy critical (worth 2-stage training)\n- Production systems (encode once, reuse)\n\nTypical results:\n- Encoding time: 1-5 minutes (train autoencoder)\n- Inference encoding: <10ms per sample\n- Training time: 0.2-0.4× (60-80% faster)\n- Accuracy: +1-3% vs PCA, +2-4% vs none\n- Memory: 0.1-0.3× (aggressive compression)\n\nExample:\n- 150 features → 15 autoencoder latents (10:1 compression)\n- Autoencoder training: 2 minutes\n- Model training: 90s → 25s (3.6× faster)\n- Accuracy: 56% → 59% (+3%)\n\n4.4) VAE (Variational Autoencoder):\n\nBehavior:\n- Probabilistic version of autoencoder\n- Learns distribution over latent space (not point estimate)\n- Encoding time: 1-10 minutes (slower than regular autoencoder)\n\nAdvantages:\n- PROBABILISTIC (uncertainty quantification)\n- Smoother latent space (better generalization)\n- Can generate synthetic samples\n- Regularization via KL divergence\n- Better for generative tasks\n\nDisadvantages:\n- SLOWER than regular autoencoder\n- More hyperparameters (KL weight, etc.)\n- Harder to tune (balance reconstruction + KL)\n- Marginal improvement over autoencoder for forecasting\n- Overkill for most use cases\n\nWhen to use:\n- Research projects\n- When you need uncertainty estimates\n- Generative modeling (synthetic data)\n- Similar use cases as autoencoder but want probabilistic\n\nTypical results:\n- Similar to autoencoder\n- Accuracy: +1-3% (comparable to autoencoder)\n- Training time: 1.5-2× autoencoder time\n\n4.5) LATENTS (Pre-trained Diffusion):\n\nBehavior:\n- Uses latent representation from pre-trained diffusion model\n- Leverages existing diffusion encoder\n- Encoding time: 10-60 minutes (depends on diffusion model)\n\nAdvantages:\n- MAXIMUM REPRESENTATION POWER (diffusion-learned features)\n- Transfer learning (leverage pre-trained knowledge)\n- Best accuracy potential (+2-5%)\n- Learns temporal dependencies well\n\nDisadvantages:\n- VERY SLOW (requires diffusion model first)\n- Only works with diffusion models\n- Experimental (not well-tested)\n- Overkill unless using diffusion anyway\n- Complex setup\n\nWhen to use:\n- Already using diffusion models\n- Research projects\n- Maximum accuracy at any cost\n- Have GPU and time\n\nTypical results:\n- Encoding time: 10-60 minutes\n- Accuracy: +2-5% (best possible)\n- Only practical if diffusion model already trained\n\n5) TYPICAL RANGE / DEFAULT VALUES:\n\n- BEGINNER DEFAULT: none (simplest)\n- INTERMEDIATE DEFAULT: pca (good balance)\n- ADVANCED DEFAULT: autoencoder (best accuracy/speed tradeoff)\n- RESEARCH DEFAULT: vae or latents\n\nUsage distribution:\n- 50% use none (beginners, simple models, <50 features)\n- 30% use pca (intermediate, 50-100 features)\n- 15% use autoencoder (advanced, >100 features)\n- 5% use vae/latents (researchers)\n\nRecommendation by feature count:\n- <30 features: none (no benefit from encoding)\n- 30-80 features: pca (fast compression)\n- 80-200 features: autoencoder (non-linear compression)\n- >200 features: autoencoder or pca (both work)\n\nRecommendation by model:\n- Ridge/Lasso/ElasticNet: none or pca (linear encoder matches linear model)\n- Random Forest: none or pca (RF handles high dimensions well)\n- LightGBM: pca or autoencoder (benefits from compressed features)\n- Diffusion: latents (leverages diffusion encoder)\n\n6) ADDITIONAL NOTES / BEST PRACTICES:\n\nChoosing number of components:\n- PCA: Keep 80-95% explained variance (typically 20-40 components)\n- Autoencoder: Compression ratio 5:1 to 10:1 (100 features → 10-20 latents)\n- Rule of thumb: Aim for 10-30 encoded dimensions\n\nEncoding workflow:\n1. Train encoder on training data only (avoid data leakage!)\n2. Transform training data\n3. Train model on transformed features\n4. Transform test data using same encoder\n5. Evaluate model on transformed test data\n\nData leakage warning:\n- NEVER fit encoder on full dataset (train+test)\n- Fit encoder on training set only\n- Apply same transformation to test set\n- ForexGPT handles this automatically\n\nWhen accuracy stuck:\n- If none → pca gives +0-2%: Features were redundant\n- If pca → autoencoder gives +2-4%: Non-linear relationships exist\n- If autoencoder gives +0%: Problem is feature quality, not encoding\n\nSpeed vs Accuracy:\n- none: 1× speed, baseline accuracy\n- pca: 2-3× faster training, +0-2% accuracy\n- autoencoder: 2-4× faster training (after encoding), +2-4% accuracy\n- Total time including encoding:\n  * pca: +10 seconds one-time\n  * autoencoder: +1-5 minutes one-time\n\nProduction deployment:\n- Save encoder with model (both needed for inference)\n- Encoding adds latency: pca <1ms, autoencoder ~10ms\n- For high-frequency: prefer none or pca (<1ms overhead)\n- For swing trading: autoencoder fine (~10ms acceptable)\n\nInteraction with other parameters:\n- Many features + no encoder + complex model = SLOW (hours)\n- Many features + pca + any model = FAST (minutes)\n- Few features (<30) + autoencoder = WASTE (no benefit)\n- Diffusion model + latents encoder = SYNERGY (best combo)\n\nHow ForexGPT uses encoders:\n1. Fits encoder on training features\n2. Transforms train and test sets\n3. Trains model on encoded features\n4. Saves both encoder and model to artifacts/\n5. At inference: encode(features) → model.predict(encoded)"
    },
    
    "optimization": {
      "label": "Optimization:",
      "tooltip": "Hyperparameter Optimization - Automatic Model Tuning\n\n1) WHAT IT IS:\nAutomatic search for best model hyperparameters.\nInstead of using default values, systematically tests combinations to find optimal settings.\n\nAvailable methods:\n- none: Use default hyperparameters (no optimization)\n- genetic-basic: Genetic algorithm (simple, single-objective)\n- nsga2: NSGA-II (multi-objective: accuracy + speed + simplicity)\n\n2) HOW AND WHEN TO USE:\n\nWhen to use NONE (no optimization):\n- Beginner-friendly (fast, simple)\n- Quick experiments and iteration\n- Linear models (ridge, lasso) - few hyperparameters\n- When training time critical (<5 minutes)\n- Prototyping phase\n- Defaults are usually 80% optimal anyway\n\nWhen to use GENETIC-BASIC:\n- Tree-based models (rf, lightning) - many hyperparameters\n- Want to squeeze extra 1-3% accuracy\n- Have 30-120 minutes for optimization\n- Single goal: maximize accuracy\n- Production models (worth the time investment)\n\nWhen to use NSGA2:\n- Multiple objectives (accuracy AND speed AND simplicity)\n- Want Pareto frontier of solutions\n- Have 60-180 minutes for optimization\n- Research and advanced users\n- Need to balance trade-offs explicitly\n\n3) WHY TO USE IT:\n- Improve accuracy (typically +1-5%)\n- Find optimal hyperparameters automatically\n- Avoid manual trial-and-error\n- Production models benefit from tuning\n- Tree models have MANY hyperparameters (learning rate, depth, trees, etc.)\n- Different datasets need different settings\n\n4) EFFECTS:\n\n4.1) NONE (No Optimization):\n\nBehavior:\n- Uses default hyperparameters\n- Single training run\n- No search or optimization\n\nAdvantages:\n- FASTEST (single training, no overhead)\n- Simplest (no tuning complexity)\n- Deterministic (same result every time)\n- Good for rapid iteration\n- Defaults are decent (80% optimal)\n\nDisadvantages:\n- NOT OPTIMAL (typically 2-5% below best possible)\n- May be far from optimal on some datasets\n- Leaves performance on table\n- Not suitable for production without tuning\n\nWhen to use:\n- DEFAULT for beginners\n- Linear models (ridge, lasso, elasticnet) - few hyperparameters to tune\n- Quick experiments\n- When time matters (minutes not hours)\n- Prototyping phase\n\nTypical results:\n- Training time: 10-30 seconds (ridge), 5-15 minutes (lightning)\n- Accuracy: baseline (54-58% ridge, 58-62% lightning)\n- Total time: just training time\n\n4.2) GENETIC-BASIC:\n\nBehavior:\n- Single-objective genetic algorithm\n- Objective: Maximize out-of-sample accuracy\n- Population: 20-50 individuals (hyperparameter sets)\n- Generations: 20-50 iterations\n- Total evaluations: 400-2500 training runs\n\nAdvantages:\n- FINDS BETTER HYPERPARAMETERS (typically +1-3% vs defaults)\n- Systematic search (not random)\n- Handles complex search spaces (10+ hyperparameters)\n- Converges to near-optimal solution\n- Evolutionary approach (explores + exploits)\n\nDisadvantages:\n- VERY SLOW (30-120 minutes typical)\n- Many training runs (population × generations)\n- Single objective only (just accuracy)\n- Non-deterministic (different runs give slightly different results)\n- Overkill for linear models\n\nWhen to use:\n- Tree-based models (rf, lightning) - benefit most from tuning\n- Production models (worth time investment)\n- When you have 30-120 minutes\n- Want to squeeze maximum accuracy\n- Final model after feature engineering done\n\nTypical results:\n- Optimization time: 30-120 minutes (depending on pop × gen)\n- Training runs: 400-2500 (e.g., 40 pop × 25 gen = 1000)\n- Accuracy improvement: +1-3% vs defaults\n- Example: Lightning 58% → 61% (+3%)\n- Total time: 40× to 100× slower than no optimization\n\nExample:\n- Population: 40, Generations: 25\n- Total evaluations: 1000 (40 × 25)\n- Per evaluation: 3 minutes (lightning training)\n- Total time: 3000 minutes = 50 hours... NO!\n- With parallelization (4 cores): 12.5 hours\n- More realistic: pop 20, gen 10 = 200 evals = 10 hours on 4 cores\n\n4.3) NSGA2 (Multi-Objective):\n\nBehavior:\n- Multi-objective genetic algorithm (Non-dominated Sorting GA)\n- Objectives: (1) Maximize accuracy, (2) Minimize training time, (3) Minimize model complexity\n- Returns Pareto frontier (multiple optimal solutions)\n- Same population/generation settings as genetic-basic\n\nAdvantages:\n- MULTIPLE OBJECTIVES (accuracy + speed + simplicity)\n- Pareto frontier (trade-off curve)\n- Choose solution based on preference\n- Example: 58% fast model vs 60% slow model vs 59% balanced\n- More sophisticated than single-objective\n- Real-world relevance (always trade-offs)\n\nDisadvantages:\n- EVEN SLOWER (evaluates 3 objectives)\n- More complex to interpret (Pareto frontier)\n- Needs more evaluations to converge\n- Overkill for most users\n- Requires understanding of multi-objective optimization\n\nWhen to use:\n- Research and advanced users\n- When you care about speed AND accuracy\n- Production deployment with latency constraints\n- Want to understand trade-offs explicitly\n- Have 60-180 minutes for optimization\n\nTypical results:\n- Optimization time: 60-180 minutes\n- Returns: 5-20 Pareto-optimal solutions\n- Example solutions:\n  * Solution 1: 61% accuracy, 10 min training, 1000 trees (slow, accurate)\n  * Solution 2: 59% accuracy, 3 min training, 300 trees (balanced)\n  * Solution 3: 57% accuracy, 1 min training, 100 trees (fast, simpler)\n- You choose based on constraints\n\n5) TYPICAL RANGE / DEFAULT VALUES:\n\n- BEGINNER DEFAULT: none (fastest)\n- INTERMEDIATE DEFAULT: genetic-basic (if time permits)\n- ADVANCED DEFAULT: genetic-basic or nsga2\n- PRODUCTION DEFAULT: genetic-basic (worth the investment)\n\nUsage distribution:\n- 70% use none (beginners, linear models, fast iteration)\n- 25% use genetic-basic (production models, tree-based)\n- 5% use nsga2 (research, advanced users)\n\nWhen to optimize:\n- Linear models (ridge, lasso): Usually skip (few hyperparameters)\n- Random Forest: Worthwhile (+2-4%)\n- LightGBM: HIGHLY RECOMMENDED (+2-5%)\n- Diffusion: Complex (many hyperparameters) but very slow optimization\n\nPopulation and generation settings:\n- Quick search: pop 20, gen 10 = 200 evals (~5-10 hours)\n- Balanced: pop 30, gen 20 = 600 evals (~15-30 hours)\n- Thorough: pop 50, gen 30 = 1500 evals (~40-80 hours)\n\nDefault settings in ForexGPT:\n- Population: 40 (configurable)\n- Generations: 25 (configurable)\n- Total: 1000 evaluations\n\n6) ADDITIONAL NOTES / BEST PRACTICES:\n\nHyperparameters optimized (LightGBM example):\n- learning_rate: 0.001 to 0.3 (log scale)\n- max_depth: 3 to 15\n- num_leaves: 10 to 300\n- min_data_in_leaf: 5 to 100\n- feature_fraction: 0.5 to 1.0\n- bagging_fraction: 0.5 to 1.0\n- lambda_l1: 0 to 10\n- lambda_l2: 0 to 10\n- num_iterations: 50 to 1000\n\nOptimization workflow:\n1. Train baseline model with defaults (5 min)\n2. Run optimization (30-120 min)\n3. Train final model with best hyperparameters (5 min)\n4. Compare: optimized vs baseline\n5. If improvement <1%: Not worth it, use defaults\n6. If improvement >2%: Definitely worth it\n\nWhen NOT to optimize:\n- Feature engineering phase (still iterating features)\n- Linear models (few hyperparameters, defaults good)\n- Small datasets (<1K samples, high variance in optimization)\n- Time-constrained prototyping\n\nWhen TO optimize:\n- Final production model\n- Tree-based models (many hyperparameters)\n- Large datasets (>10K samples, optimization more stable)\n- After feature engineering complete\n- When accuracy critical\n\nSpeed up optimization:\n- Reduce population (40 → 20)\n- Reduce generations (25 → 10)\n- Use fewer days (90 → 30, faster per evaluation)\n- Parallelize (use all CPU cores)\n- Use smaller validation set\n\nDiminishing returns:\n- First 100 evaluations: +2-3%\n- Next 400 evaluations: +0.5-1%\n- Next 500 evaluations: +0.1-0.3%\n- Beyond 1000: Minimal gains\n\nRecommendation:\n- For most: pop 20, gen 10 = 200 evals is sufficient (80% of gains)\n- Only go higher if accuracy CRITICAL\n\nInteraction with other parameters:\n- Optimization + many features = EXTREMELY SLOW (hours to days)\n- Optimization + PCA encoder = FASTER (fewer dimensions)\n- Optimization + few days (7-30) = FASTER (each eval quicker)\n- Optimization + linear models = WASTE (few hyperparameters to tune)\n\nHow ForexGPT runs optimization:\n1. Defines search space (hyperparameter ranges)\n2. Initializes population (random hyperparameter sets)\n3. For each generation:\n   a. Evaluate each individual (train model, measure accuracy)\n   b. Select best individuals (tournament selection)\n   c. Create offspring (crossover + mutation)\n   d. Replace population\n4. Returns best individual (hyperparameters)\n5. Trains final model with best hyperparameters\n6. Saves to artifacts/ with metadata"
    },
    
    "use_gpu_training": {
      "label": "Use GPU Training:",
      "tooltip": "GPU Acceleration - Train Models on Graphics Card\n\n1) WHAT IT IS:\nCheckbox to enable GPU (Graphics Processing Unit) for model training.\nWhen enabled, uses NVIDIA GPU instead of CPU for compatible operations.\nRequires CUDA-capable NVIDIA GPU and proper drivers installed.\n\n2) HOW AND WHEN TO USE:\n\nWhen to ENABLE (checked):\n- You have NVIDIA GPU (GTX 1060+ or RTX series)\n- Using neural network models (autoencoder, VAE, diffusion)\n- Large datasets (>50K samples)\n- Training takes >5 minutes on CPU\n- Using LightGBM with GPU support\n\nWhen to DISABLE (unchecked):\n- No NVIDIA GPU available\n- Using linear models (ridge, lasso, elasticnet) - no GPU benefit\n- Small datasets (<10K samples) - GPU overhead not worth it\n- Using Random Forest - minimal GPU benefit\n- Quick experiments on CPU are sufficient\n\n3) WHY TO USE IT:\n- Dramatically faster training (5-50× speedup for neural networks)\n- Enable larger models (more capacity on GPU memory)\n- Faster experimentation (more iterations per hour)\n- Essential for diffusion models (otherwise impractical)\n- Some models REQUIRE GPU for reasonable speed\n\n4) EFFECTS:\n\n4.1) DISABLED (CPU Training):\n\nBehavior:\n- All training on CPU (Central Processing Unit)\n- Uses RAM for model and data\n- Single or multi-core parallelization\n\nAdvantages:\n- ALWAYS WORKS (no special hardware needed)\n- No driver/CUDA setup required\n- Sufficient for linear models\n- Sufficient for small datasets\n- Lower power consumption\n- More RAM available (vs limited GPU memory)\n\nDisadvantages:\n- SLOW for neural networks (10-100× slower)\n- Diffusion models impractical (hours → days)\n- Limited parallelization for neural ops\n- Wastes GPU if available\n\nWhen to use:\n- No GPU available\n- Linear models (ridge, lasso, elasticnet)\n- Random Forest (minimal GPU benefit)\n- Small datasets (<10K samples)\n\nTypical training times (CPU):\n- Ridge: 10-30 seconds\n- LightGBM: 5-15 minutes\n- Autoencoder: 2-10 minutes\n- Diffusion: 10-50 hours (!)\n\n4.2) ENABLED (GPU Training):\n\nBehavior:\n- Neural network operations on GPU\n- Matrix operations parallelized (thousands of cores)\n- Data transferred CPU → GPU → CPU\n- Requires CUDA, cuDNN, and drivers\n\nAdvantages:\n- MASSIVE SPEEDUP for neural networks (5-50×)\n- Diffusion models practical (10h → 30min)\n- Autoencoder training fast (5min → 30sec)\n- LightGBM faster (2-3× speedup)\n- More experiments per day\n- Essential for serious deep learning\n\nDisadvantages:\n- Requires NVIDIA GPU (AMD/Intel not supported)\n- Requires CUDA setup (drivers, toolkit, cuDNN)\n- Limited GPU memory (8-24GB typical vs 32-128GB RAM)\n- Transfer overhead CPU↔GPU (small datasets may be slower)\n- Higher power consumption\n- Compatibility issues (CUDA versions, drivers)\n\nWhen to use:\n- Neural network models (autoencoder, VAE, diffusion)\n- LightGBM with large datasets\n- Large datasets (>50K samples)\n- When training takes >5 minutes on CPU\n\nTypical training times (GPU):\n- Ridge: 10-30 seconds (NO CHANGE - CPU-bound)\n- LightGBM: 2-5 minutes (2-3× faster)\n- Autoencoder: 30-120 seconds (5-10× faster)\n- Diffusion: 30 minutes to 2 hours (10-50× faster)\n\nSpeedup by model:\n- Ridge/Lasso/ElasticNet: 1× (no benefit)\n- Random Forest: 1-1.5× (minimal benefit)\n- LightGBM: 2-3× (moderate benefit)\n- Autoencoder: 5-10× (large benefit)\n- Diffusion: 10-50× (MASSIVE benefit, essential)\n\n5) TYPICAL RANGE / DEFAULT VALUES:\n\n- DEFAULT: Disabled (works everywhere)\n- If GPU available: Enable for neural networks\n- If NO GPU: Always disabled\n\nUsage distribution:\n- 60% train on CPU (no GPU, or linear models)\n- 40% train on GPU (have GPU, use neural models)\n\nRecommendation:\n- Linear models: Leave DISABLED\n- Tree models (RF, LightGBM): Enable if GPU available (moderate speedup)\n- Neural models (autoencoder, diffusion): ALWAYS ENABLE if GPU available (essential)\n\n6) ADDITIONAL NOTES / BEST PRACTICES:\n\nGPU requirements:\n- NVIDIA GPU (GTX 1060 or better, RTX series recommended)\n- 6GB+ VRAM minimum (8-16GB recommended)\n- CUDA 11.x or 12.x installed\n- cuDNN library installed\n- Compatible PyTorch/TensorFlow version\n\nSetup check:\n- Windows: Run nvidia-smi in command prompt\n- Should show GPU name and memory\n- If error: Drivers not installed or GPU not NVIDIA\n\nMemory considerations:\n- GPU memory (VRAM) limited (8-24GB typical)\n- Large datasets may not fit (use smaller batches)\n- If out-of-memory error: Reduce batch size or use CPU\n- RAM usage still applies (data loading)\n\nWhen GPU slower than CPU:\n- Very small datasets (<1K samples) - transfer overhead\n- Linear models - not GPU-optimized\n- Old GPU (GTX 900 series or older) - may be slower than modern CPU\n\nTroubleshooting:\n- \"CUDA not available\": Install CUDA toolkit and drivers\n- \"Out of memory\": Reduce batch size or days parameter\n- \"cudnn error\": Install compatible cuDNN version\n- Slow performance: Check GPU usage (nvidia-smi), may be using CPU\n\nPower consumption:\n- CPU training: 50-150W\n- GPU training: 200-400W (GPU + CPU)\n- Cost: ~$0.05-0.15 per hour (electricity)\n- Worth it for time savings\n\nMultiple GPUs:\n- ForexGPT uses single GPU by default\n- Multi-GPU support for diffusion models (advanced)\n- Specify GPU: CUDA_VISIBLE_DEVICES=0 (first GPU)\n\nCloud GPU options:\n- Google Colab: Free T4 GPU (limited hours)\n- AWS EC2 p3.2xlarge: V100 GPU ($3/hour)\n- Lambda Labs: RTX 3090 ($0.50/hour)\n- Vast.ai: Cheapest spot instances ($0.20-0.60/hour)\n\nInteraction with other parameters:\n- GPU + diffusion models = ESSENTIAL (otherwise impractical)\n- GPU + autoencoder = RECOMMENDED (5-10× faster)\n- GPU + LightGBM = NICE TO HAVE (2-3× faster)\n- GPU + ridge/lasso = NO BENEFIT (skip)\n- GPU + small dataset (<10K) = MAY BE SLOWER (overhead)\n- GPU + hyperparameter optimization = CRITICAL (100-1000 training runs)\n\nHow ForexGPT uses GPU:\n1. Checks if GPU available (torch.cuda.is_available())\n2. Moves model to GPU (model.to('cuda'))\n3. Moves data batches to GPU during training\n4. Performs forward/backward pass on GPU\n5. Moves results back to CPU for logging\n6. Saves model (CPU version) to artifacts/"
    },
    
    "indicators": {
      "master_toggle": {
        "label": "Use Indicators:",
        "tooltip": "Master Indicator Toggle\n\n1) WHAT IT IS:\nMaster checkbox to enable/disable ALL technical indicators.\nWhen checked, selected indicators calculated and included as features.\nWhen unchecked, NO indicators used (only raw OHLCV data).\n\n2) HOW AND WHEN TO USE:\n\nWhen to ENABLE:\n- Want technical analysis features\n- Most trading strategies (indicators proven useful)\n- Tree-based models (benefit from indicator features)\n- DEFAULT recommendation (indicators improve accuracy 2-5%)\n\nWhen to DISABLE:\n- Pure price action models (OHLCV only)\n- Testing if indicators actually help\n- Very fast training (fewer features = faster)\n- Research: compare with vs without indicators\n\n3) WHY TO USE IT:\n- Indicators capture patterns humans recognize (RSI overbought, MACD crossover, etc.)\n- Proven to improve model accuracy (typically +2-5%)\n- Multi-timeframe indicators provide context\n- Standard in quantitative trading\n\n4) EFFECTS:\n\n4.1) DISABLED:\n- Only raw OHLCV features (open, high, low, close, volume)\n- ~10-20 base features total\n- Faster training (fewer features)\n- Model learns from price patterns only\n\n4.2) ENABLED:\n- 100-300+ indicator features (depending on selections)\n- Slower training (more features to process)\n- Better accuracy (typically +2-5%)\n- Model learns from technical patterns\n\n5) TYPICAL RANGE:\n- DEFAULT: ENABLED (recommended)\n- 90% of users enable indicators\n- Only disable for specific research questions\n\n6) ADDITIONAL NOTES:\n- Select individual indicators below (ATR, RSI, Bollinger, etc.)\n- Each indicator calculated on multiple timeframes\n- More indicators ≠ always better (diminishing returns after ~10 indicators)\n- Start with defaults: ATR, RSI, Bollinger, MACD, Stochastic"
      },
      
      "atr": {
        "label": "ATR (Average True Range):",
        "tooltip": "ATR - Average True Range (Volatility Indicator)\n\n1) WHAT IT IS:\nMeasures market volatility by calculating average of true ranges over N periods.\nTrue Range = max(high-low, |high-close_prev|, |low-close_prev|)\nATR = moving average of True Range (typically 14 periods)\n\nHigher ATR = higher volatility (larger price swings)\nLower ATR = lower volatility (smaller price swings)\n\n2) HOW AND WHEN TO USE:\n\nWhen to INCLUDE ATR:\n- HIGHLY RECOMMENDED (one of top 3 most useful indicators)\n- Volatility-based strategies (breakout, range, adaptive stops)\n- Position sizing (scale size by volatility)\n- Risk management (wider stops in high ATR)\n- Works on ALL timeframes\n\nTimeframes to select:\n- Scalping (1m): Select 1m, 5m (immediate + context)\n- Day trading (15m-1h): Select 5m, 15m, 30m, 1h (multi-timeframe view)\n- Swing trading (4h-1d): Select 1h, 4h, 1d (longer context)\n- DEFAULT: Select ALL available timeframes (ATR always useful)\n\n3) WHY TO USE IT:\n- Volatility is CRITICAL for trading decisions\n- High ATR = big moves expected (good for breakouts, bad for range)\n- Low ATR = consolidation (good for range, wait for breakout)\n- Position sizing: Risk 1% of account / ATR = shares\n- Stop loss: 2-3× ATR from entry (volatility-adjusted)\n- Model learns: 'Don't trade breakout when ATR low' (low volatility = false breakout)\n\n4) EFFECTS:\n\n4.1) NOT SELECTED (No ATR features):\n- Model blind to volatility changes\n- Can't adapt to market regimes\n- Position sizing not volatility-aware\n- Accuracy loss ~1-2%\n\n4.2) SELECTED (ATR features included):\n- Model learns volatility patterns\n- Distinguishes high-volatility breakouts from low-volatility noise\n- Typical improvement: +1-3% accuracy\n- Especially useful for tree-based models (captures regime changes)\n\nFeatures generated (if all timeframes selected):\n- atr_1m, atr_5m, atr_15m, atr_30m, atr_1h, atr_4h, atr_1d\n- atr_ratio (current ATR / 20-period ATR avg) - volatility expansion/contraction\n- ~5-10 features per timeframe × timeframes selected\n\n5) TYPICAL RANGE / DEFAULT VALUES:\n\n- DEFAULT: SELECT ALL TIMEFRAMES (ATR universally useful)\n- Period: 14 (standard, configurable in advanced settings)\n- Most important indicator (top 3 with RSI and Bollinger)\n\nUsage:\n- 95% of users include ATR\n- Top 3 most predictive indicator\n\nRecommendation:\n- ALWAYS INCLUDE ATR (unless pure price action study)\n- Select at least 3 timeframes (current + 1 higher + 1 lower)\n\n6) ADDITIONAL NOTES / BEST PRACTICES:\n\nInterpretation:\n- ATR increases during breakouts, news, volatility spikes\n- ATR decreases during consolidation, quiet periods\n- Expanding ATR: Momentum building, potential breakout\n- Contracting ATR: Consolidation, potential breakout soon (compression)\n\nTrading applications:\n- Stop loss: entry ± 2×ATR (adjust to volatility)\n- Position sizing: risk $ / (ATR × point_value) = shares\n- Breakout confirmation: Valid if ATR expanding\n- Range detection: Low ATR = range-bound market\n\nMulti-timeframe ATR:\n- 1m ATR: Immediate micro-volatility\n- 1h ATR: Intraday volatility regime\n- 1d ATR: Overall market volatility\n- Model learns: 'High 1d ATR + breakout on 15m = valid signal'\n\nInteraction with model:\n- Tree models: Splits on ATR thresholds (if atr_1h > 0.003 then bullish)\n- Linear models: ATR coefficient (positive = volatility helps)\n- Neural models: Learns complex ATR patterns across timeframes\n\nATR vs other volatility indicators:\n- ATR: Absolute volatility (pips/points)\n- Bollinger Bands: Relative volatility (stddev from MA)\n- Use both: ATR for absolute, Bollinger for relative"
      },
      
      "rsi": {
        "label": "RSI (Relative Strength Index):",
        "tooltip": "RSI - Relative Strength Index (Momentum Oscillator)\n\n1) WHAT IT IS:\nMomentum oscillator measuring speed and magnitude of price changes.\nRSI = 100 - (100 / (1 + RS))\nRS = Average Gain / Average Loss over N periods (typically 14)\n\nRange: 0-100\n- RSI > 70: Overbought (price may fall)\n- RSI < 30: Oversold (price may rise)\n- RSI 50: Neutral (balanced momentum)\n\n2) HOW AND WHEN TO USE:\n\nWhen to INCLUDE RSI:\n- HIGHLY RECOMMENDED (one of top 3 most useful indicators)\n- Mean reversion strategies (buy oversold, sell overbought)\n- Momentum strategies (trending with RSI > 50)\n- Divergence detection (price vs RSI direction)\n- Works on ALL timeframes\n\nTimeframes to select:\n- Scalping (1m): Select 1m, 5m, 15m (quick momentum shifts)\n- Day trading (15m-1h): Select 15m, 30m, 1h (intraday momentum)\n- Swing trading (4h-1d): Select 1h, 4h, 1d (swing momentum)\n- DEFAULT: Select 5+ timeframes (multi-timeframe RSI very predictive)\n\n3) WHY TO USE IT:\n- Identifies overbought/oversold conditions (reversal opportunities)\n- Momentum confirmation (RSI > 50 = bullish, < 50 = bearish)\n- Divergence signals (price makes new high, RSI doesn't = weakness)\n- Works in trends AND ranges\n- Model learns: 'RSI < 30 on 1h + RSI rising on 15m = buy signal'\n\n4) EFFECTS:\n\n4.1) NOT SELECTED (No RSI features):\n- Model blind to momentum extremes\n- Can't identify overbought/oversold\n- Misses mean reversion opportunities\n- Accuracy loss ~1-3%\n\n4.2) SELECTED (RSI features included):\n- Model learns momentum patterns\n- Distinguishes strong trends (RSI > 70 sustained) from reversals\n- Typical improvement: +2-4% accuracy\n- Top 3 most predictive indicator\n\nFeatures generated:\n- rsi_1m, rsi_5m, rsi_15m, rsi_30m, rsi_1h, rsi_4h, rsi_1d\n- rsi_change (RSI delta over 1 period) - momentum acceleration\n- rsi_cross_50 (recent cross above/below 50) - trend changes\n- rsi_divergence (price vs RSI direction mismatch)\n- ~8-12 features per timeframe × timeframes selected\n\n5) TYPICAL RANGE / DEFAULT VALUES:\n\n- DEFAULT: SELECT MULTIPLE TIMEFRAMES (RSI on 3+ timeframes recommended)\n- Period: 14 (standard, configurable)\n- Overbought: 70 (traditional)\n- Oversold: 30 (traditional)\n- Most important indicator (top 3 with ATR and Bollinger)\n\nUsage:\n- 90% of users include RSI\n- #2 most predictive indicator (after ATR)\n\nRecommendation:\n- ALWAYS INCLUDE RSI (extremely predictive)\n- Select current timeframe + 2 higher + 2 lower (5 total minimum)\n\n6) ADDITIONAL NOTES / BEST PRACTICES:\n\nInterpretation:\n- RSI > 70: Overbought (caution on longs, consider shorts)\n- RSI < 30: Oversold (caution on shorts, consider longs)\n- RSI > 50: Bullish momentum (trend up)\n- RSI < 50: Bearish momentum (trend down)\n- RSI 40-60: Neutral/choppy\n\nTrading applications:\n- Mean reversion: Buy when RSI < 30, sell when > 70\n- Trend following: Only long when RSI > 50, only short when < 50\n- Divergence: Price new high but RSI lower = bearish divergence (reversal)\n- RSI trendlines: Draw trendlines on RSI itself\n\nMulti-timeframe RSI:\n- All timeframes RSI > 50: STRONG UPTREND (high confidence longs)\n- All timeframes RSI < 50: STRONG DOWNTREND (high confidence shorts)\n- Mixed RSI: Choppy, lower confidence\n- Example: 1d RSI > 70 + 1h RSI < 30 = pullback in uptrend (buy dip)\n\nRSI in different market conditions:\n- Strong trends: RSI can stay > 70 (overbought) for extended periods (don't fade)\n- Ranging markets: RSI 30/70 levels work well (mean reversion)\n- Trending markets: RSI 40/60 better (trend continuation)\n\nInteraction with model:\n- Tree models: Splits on RSI thresholds (if rsi_1h < 30 then buy)\n- Linear models: RSI coefficient shows mean reversion vs momentum\n- Neural models: Learns complex RSI patterns (divergences, multi-TF confluence)\n\nRSI vs other momentum indicators:\n- RSI: Oscillator (0-100), overbought/oversold levels\n- Stochastic: Similar but more volatile (faster signals)\n- MACD: Trend + momentum, no fixed range\n- Use all three: RSI for overbought/oversold, MACD for trend, Stochastic for precise timing"
      },
      
      "bollinger": {
        "label": "Bollinger Bands:",
        "tooltip": "Bollinger Bands - Volatility Bands (Price Envelopes)\n\n1) WHAT IT IS:\nVolatility bands around a moving average.\nMiddle Band = SMA(N) - typically 20-period\nUpper Band = SMA(N) + K×StdDev(N) - typically +2 stddev\nLower Band = SMA(N) - K×StdDev(N) - typically -2 stddev\n\nPrice within bands = normal volatility\nPrice touching/crossing bands = extreme move\n\n2) HOW AND WHEN TO USE:\n\nWhen to INCLUDE Bollinger Bands:\n- HIGHLY RECOMMENDED (one of top 3 most useful indicators)\n- Volatility-based strategies (breakout, mean reversion)\n- Identifying overbought/oversold relative to volatility\n- Detecting consolidation (bands squeeze) before breakouts\n- Works on ALL timeframes\n\nTimeframes to select:\n- Scalping (1m): Select 1m, 5m (immediate volatility context)\n- Day trading (15m-1h): Select 15m, 30m, 1h, 4h\n- Swing trading (4h-1d): Select 4h, 1d\n- DEFAULT: Select 4-5 timeframes (multi-timeframe bands context)\n\n3) WHY TO USE IT:\n- Shows RELATIVE volatility (price extreme relative to recent range)\n- Combines trend (middle band) + volatility (band width) + position (where price is)\n- Squeeze: Bands narrow = low volatility = breakout imminent\n- Expansion: Bands widen = high volatility = strong move happening\n- Model learns: 'Price at lower band + bands squeezing + volume spike = breakout up'\n\n4) EFFECTS:\n\n4.1) NOT SELECTED (No Bollinger features):\n- Model blind to relative volatility extremes\n- Can't identify squeezes (compression before breakout)\n- Misses mean reversion signals\n- Accuracy loss ~2-3%\n\n4.2) SELECTED (Bollinger features included):\n- Model learns volatility expansion/contraction cycles\n- Identifies squeezes (low volatility = high probability setup)\n- Mean reversion signals when price extreme\n- Typical improvement: +2-4% accuracy\n- Top 3 most predictive indicator\n\nFeatures generated:\n- bb_upper, bb_middle, bb_lower (band values)\n- bb_width (upper - lower, volatility measure)\n- bb_percent ((price - lower) / (upper - lower), position 0-100%)\n- bb_squeeze (width < 20-period avg width, compression)\n- bb_breakout (price crossed outside bands)\n- ~10-15 features per timeframe × timeframes selected\n\n5) TYPICAL RANGE / DEFAULT VALUES:\n\n- DEFAULT: SELECT 4-5 TIMEFRAMES (Bollinger very predictive)\n- Period: 20 (SMA length, standard)\n- StdDev: 2.0 (band width, standard)\n- Most important indicator (top 3 with ATR and RSI)\n\nUsage:\n- 85% of users include Bollinger Bands\n- #3 most predictive indicator\n\nRecommendation:\n- ALWAYS INCLUDE Bollinger Bands\n- Select current timeframe + 2 higher timeframes\n\n6) ADDITIONAL NOTES / BEST PRACTICES:\n\nInterpretation:\n- Price at upper band: Overbought/strong uptrend\n- Price at lower band: Oversold/strong downtrend\n- Price at middle band: Neutral/mean\n- Squeeze (narrow bands): Low volatility, breakout coming\n- Expansion (wide bands): High volatility, move in progress\n\nTrading applications:\n- Mean reversion: Buy at lower band, sell at upper band (ranging markets)\n- Breakout: Buy when price breaks above upper band with expansion (trending markets)\n- Squeeze play: Wait for bands to narrow, then trade the breakout direction\n- Walk the bands: In strong trends, price 'walks' along upper band (don't fade)\n\nMulti-timeframe Bollinger:\n- 1d bands: Overall volatility regime\n- 1h bands: Intraday extremes\n- 15m bands: Precise entry/exit\n- Example: 1d squeeze + 1h breakout above upper band = strong long signal\n\nBollinger Band patterns:\n- Squeeze: Width < 20-period avg, coiling for breakout\n- Walking the band: Strong trend, price stays near one band (don't fade)\n- Double bottom/top: Price hits lower band twice, reversal signal\n- M-top / W-bottom: Classic reversal patterns\n\nInteraction with model:\n- Tree models: Splits on bb_percent (if bb_percent < 20 then oversold)\n- Linear models: bb_width coefficient (volatility expansion predictive)\n- Neural models: Learns squeeze → expansion patterns across timeframes\n\nBollinger vs other volatility indicators:\n- Bollinger: Relative volatility (stddev from MA)\n- ATR: Absolute volatility (pips/points)\n- Keltner Channels: Similar but uses ATR instead of stddev\n- Use Bollinger + ATR: Bollinger for relative extremes, ATR for absolute sizing"
      },
      
      "macd": {
        "label": "MACD:",
        "tooltip": "MACD - Moving Average Convergence Divergence (Trend + Momentum)\n\n1) WHAT IT IS:\nTrend-following momentum indicator showing relationship between two moving averages.\nMACD Line = EMA(12) - EMA(26)\nSignal Line = EMA(9) of MACD Line\nHistogram = MACD Line - Signal Line\n\nMACD > 0: Bullish (short-term MA above long-term MA)\nMACD < 0: Bearish (short-term MA below long-term MA)\nHistogram > 0: MACD above signal (bullish momentum)\nHistogram < 0: MACD below signal (bearish momentum)\n\n2) HOW AND WHEN TO USE:\n\nWhen to INCLUDE MACD:\n- RECOMMENDED (top 5 most useful indicators)\n- Trend following strategies (ride trends with MACD)\n- Momentum strategies (MACD crossovers)\n- Divergence detection (price vs MACD direction)\n- Best on higher timeframes (15m+)\n\nTimeframes to select:\n- Scalping (1m): Skip (too noisy on 1m)\n- Day trading (15m-1h): Select 15m, 30m, 1h, 4h\n- Swing trading (4h-1d): Select 4h, 1d (MACD best on higher TF)\n- DEFAULT: Select 4h, 1d (MACD most reliable on daily)\n\n3) WHY TO USE IT:\n- Combines trend (MACD sign) + momentum (histogram) in one indicator\n- Crossovers signal trend changes (MACD crosses signal = trade signal)\n- Divergence: Price new high but MACD lower = bearish divergence (reversal)\n- Zero line: MACD crosses above 0 = bullish, below 0 = bearish\n- Model learns: 'MACD histogram expanding + price above EMA = strong trend'\n\n4) EFFECTS:\n\n4.1) NOT SELECTED (No MACD features):\n- Model blind to trend momentum\n- Misses trend change signals\n- Can't detect momentum divergences\n- Accuracy loss ~1-2%\n\n4.2) SELECTED (MACD features included):\n- Model learns trend + momentum patterns\n- Identifies trend changes (crossovers)\n- Detects momentum weakening (divergences)\n- Typical improvement: +1-3% accuracy\n- Top 5 most predictive indicator\n\nFeatures generated:\n- macd_line (MACD value)\n- macd_signal (signal line value)\n- macd_histogram (MACD - signal)\n- macd_cross_above (bullish crossover signal)\n- macd_cross_below (bearish crossover signal)\n- macd_divergence (price vs MACD direction mismatch)\n- ~10-12 features per timeframe × timeframes selected\n\n5) TYPICAL RANGE / DEFAULT VALUES:\n\n- DEFAULT: SELECT HIGHER TIMEFRAMES (4h, 1d recommended)\n- Fast EMA: 12 periods (standard)\n- Slow EMA: 26 periods (standard)\n- Signal EMA: 9 periods (standard)\n- Important indicator (top 5)\n\nUsage:\n- 70% of users include MACD\n- Top 5 most predictive indicator (especially on daily timeframe)\n\nRecommendation:\n- INCLUDE MACD on higher timeframes (4h, 1d)\n- Skip on very low timeframes (1m, 5m too noisy)\n\n6) ADDITIONAL NOTES / BEST PRACTICES:\n\nInterpretation:\n- MACD > 0: Short-term momentum stronger (bullish)\n- MACD < 0: Short-term momentum weaker (bearish)\n- Histogram growing: Momentum accelerating\n- Histogram shrinking: Momentum decelerating\n- Crossover: MACD crosses signal = trend change\n\nTrading applications:\n- Crossover: Buy when MACD crosses above signal, sell when crosses below\n- Zero line: Buy when MACD crosses above 0, sell when crosses below 0\n- Divergence: Price new high but MACD lower = bearish divergence (short)\n- Histogram: Expanding histogram = strong trend (hold), shrinking = weakening (exit)\n\nMulti-timeframe MACD:\n- 1d MACD: Overall trend direction\n- 4h MACD: Swing trend\n- 1h MACD: Intraday momentum\n- Example: 1d MACD > 0 + 1h MACD crosses above signal = strong long\n\nMACD patterns:\n- Bullish crossover: MACD crosses above signal (buy signal)\n- Bearish crossover: MACD crosses below signal (sell signal)\n- Zero line cross: MACD crosses 0 (stronger signal than crossover)\n- Divergence: Price makes new extreme but MACD doesn't (reversal warning)\n\nMACD in different markets:\n- Trending: MACD excellent (catches trends early)\n- Ranging: MACD poor (many false crossovers, whipsaws)\n- Solution: Combine with ADX (only trade MACD when ADX > 25)\n\nInteraction with model:\n- Tree models: Splits on MACD crossovers (if macd_cross_above then buy)\n- Linear models: MACD histogram coefficient (momentum predictive)\n- Neural models: Learns complex MACD patterns (divergences, multi-TF)\n\nMACD vs other trend/momentum indicators:\n- MACD: Trend + momentum, crossovers, divergences\n- RSI: Momentum oscillator, overbought/oversold\n- Stochastic: Momentum oscillator, faster than MACD\n- ADX: Trend strength (no direction), pairs well with MACD\n- Use MACD + RSI + ADX: MACD for trend, RSI for extremes, ADX for strength"
      },
      
      "stochastic": {
        "label": "Stochastic:",
        "tooltip": "Stochastic Oscillator - Fast Momentum Indicator\n\n1) WHAT IT IS:\nMomentum oscillator comparing closing price to price range over N periods.\n%K = 100 × (Close - Low_N) / (High_N - Low_N)\n%D = SMA(3) of %K (signal line)\n\nRange: 0-100\n- Stochastic > 80: Overbought (price near top of range)\n- Stochastic < 20: Oversold (price near bottom of range)\n- Similar to RSI but more volatile (faster signals)\n\n2) HOW AND WHEN TO USE:\n\nWhen to INCLUDE Stochastic:\n- Mean reversion strategies (buy oversold, sell overbought)\n- Precise entry timing (faster than RSI)\n- Range-bound markets (works well in consolidation)\n- Divergence detection\n- Recommended for short-term trading\n\nTimeframes to select:\n- Scalping (1m): Select 1m, 5m (very fast signals)\n- Day trading (15m-1h): Select 5m, 15m, 30m, 1h\n- Swing trading (4h-1d): Select 1h, 4h (slower timeframes better)\n- DEFAULT: Select 3-5 timeframes\n\n3) WHY TO USE IT:\n- FASTER than RSI (more responsive to price changes)\n- Excellent for timing entries in ranging markets\n- %K/%D crossovers give precise signals\n- Works well with Bollinger Bands for mean reversion\n- Model learns: 'Stoch < 20 + %K crosses above %D = buy signal'\n\n4) EFFECTS:\n\n4.1) NOT SELECTED:\n- Model misses fast momentum shifts\n- Can't time precise entries in ranges\n- Accuracy loss ~1-2%\n\n4.2) SELECTED:\n- Model learns fast mean reversion patterns\n- Precise timing for entries/exits\n- Typical improvement: +1-2% accuracy\n- Especially useful in ranging markets\n\nFeatures generated:\n- stoch_k (%K line)\n- stoch_d (%D signal line)\n- stoch_cross_above (%K crosses above %D, bullish)\n- stoch_cross_below (%K crosses below %D, bearish)\n- stoch_oversold (< 20)\n- stoch_overbought (> 80)\n- ~8-10 features per timeframe\n\n5) TYPICAL RANGE / DEFAULT VALUES:\n\n- DEFAULT: SELECT 3-5 TIMEFRAMES\n- %K Period: 14 (standard)\n- %D Period: 3 (SMA smoothing)\n- Overbought: 80\n- Oversold: 20\n\nUsage:\n- 60% of users include Stochastic\n- Good for range-bound markets\n- Pairs well with RSI and Bollinger\n\nRecommendation:\n- INCLUDE for day trading and scalping\n- Combine with RSI (Stoch for timing, RSI for context)\n\n6) ADDITIONAL NOTES / BEST PRACTICES:\n\nInterpretation:\n- Stoch > 80: Overbought, consider shorts\n- Stoch < 20: Oversold, consider longs\n- %K crosses above %D: Bullish crossover (buy)\n- %K crosses below %D: Bearish crossover (sell)\n\nTrading applications:\n- Buy: Stoch < 20 + %K crosses above %D\n- Sell: Stoch > 80 + %K crosses below %D\n- Divergence: Price new low but Stoch higher = bullish divergence\n\nStochastic vs RSI:\n- Stochastic: Faster, more signals, more noise\n- RSI: Slower, fewer signals, more reliable\n- Use both: RSI for trend, Stochastic for precise timing\n\nMulti-timeframe Stochastic:\n- 1h Stoch < 20: Oversold on hourly\n- 15m Stoch crosses up: Entry signal\n- Confluence = high probability trade\n\nInteraction with model:\n- Tree models: Splits on crossovers (if stoch_cross_above and stoch < 30 then buy)\n- Works best in ranging markets (low ADX)"
      },
      
      "cci": {
        "label": "CCI (Commodity Channel Index):",
        "tooltip": "CCI - Commodity Channel Index (Cyclical Momentum)\n\n1) WHAT IT IS:\nMomentum oscillator measuring deviation from average price.\nCCI = (Typical Price - SMA) / (0.015 × Mean Deviation)\nTypical Price = (High + Low + Close) / 3\n\nRange: Typically -200 to +200 (unbounded)\n- CCI > +100: Overbought\n- CCI < -100: Oversold\n- CCI oscillates around 0\n\n2) HOW AND WHEN TO USE:\n\nWhen to INCLUDE CCI:\n- Cyclical markets (identifies cycles)\n- Overbought/oversold extremes (similar to RSI)\n- Trend strength measurement\n- Divergence detection\n- Good for commodities and forex\n\nTimeframes to select:\n- Day trading: Select 15m, 30m, 1h\n- Swing trading: Select 4h, 1d\n- DEFAULT: Select 2-3 higher timeframes\n\n3) WHY TO USE IT:\n- Identifies price extremes (±100 levels)\n- Unbounded range captures extreme moves\n- Good for detecting trend strength\n- Originally designed for commodities (works well on forex)\n\n4) EFFECTS:\n\n4.1) NOT SELECTED:\n- Model misses cyclical patterns\n- Can't detect extreme deviations\n- Accuracy loss ~0.5-1%\n\n4.2) SELECTED:\n- Model learns cycle patterns\n- Identifies extreme price movements\n- Typical improvement: +0.5-1.5% accuracy\n\nFeatures generated:\n- cci_value\n- cci_overbought (> 100)\n- cci_oversold (< -100)\n- cci_cross_zero\n- ~5-8 features per timeframe\n\n5) TYPICAL RANGE / DEFAULT VALUES:\n\n- DEFAULT: OPTIONAL (not as critical as ATR/RSI/Bollinger)\n- Period: 20 (standard)\n- Overbought: +100\n- Oversold: -100\n\nUsage:\n- 40% of users include CCI\n- Secondary indicator (after ATR/RSI/Bollinger)\n\nRecommendation:\n- OPTIONAL - include if trading commodities/forex cycles\n- Skip if already using RSI (similar functionality)\n\n6) ADDITIONAL NOTES / BEST PRACTICES:\n\nInterpretation:\n- CCI > +100: Strong uptrend, overbought\n- CCI < -100: Strong downtrend, oversold\n- CCI crossing 0: Trend change signal\n\nTrading applications:\n- Buy: CCI crosses above -100 (exit oversold)\n- Sell: CCI crosses below +100 (exit overbought)\n- Trend: CCI > 100 = strong uptrend (hold longs)\n\nCCI vs RSI:\n- CCI: Unbounded, more extreme readings\n- RSI: Bounded 0-100, normalized\n- Use CCI for volatile markets, RSI for normal conditions"
      },
      
      "williams_r": {
        "label": "Williams %R:",
        "tooltip": "Williams %R - Fast Overbought/Oversold Oscillator\n\n1) WHAT IT IS:\nMomentum oscillator showing where closing price is relative to high-low range.\n%R = -100 × (High_N - Close) / (High_N - Low_N)\n\nRange: -100 to 0\n- %R > -20: Overbought (price near top)\n- %R < -80: Oversold (price near bottom)\n- Inverted scale (0 at top, -100 at bottom)\n\n2) HOW AND WHEN TO USE:\n\nWhen to INCLUDE Williams %R:\n- Fast momentum signals (similar to Stochastic but faster)\n- Overbought/oversold detection\n- Short-term trading (scalping, day trading)\n- Timing exits (not entries)\n\nTimeframes to select:\n- Scalping: Select 1m, 5m, 15m\n- Day trading: Select 5m, 15m, 30m\n- DEFAULT: OPTIONAL (similar to Stochastic)\n\n3) WHY TO USE IT:\n- VERY FAST signals (14-period lookback)\n- Good for exit timing\n- Identifies short-term extremes\n\n4) EFFECTS:\n\n4.1) NOT SELECTED:\n- Model misses very short-term momentum\n- Similar to Stochastic (not critical if Stoch included)\n- Accuracy loss ~0.5-1%\n\n4.2) SELECTED:\n- Model learns fast momentum shifts\n- Precise exit timing\n- Typical improvement: +0.5-1% accuracy\n\nFeatures generated:\n- williams_r_value\n- williams_r_overbought (> -20)\n- williams_r_oversold (< -80)\n- ~4-6 features per timeframe\n\n5) TYPICAL RANGE / DEFAULT VALUES:\n\n- DEFAULT: SKIP (redundant with Stochastic)\n- Period: 14\n- Overbought: -20\n- Oversold: -80\n\nUsage:\n- 30% of users include Williams %R\n- Often skipped (Stochastic preferred)\n\nRecommendation:\n- SKIP if using Stochastic (very similar)\n- Include ONLY if you want very fast signals\n\n6) ADDITIONAL NOTES / BEST PRACTICES:\n\nInterpretation:\n- %R > -20: Overbought (consider exits on longs)\n- %R < -80: Oversold (consider exits on shorts)\n\nWilliams %R vs Stochastic:\n- Very similar indicators\n- Williams %R slightly faster\n- Stochastic more popular\n- Choose one, not both (redundant)"
      },
      
      "adx": {
        "label": "ADX (Average Directional Index):",
        "tooltip": "ADX - Average Directional Index (Trend Strength)\n\n1) WHAT IT IS:\nTrend strength indicator (NOT direction).\nADX = MA of DX (Directional Movement Index)\nDX = 100 × |+DI - -DI| / (+DI + -DI)\n\nRange: 0-100\n- ADX < 20: Weak trend, ranging market\n- ADX 20-40: Moderate trend\n- ADX > 40: Strong trend\n- ADX does NOT show direction (only strength)\n\n2) HOW AND WHEN TO USE:\n\nWhen to INCLUDE ADX:\n- HIGHLY RECOMMENDED (top 5 indicators)\n- Trend vs range detection (critical for strategy selection)\n- Filter for other indicators (only trade MACD when ADX > 25)\n- Avoid false signals in choppy markets\n- Works on ALL timeframes\n\nTimeframes to select:\n- Day trading: Select 15m, 30m, 1h, 4h\n- Swing trading: Select 4h, 1d\n- DEFAULT: Select 3-4 timeframes\n\n3) WHY TO USE IT:\n- Identifies trending vs ranging markets\n- Avoid trading in choppy conditions (ADX < 20)\n- Confirm trend strength before entering\n- Pair with directional indicators (MACD, RSI)\n- Model learns: 'Only trade breakout when ADX > 25' (strong trend)\n\n4) EFFECTS:\n\n4.1) NOT SELECTED:\n- Model can't distinguish trending vs ranging\n- Trades in choppy markets (many false signals)\n- Accuracy loss ~1-2%\n\n4.2) SELECTED:\n- Model learns to avoid choppy markets\n- Identifies strong trends (high probability)\n- Typical improvement: +1-3% accuracy\n- Top 5 most useful indicator\n\nFeatures generated:\n- adx_value (trend strength)\n- plus_di (+DI, bullish direction)\n- minus_di (-DI, bearish direction)\n- adx_trending (ADX > 25)\n- adx_strong_trend (ADX > 40)\n- ~6-8 features per timeframe\n\n5) TYPICAL RANGE / DEFAULT VALUES:\n\n- DEFAULT: HIGHLY RECOMMENDED\n- Period: 14 (standard)\n- Trending threshold: 25\n- Strong trend: 40\n\nUsage:\n- 75% of users include ADX\n- Top 5 most important indicator\n\nRecommendation:\n- ALWAYS INCLUDE ADX (critical for trend detection)\n- Select multiple timeframes (trend context)\n\n6) ADDITIONAL NOTES / BEST PRACTICES:\n\nInterpretation:\n- ADX < 20: Ranging, avoid trend strategies\n- ADX 20-40: Moderate trend, standard trading\n- ADX > 40: Strong trend, trend-following optimal\n- ADX rising: Trend strengthening\n- ADX falling: Trend weakening\n\nTrading applications:\n- Trend filter: Only trade MACD/MA crossovers when ADX > 25\n- Range detection: Trade mean reversion when ADX < 20\n- Breakout confirmation: Valid breakout if ADX rising above 25\n\nDirection from +DI/-DI:\n- +DI > -DI: Bullish direction\n- -DI > +DI: Bearish direction\n- ADX shows strength, DI shows direction\n\nMulti-timeframe ADX:\n- 1d ADX > 40: Strong overall trend\n- 1h ADX < 20: Intraday ranging\n- Trade intraday mean reversion within daily trend\n\nADX + MACD combination:\n- ADX > 25 + MACD crossover = HIGH PROBABILITY\n- ADX < 20 + MACD crossover = LOW PROBABILITY (skip)\n\nInteraction with model:\n- Tree models: Splits on ADX (if adx_1h > 25 then trade trend)\n- Critical for regime detection (trend vs range)\n\nADX vs other trend indicators:\n- ADX: Strength only (no direction)\n- MACD: Direction and momentum\n- MA: Direction only\n- Use ADX + MACD: ADX confirms trend, MACD gives direction"
      },
      
      "mfi": {
        "label": "MFI (Money Flow Index):",
        "tooltip": "MFI - Money Flow Index (Volume-Weighted RSI)\n\n1) WHAT IT IS:\nMomentum oscillator incorporating volume (RSI with volume).\nTypical Price = (High + Low + Close) / 3\nMoney Flow = Typical Price × Volume\nMFI = 100 - (100 / (1 + Money Flow Ratio))\n\nRange: 0-100\n- MFI > 80: Overbought (high volume buying)\n- MFI < 20: Oversold (high volume selling)\n- Similar to RSI but considers volume\n\n2) HOW AND WHEN TO USE:\n\nWhen to INCLUDE MFI:\n- Volume-based strategies\n- Confirmation of RSI signals\n- Identifying buying/selling pressure\n- Volume divergences (price up but MFI down = weak)\n\nTimeframes to select:\n- Day trading: Select 15m, 30m, 1h\n- Swing trading: Select 4h, 1d\n- DEFAULT: OPTIONAL (if volume important)\n\n3) WHY TO USE IT:\n- Adds volume context to momentum\n- Volume divergences powerful signals\n- Confirms RSI signals with volume\n\n4) EFFECTS:\n\n4.1) NOT SELECTED:\n- Model misses volume-momentum relationship\n- Can't detect volume divergences\n- Accuracy loss ~0.5-1%\n\n4.2) SELECTED:\n- Model learns volume-weighted momentum\n- Identifies strong vs weak moves (volume confirmation)\n- Typical improvement: +0.5-1.5% accuracy\n\nFeatures generated:\n- mfi_value\n- mfi_overbought (> 80)\n- mfi_oversold (< 20)\n- mfi_divergence (price vs MFI mismatch)\n- ~6-8 features per timeframe\n\n5) TYPICAL RANGE / DEFAULT VALUES:\n\n- DEFAULT: OPTIONAL (secondary to RSI)\n- Period: 14\n- Overbought: 80\n- Oversold: 20\n\nUsage:\n- 50% of users include MFI\n- Useful when volume data reliable\n\nRecommendation:\n- INCLUDE if volume important to your strategy\n- SKIP if RSI sufficient\n\n6) ADDITIONAL NOTES / BEST PRACTICES:\n\nInterpretation:\n- MFI > 80: Strong buying pressure, overbought\n- MFI < 20: Strong selling pressure, oversold\n- MFI divergence: Price new high but MFI lower = bearish\n\nMFI vs RSI:\n- MFI: Includes volume (stronger signal)\n- RSI: Price-only (more common)\n- Use MFI when volume data reliable (forex volume limited)\n\nTrading applications:\n- Buy: MFI < 20 + rising (accumulation)\n- Sell: MFI > 80 + falling (distribution)\n- Divergence: Price up + MFI down = weak rally (sell)\n\nNote on forex:\n- Forex has no centralized volume (only tick volume)\n- MFI less reliable on forex than stocks/futures\n- Consider skipping MFI for forex trading"
      },
      
      "obv": {
        "label": "OBV (On-Balance Volume):",
        "tooltip": "OBV - On-Balance Volume (Cumulative Volume Indicator)\n\n1) WHAT IT IS:\nCumulative volume indicator tracking buying/selling pressure.\nOBV = Previous OBV + (Close > Previous Close ? Volume : -Volume)\n\nRising OBV = accumulation (buying pressure)\nFalling OBV = distribution (selling pressure)\nOBV confirms price trends with volume\n\n2) HOW AND WHEN TO USE:\n\nWhen to INCLUDE OBV:\n- Volume confirmation of trends\n- Divergence detection (price up, OBV down = weak)\n- Identifying accumulation/distribution\n- Volume analysis strategies\n\nTimeframes to select:\n- Day trading: Select 15m, 30m, 1h\n- Swing trading: Select 4h, 1d\n- DEFAULT: OPTIONAL\n\n3) WHY TO USE IT:\n- Volume confirms price moves (strong move = volume confirmation)\n- Divergences warn of reversals\n- Leading indicator (OBV often moves before price)\n\n4) EFFECTS:\n\n4.1) NOT SELECTED:\n- Model misses volume trends\n- Can't detect volume divergences\n- Accuracy loss ~0.5-1%\n\n4.2) SELECTED:\n- Model learns volume trend confirmation\n- Identifies accumulation/distribution phases\n- Typical improvement: +0.5-1% accuracy\n\nFeatures generated:\n- obv_value\n- obv_trend (rising/falling)\n- obv_divergence\n- ~4-6 features per timeframe\n\n5) TYPICAL RANGE / DEFAULT VALUES:\n\n- DEFAULT: OPTIONAL (skip for forex)\n- No period parameter (cumulative)\n\nUsage:\n- 40% include OBV\n- Better for stocks than forex\n\nRecommendation:\n- SKIP for forex (volume unreliable)\n- INCLUDE for stocks/futures\n\n6) ADDITIONAL NOTES / BEST PRACTICES:\n\nInterpretation:\n- OBV rising + price rising = confirmed uptrend\n- OBV falling + price falling = confirmed downtrend\n- OBV flat + price moving = weak trend (reversal coming)\n\nDivergence:\n- Price new high + OBV lower = bearish divergence\n- Price new low + OBV higher = bullish divergence\n\nForex limitation:\n- No centralized volume in forex\n- Tick volume not true volume\n- OBV less reliable on forex pairs"
      },
      
      "trix": {
        "label": "TRIX:",
        "tooltip": "TRIX - Triple Exponential Average (Momentum Oscillator)\n\n1) WHAT IT IS:\nMomentum oscillator showing rate of change of triple-smoothed EMA.\nTRIX = % change in EMA(EMA(EMA(Close, N)))\n\nOscillates around zero:\n- TRIX > 0: Bullish momentum\n- TRIX < 0: Bearish momentum\n- TRIX crossover signal line: Trade signal\n\n2) HOW AND WHEN TO USE:\n\nWhen to INCLUDE TRIX:\n- Filtered momentum (triple smoothing removes noise)\n- Long-term trend following\n- Divergence detection\n- Best on higher timeframes (4h, 1d)\n\nTimeframes to select:\n- Swing trading: Select 4h, 1d\n- DEFAULT: OPTIONAL (redundant with MACD)\n\n3) WHY TO USE IT:\n- Very smooth (filters noise with triple EMA)\n- Good for long-term trends\n- Fewer false signals than MACD\n\n4) EFFECTS:\n\n4.1) NOT SELECTED:\n- Model misses smoothed momentum\n- Similar to MACD (not critical)\n- Accuracy loss ~0.3-0.5%\n\n4.2) SELECTED:\n- Model learns smooth trend momentum\n- Fewer false signals\n- Typical improvement: +0.3-0.8% accuracy\n\nFeatures generated:\n- trix_value\n- trix_signal (signal line)\n- trix_cross_above\n- ~4-6 features per timeframe\n\n5) TYPICAL RANGE / DEFAULT VALUES:\n\n- DEFAULT: SKIP (MACD better)\n- Period: 15 (standard)\n- Signal: 9 (EMA of TRIX)\n\nUsage:\n- 25% include TRIX\n- Less popular than MACD\n\nRecommendation:\n- SKIP (use MACD instead)\n- INCLUDE only if you want very smooth momentum\n\n6) ADDITIONAL NOTES / BEST PRACTICES:\n\nInterpretation:\n- TRIX crosses above 0: Bullish\n- TRIX crosses below 0: Bearish\n- TRIX crosses signal line: Trade signal\n\nTRIX vs MACD:\n- TRIX: Triple smoothing, slower, fewer signals\n- MACD: Double smoothing, faster, more signals\n- Use MACD for most cases"
      },
      
      "ultimate": {
        "label": "Ultimate Oscillator:",
        "tooltip": "Ultimate Oscillator - Multi-Period Momentum\n\n1) WHAT IT IS:\nMomentum oscillator using 3 timeframes (7, 14, 28 periods).\nCombines short, medium, long-term momentum.\n\nRange: 0-100\n- Ultimate > 70: Overbought\n- Ultimate < 30: Oversold\n- Reduces false signals by using multiple periods\n\n2) HOW AND WHEN TO USE:\n\nWhen to INCLUDE Ultimate:\n- Multi-timeframe momentum confirmation\n- Reducing false signals\n- Divergence detection\n- Alternative to RSI\n\nTimeframes to select:\n- Day trading: Select 15m, 30m, 1h\n- Swing trading: Select 4h, 1d\n- DEFAULT: OPTIONAL\n\n3) WHY TO USE IT:\n- Combines 3 periods (more robust than single-period RSI)\n- Fewer false signals\n- Good divergence indicator\n\n4) EFFECTS:\n\n4.1) NOT SELECTED:\n- Model misses multi-period momentum\n- Similar to RSI (not critical)\n- Accuracy loss ~0.3-0.5%\n\n4.2) SELECTED:\n- Model learns multi-period momentum\n- More robust signals than RSI\n- Typical improvement: +0.3-0.8% accuracy\n\nFeatures generated:\n- ultimate_value\n- ultimate_overbought\n- ultimate_oversold\n- ~4-5 features per timeframe\n\n5) TYPICAL RANGE / DEFAULT VALUES:\n\n- DEFAULT: SKIP (RSI sufficient)\n- Periods: 7, 14, 28 (standard)\n- Overbought: 70\n- Oversold: 30\n\nUsage:\n- 20% include Ultimate\n- Less common than RSI\n\nRecommendation:\n- SKIP (use RSI instead)\n- INCLUDE only if reducing false signals critical\n\n6) ADDITIONAL NOTES / BEST PRACTICES:\n\nInterpretation:\n- Ultimate > 70: Overbought\n- Ultimate < 30: Oversold\n- Divergence: Price new high + Ultimate lower = bearish\n\nUltimate vs RSI:\n- Ultimate: Multi-period (7, 14, 28), more robust\n- RSI: Single period (14), simpler\n- Use RSI for most cases"
      },
      
      "donchian": {
        "label": "Donchian Channels:",
        "tooltip": "Donchian Channels - Breakout Indicator\n\n1) WHAT IT IS:\nPrice channel showing highest high and lowest low over N periods.\nUpper Band = Highest high over N periods\nMiddle Band = (Upper + Lower) / 2\nLower Band = Lowest low over N periods\n\nPrice breaks above upper = bullish breakout\nPrice breaks below lower = bearish breakout\n\n2) HOW AND WHEN TO USE:\n\nWhen to INCLUDE Donchian:\n- Breakout strategies (price leaving range)\n- Support/resistance levels\n- Trend following (breakouts)\n- Volatility measurement\n\nTimeframes to select:\n- Day trading: Select 15m, 30m, 1h\n- Swing trading: Select 4h, 1d\n- DEFAULT: OPTIONAL\n\n3) WHY TO USE IT:\n- Identifies breakout levels clearly\n- Support/resistance from recent extremes\n- Simple and effective for breakout trading\n\n4) EFFECTS:\n\n4.1) NOT SELECTED:\n- Model misses breakout signals\n- Can't identify support/resistance from channels\n- Accuracy loss ~0.5-1%\n\n4.2) SELECTED:\n- Model learns breakout patterns\n- Identifies valid breakouts from consolidation\n- Typical improvement: +0.5-1% accuracy\n\nFeatures generated:\n- donchian_upper\n- donchian_middle\n- donchian_lower\n- donchian_breakout_up\n- donchian_breakout_down\n- ~6-8 features per timeframe\n\n5) TYPICAL RANGE / DEFAULT VALUES:\n\n- DEFAULT: OPTIONAL (similar to Bollinger)\n- Period: 20 (standard)\n\nUsage:\n- 35% include Donchian\n- Popular for breakout trading\n\nRecommendation:\n- INCLUDE if trading breakouts\n- SKIP if using Bollinger Bands (similar concept)\n\n6) ADDITIONAL NOTES / BEST PRACTICES:\n\nInterpretation:\n- Price at upper band: At resistance (breakout imminent)\n- Price at lower band: At support (breakdown imminent)\n- Price breaks above upper: Bullish breakout (buy)\n- Price breaks below lower: Bearish breakdown (sell)\n\nDonchian vs Bollinger:\n- Donchian: High/Low extremes, clearer breakout levels\n- Bollinger: StdDev bands, relative volatility\n- Use Donchian for breakouts, Bollinger for mean reversion\n\nTrading applications:\n- Breakout: Buy when price breaks above upper band\n- Stop loss: Below middle band (or lower band)\n- Trend: Price near upper band = uptrend\n\nTurtle Trading:\n- Famous strategy uses Donchian Channels\n- 20-period Donchian for entries\n- 10-period Donchian for exits"
      },
      
      "keltner": {
        "label": "Keltner Channels:",
        "tooltip": "Keltner Channels - ATR-Based Volatility Bands\n\n1) WHAT IT IS:\nVolatility bands around EMA using ATR (not StdDev).\nMiddle Band = EMA(N)\nUpper Band = EMA + (Multiplier × ATR)\nLower Band = EMA - (Multiplier × ATR)\n\nSimilar to Bollinger but uses ATR instead of StdDev\n\n2) HOW AND WHEN TO USE:\n\nWhen to INCLUDE Keltner:\n- ATR-based volatility (absolute vs relative)\n- Trend following with volatility context\n- Breakout confirmation\n- Alternative to Bollinger Bands\n\nTimeframes to select:\n- Day trading: Select 15m, 30m, 1h\n- Swing trading: Select 4h, 1d\n- DEFAULT: OPTIONAL\n\n3) WHY TO USE IT:\n- ATR more stable than StdDev (smoother bands)\n- Good for trending markets\n- Combines trend (EMA) + volatility (ATR)\n\n4) EFFECTS:\n\n4.1) NOT SELECTED:\n- Model misses ATR-based bands\n- Similar to Bollinger (not critical)\n- Accuracy loss ~0.3-0.5%\n\n4.2) SELECTED:\n- Model learns ATR-based volatility patterns\n- Smoother signals than Bollinger\n- Typical improvement: +0.3-0.8% accuracy\n\nFeatures generated:\n- keltner_upper\n- keltner_middle (EMA)\n- keltner_lower\n- keltner_width\n- keltner_percent\n- ~6-8 features per timeframe\n\n5) TYPICAL RANGE / DEFAULT VALUES:\n\n- DEFAULT: SKIP (use Bollinger or ATR)\n- EMA Period: 20\n- ATR Period: 10\n- Multiplier: 2.0\n\nUsage:\n- 30% include Keltner\n- Less common than Bollinger\n\nRecommendation:\n- SKIP if using Bollinger + ATR (redundant)\n- INCLUDE only if you prefer ATR-based bands\n\n6) ADDITIONAL NOTES / BEST PRACTICES:\n\nInterpretation:\n- Price at upper band: Strong uptrend\n- Price at lower band: Strong downtrend\n- Price breaks above upper: Continuation (not reversal like Bollinger)\n- Narrow bands: Low volatility (breakout coming)\n\nKeltner vs Bollinger:\n- Keltner: ATR-based, smoother, trend-following\n- Bollinger: StdDev-based, more reactive, mean-reversion\n- Keltner gives fewer false breakout signals\n\nTrading applications:\n- Trend: Stay long when price above middle band\n- Breakout: Price breaks above upper = continuation\n- Stop loss: Below lower band"
      },
      
      "ema": {
        "label": "EMA (Exponential Moving Average):",
        "tooltip": "EMA - Exponential Moving Average (Trend Indicator)\n\n1) WHAT IT IS:\nWeighted moving average giving more weight to recent prices.\nEMA = (Close × Multiplier) + (Previous EMA × (1 - Multiplier))\nMultiplier = 2 / (Period + 1)\n\nMore responsive than SMA (reacts faster to price changes)\n\n2) HOW AND WHEN TO USE:\n\nWhen to INCLUDE EMA:\n- Trend identification (price above EMA = uptrend)\n- Dynamic support/resistance\n- EMA crossovers (fast EMA crosses slow EMA)\n- Moving average strategies\n\nTimeframes to select:\n- Day trading: Select 15m, 30m, 1h\n- Swing trading: Select 4h, 1d\n- DEFAULT: RECOMMENDED (basic trend indicator)\n\n3) WHY TO USE IT:\n- Simple trend indicator\n- Dynamic support/resistance\n- EMA crossovers powerful signals\n- Foundation of many strategies\n\n4) EFFECTS:\n\n4.1) NOT SELECTED:\n- Model misses trend context\n- Can't use MA crossover signals\n- Accuracy loss ~1-1.5%\n\n4.2) SELECTED:\n- Model learns trend from MAs\n- Identifies support/resistance\n- Typical improvement: +1-2% accuracy\n\nFeatures generated:\n- ema_value (multiple periods: 9, 21, 50, 200)\n- ema_cross_above (fast crosses above slow)\n- ema_cross_below\n- price_above_ema\n- ema_slope (rising/falling)\n- ~8-10 features per timeframe\n\n5) TYPICAL RANGE / DEFAULT VALUES:\n\n- DEFAULT: RECOMMENDED\n- Common periods: 9, 21, 50, 200\n- 9/21 EMA: Short-term crossover\n- 50/200 EMA: Golden/Death cross\n\nUsage:\n- 70% include EMA\n- Fundamental indicator\n\nRecommendation:\n- INCLUDE EMA (basic trend indicator)\n- Essential for trend-following strategies\n\n6) ADDITIONAL NOTES / BEST PRACTICES:\n\nInterpretation:\n- Price above EMA: Uptrend\n- Price below EMA: Downtrend\n- Fast EMA crosses above slow: Bullish (buy)\n- Fast EMA crosses below slow: Bearish (sell)\n\nCommon EMA combinations:\n- 9/21 EMA: Short-term crossover\n- 12/26 EMA: MACD components\n- 50/200 EMA: Golden cross (bullish) / Death cross (bearish)\n\nMulti-timeframe EMAs:\n- 200 EMA on daily: Major trend\n- 50 EMA on 4h: Swing trend\n- 21 EMA on 1h: Intraday trend\n\nEMA vs SMA:\n- EMA: Weighted, more responsive, better for trending\n- SMA: Equal weight, slower, better for support/resistance\n- Use EMA for most cases (more popular)"
      },
      
      "sma": {
        "label": "SMA (Simple Moving Average):",
        "tooltip": "SMA - Simple Moving Average (Trend Indicator)\n\n1) WHAT IT IS:\nArithmetic average of prices over N periods (equal weight).\nSMA = (Sum of Close prices over N periods) / N\n\nSlower than EMA (all prices weighted equally)\n\n2) HOW AND WHEN TO USE:\n\nWhen to INCLUDE SMA:\n- Smooth trend identification\n- Strong support/resistance levels\n- SMA crossovers\n- Alternative to EMA (smoother)\n\nTimeframes to select:\n- Day trading: Select 15m, 30m, 1h\n- Swing trading: Select 4h, 1d\n- DEFAULT: OPTIONAL (EMA more popular)\n\n3) WHY TO USE IT:\n- Smoother than EMA (less noise)\n- Strong support/resistance (psychological levels)\n- Institutional traders watch SMAs\n\n4) EFFECTS:\n\n4.1) NOT SELECTED:\n- Model misses smooth trend\n- Similar to EMA (not critical if EMA included)\n- Accuracy loss ~0.5-1%\n\n4.2) SELECTED:\n- Model learns smooth trend\n- Strong support/resistance levels\n- Typical improvement: +0.5-1% accuracy\n\nFeatures generated:\n- sma_value (multiple periods)\n- sma_cross_above\n- sma_cross_below\n- price_above_sma\n- sma_slope\n- ~8-10 features per timeframe\n\n5) TYPICAL RANGE / DEFAULT VALUES:\n\n- DEFAULT: SKIP (use EMA instead)\n- Common periods: 20, 50, 100, 200\n- 200 SMA: Major support/resistance\n\nUsage:\n- 50% include SMA\n- Less popular than EMA\n\nRecommendation:\n- SKIP if using EMA (redundant)\n- INCLUDE if you prefer smoother MAs\n\n6) ADDITIONAL NOTES / BEST PRACTICES:\n\nInterpretation:\n- Price above SMA: Uptrend\n- Price below SMA: Downtrend\n- Price bounces off SMA: Support/resistance\n\nCommon SMAs:\n- 20 SMA: Bollinger Bands middle line\n- 50 SMA: Intermediate trend\n- 200 SMA: Major trend (most watched)\n\nSMA vs EMA:\n- SMA: Equal weight, slower, smoother\n- EMA: Weighted, faster, more responsive\n- SMA better for support/resistance\n- EMA better for crossover signals\n\n200 SMA significance:\n- Most watched MA on daily chart\n- Strong psychological support/resistance\n- Institutional traders use 200 SMA\n- Price above 200 SMA = bull market\n- Price below 200 SMA = bear market"
      },
      
      "vwap": {
        "label": "VWAP (Volume Weighted Average Price):",
        "tooltip": "VWAP - Volume Weighted Average Price (Intraday Benchmark)\n\n1) WHAT IT IS:\nAverage price weighted by volume (resets daily).\nVWAP = Σ(Price × Volume) / Σ(Volume)\n\nTypically used on intraday timeframes (resets at market open)\n\n2) HOW AND WHEN TO USE:\n\nWhen to INCLUDE VWAP:\n- Intraday trading (1m-15m timeframes)\n- Institutional benchmark (institutions try to trade near VWAP)\n- Fair value reference\n- Mean reversion around VWAP\n\nTimeframes to select:\n- Scalping: Select 1m, 5m\n- Day trading: Select 15m (VWAP best intraday)\n- DEFAULT: OPTIONAL (intraday only)\n\n3) WHY TO USE IT:\n- Institutional benchmark (liquidity near VWAP)\n- Fair value (volume-weighted average)\n- Mean reversion (price returns to VWAP)\n- Volume context\n\n4) EFFECTS:\n\n4.1) NOT SELECTED:\n- Model misses volume-weighted fair value\n- Can't trade mean reversion to VWAP\n- Accuracy loss ~0.5-1% (intraday)\n\n4.2) SELECTED:\n- Model learns VWAP mean reversion\n- Institutional behavior near VWAP\n- Typical improvement: +0.5-1.5% accuracy (intraday)\n\nFeatures generated:\n- vwap_value\n- price_above_vwap\n- distance_from_vwap\n- vwap_bands (VWAP ± StdDev)\n- ~5-8 features per timeframe\n\n5) TYPICAL RANGE / DEFAULT VALUES:\n\n- DEFAULT: OPTIONAL (skip for swing trading)\n- No period (cumulative from market open)\n- Resets daily\n\nUsage:\n- 40% include VWAP (intraday traders)\n- Essential for institutional trading\n\nRecommendation:\n- INCLUDE for day trading (1m-15m)\n- SKIP for swing trading (4h-1d)\n\n6) ADDITIONAL NOTES / BEST PRACTICES:\n\nInterpretation:\n- Price above VWAP: Bullish (paying premium)\n- Price below VWAP: Bearish (getting discount)\n- Price returns to VWAP: Mean reversion\n\nTrading applications:\n- Mean reversion: Price far from VWAP → expect return\n- Support/resistance: VWAP acts as dynamic S/R\n- Institutions: Try to execute near VWAP (fair value)\n\nVWAP bands:\n- VWAP + 1 StdDev: First resistance\n- VWAP + 2 StdDev: Strong resistance (overbought)\n- VWAP - 1 StdDev: First support\n- VWAP - 2 StdDev: Strong support (oversold)\n\nIntraday strategy:\n- Buy when price touches VWAP - 1 StdDev\n- Sell when price touches VWAP + 1 StdDev\n- Target: Return to VWAP\n\nForex limitation:\n- VWAP requires volume data\n- Forex tick volume not true volume\n- VWAP less reliable on forex than stocks\n- Consider skipping VWAP for forex"
      }
    },
    
    "advanced": {
      "warmup_bars": {
        "label": "Warmup Bars:",
        "tooltip": "Warmup Bars - Initial Bars to Discard\n\n1) WHAT IT IS:\nNumber of initial bars discarded to let indicators stabilize before training.\nIndicators need time to 'warm up' (e.g., 200-period SMA needs 200 bars).\n\nExample: Warmup 200 bars → first 200 bars not used for training\n\n2) HOW AND WHEN TO USE:\n\nWhen to use HIGH warmup (100-300 bars):\n- Using long-period indicators (200 SMA, 100 ATR)\n- Want stable, reliable indicator values\n- Large dataset (>10K samples, can afford to lose 200)\n\nWhen to use LOW warmup (10-50 bars):\n- Short-period indicators only (14 RSI, 20 Bollinger)\n- Small dataset (<5K samples, can't afford to lose many)\n- All indicators short-period\n\nWhen to use ZERO warmup (0 bars):\n- NEVER (indicator values will be NaN or unstable initially)\n- Always use at least 50 bars minimum\n\n3) WHY TO USE IT:\n- Prevent NaN values (indicators undefined initially)\n- Stable values (200 SMA on bar 10 is meaningless)\n- Avoid training on unreliable early data\n- Standard practice in time series ML\n\n4) EFFECTS:\n\n4.1) VERY LOW (0-20 bars):\n\nBehavior:\n- Almost no data discarded\n- Indicators unstable for first samples\n- Training uses potentially bad values\n\nAdvantages:\n- Maximum data usage (lose almost nothing)\n- Good if dataset very small\n\nDisadvantages:\n- UNSTABLE indicators initially (200 SMA on bar 50 = unreliable)\n- Model trains on bad values\n- Accuracy suffers\n- Risk of NaN values\n\nWhen to use:\n- AVOID (too risky)\n- Only if dataset <1K samples (desperate)\n\nTypical results:\n- Accuracy: -1 to -3% (worse due to bad initial values)\n\n4.2) LOW (50-100 bars):\n\nBehavior:\n- Moderate warmup\n- Short-period indicators stable (14 RSI, 20 BB)\n- Long-period indicators still warming up (200 SMA)\n\nAdvantages:\n- Good balance for short-period indicators\n- Don't lose too much data\n- DEFAULT for most users\n\nDisadvantages:\n- Long-period indicators still unstable\n- 200 SMA at bar 100 only half-warmed\n\nWhen to use:\n- DEFAULT (50-100 bars standard)\n- When using mostly short-period indicators\n- Dataset 5K-20K samples\n\nTypical results:\n- Accuracy: Baseline (good for short indicators)\n\n4.3) MEDIUM (100-200 bars):\n\nBehavior:\n- Good warmup for most indicators\n- Even 100-period indicators stable\n- 200-period indicators mostly stable\n\nAdvantages:\n- RECOMMENDED for production\n- All common indicators stable\n- Reliable values from start of training\n\nDisadvantages:\n- Lose 100-200 samples (not much if dataset large)\n\nWhen to use:\n- RECOMMENDED (best practice)\n- Production models\n- Using mix of short and long indicators\n- Dataset >10K samples\n\nTypical results:\n- Accuracy: +0.5-1% vs low warmup (stable indicators)\n\n4.4) HIGH (200-500 bars):\n\nBehavior:\n- Very conservative warmup\n- Even 200-period indicators fully stable\n- All indicators reliable\n\nAdvantages:\n- MAXIMUM STABILITY (all indicators warm)\n- No risk of unstable values\n- Professional/institutional standard\n\nDisadvantages:\n- Lose significant data (200-500 samples)\n- Only viable with large datasets\n\nWhen to use:\n- Using long-period indicators (200 SMA, 100 ATR)\n- Large dataset (>20K samples)\n- Production deployment (zero tolerance for unstable values)\n\nTypical results:\n- Accuracy: +1-2% vs low warmup (very stable)\n\n5) TYPICAL RANGE / DEFAULT VALUES:\n\n- Minimum: 50 bars (absolute minimum)\n- BEGINNER DEFAULT: 50 bars\n- INTERMEDIATE DEFAULT: 100 bars\n- ADVANCED DEFAULT: 200 bars\n- PROFESSIONAL: 300-500 bars\n- Maximum: 1000 bars (overkill unless using 500-period indicators)\n\nRule of thumb:\n- Warmup = 2× longest indicator period\n- Example: Using 200 SMA → warmup 400 bars\n- Practical: 100-200 bars covers most cases\n\n6) ADDITIONAL NOTES / BEST PRACTICES:\n\nCalculating required warmup:\n- Find longest period indicator:\n  * 200 SMA → need 200 bars minimum\n  * 100 ATR → need 100 bars minimum\n  * 50 EMA → need 50 bars minimum\n- Warmup = 2× longest (conservative)\n- Example: 200 SMA used → warmup 400 bars\n\nData loss consideration:\n- 7 days 1m data = 10,080 bars\n- Warmup 200 bars = lose 2% (acceptable)\n- 1 day 1m data = 1,440 bars\n- Warmup 200 bars = lose 14% (significant)\n\nInteraction with days parameter:\n- More days → can afford higher warmup\n- 7 days → warmup 100-200 bars OK\n- 1 day → warmup 50 bars maximum\n- 365 days → warmup 500 bars no problem\n\nSigns warmup too low:\n- NaN values in features\n- Training accuracy suspiciously high (overfitting on bad values)\n- Model fails immediately in production\n- Indicator values don't make sense\n\nSigns warmup too high:\n- Losing too much data (>10%)\n- Not enough training samples\n- No benefit vs lower warmup\n\nHow ForexGPT uses warmup:\n- Loads raw data\n- Calculates all indicators\n- Drops first N bars (warmup)\n- Remaining data used for train/test split\n- Metadata saves warmup value"
      },
      
      "rv_window": {
        "label": "Realized Volatility Window:",
        "tooltip": "Realized Volatility Window - Volatility Normalization Period\n\n1) WHAT IT IS:\nNumber of bars used to estimate realized volatility for feature normalization.\nRV = StdDev(returns over N bars)\n\nFeatures divided by RV to normalize by recent volatility\n(e.g., 10-pip move in low volatility ≠ 10-pip move in high volatility)\n\n2) HOW AND WHEN TO USE:\n\nWhen to use SHORT window (20-50 bars):\n- Fast adaptation to volatility changes\n- Markets with regime changes\n- Forex majors (volatility shifts quickly)\n\nWhen to use LONG window (100-200 bars):\n- Smooth volatility estimate\n- Stable markets\n- Avoid overreacting to short-term spikes\n\n3) WHY TO USE IT:\n- Normalize features by volatility\n- 10-pip move means different things in different volatility regimes\n- Model learns relationships independent of absolute volatility\n- Standard practice in quantitative finance\n\n4) EFFECTS:\n\n4.1) VERY SHORT (10-30 bars):\n\nBehavior:\n- RV estimate very responsive\n- Adapts quickly to volatility changes\n- Can be noisy\n\nAdvantages:\n- Fast adaptation to new volatility regime\n- Good for markets with frequent regime changes\n\nDisadvantages:\n- Noisy estimate (short window = high variance)\n- Can overreact to outliers\n\nWhen to use:\n- Markets with fast regime changes\n- Crypto (volatility changes rapidly)\n\nTypical results:\n- Accuracy: Baseline (good for fast-changing markets)\n\n4.2) MEDIUM (40-80 bars):\n\nBehavior:\n- DEFAULT\n- Balanced responsiveness and stability\n- RV estimate reasonable\n\nAdvantages:\n- Good balance\n- Adapts to regime changes within hours/days\n- Not too noisy\n\nDisadvantages:\n- May lag behind very fast volatility changes\n\nWhen to use:\n- DEFAULT for most cases\n- Forex majors\n- Stock indices\n\nTypical results:\n- Accuracy: Baseline (recommended)\n\n4.3) LONG (100-200 bars):\n\nBehavior:\n- Very smooth RV estimate\n- Slow to adapt to regime changes\n- Stable normalization\n\nAdvantages:\n- Smooth, stable estimate\n- Not sensitive to outliers\n- Good for stable markets\n\nDisadvantages:\n- SLOW to adapt (100 bars = 100 hours on 1h TF!)\n- May miss regime changes\n- Normalization outdated if volatility changed\n\nWhen to use:\n- Very stable markets\n- Long-term models (daily/weekly timeframes)\n\nTypical results:\n- Accuracy: Baseline (good for stable markets)\n\n5) TYPICAL RANGE / DEFAULT VALUES:\n\n- Minimum: 20 bars\n- DEFAULT: 50 bars (recommended)\n- Maximum: 200 bars\n\nRecommendation by timeframe:\n- 1m-5m: 30-50 bars (fast adaptation)\n- 15m-1h: 40-80 bars (balanced)\n- 4h-1d: 80-150 bars (smooth)\n\n6) ADDITIONAL NOTES / BEST PRACTICES:\n\nInteraction with timeframe:\n- 1m TF, 50-bar window = 50 minutes of volatility\n- 1h TF, 50-bar window = 50 hours ≈ 2 days\n- 1d TF, 50-bar window = 50 days\n\nVolatility normalization example:\n- Low vol regime: ATR = 0.001, 10-pip move = 10× ATR (huge!)\n- High vol regime: ATR = 0.005, 10-pip move = 2× ATR (normal)\n- Model learns: 'X× ATR move is significant' (not '10 pips is significant')\n\nWhen to increase window:\n- Outliers causing problems (isolated spikes)\n- Want smooth, stable normalization\n- Long-term models\n\nWhen to decrease window:\n- Fast-changing markets\n- Volatility regime shifts frequently\n- Want quick adaptation"
      },
      
      "min_coverage": {
        "label": "Min Feature Coverage:",
        "tooltip": "Minimum Feature Coverage - NaN Threshold\n\n1) WHAT IT IS:\nMinimum fraction of non-NaN values required to include a feature.\nRange: 0.0 to 1.0\n\nExample: min_coverage=0.8 → feature needs ≥80% valid (non-NaN) values\n\n2) HOW AND WHEN TO USE:\n\nWhen to use HIGH threshold (0.8-1.0):\n- Want only complete features (few NaN)\n- Can afford to lose some features\n- Quality over quantity\n\nWhen to use LOW threshold (0.3-0.6):\n- Want to keep more features (even with gaps)\n- Small feature set (can't afford to lose any)\n- Data has many gaps\n\n3) WHY TO USE IT:\n- Filter out features with too many NaN values\n- NaN values cause problems (can't train, need imputation)\n- Better to exclude feature than use bad imputation\n\n4) EFFECTS:\n\n4.1) VERY LOW (0.1-0.4):\n\nBehavior:\n- Keep almost all features (even with 60% NaN!)\n- Many features with gaps\n- Need aggressive imputation\n\nAdvantages:\n- Maximum features kept\n- Don't lose information\n\nDisadvantages:\n- Features with many NaN unreliable\n- Imputation introduces noise\n- Model learns from imputed (fake) data\n\nWhen to use:\n- Data has unavoidable gaps\n- Small feature set (can't lose any)\n\nTypical results:\n- Accuracy: -1 to -2% (imputed values hurt)\n\n4.2) MEDIUM (0.5-0.7):\n\nBehavior:\n- Keep features with ≥50-70% valid values\n- Some imputation needed\n- Reasonable balance\n\nAdvantages:\n- Keep most features\n- Not too many NaN\n\nDisadvantages:\n- Still some imputation needed\n- Some features may be unreliable\n\nWhen to use:\n- DEFAULT for most cases\n- Data has some gaps but not terrible\n\nTypical results:\n- Accuracy: Baseline\n\n4.3) HIGH (0.8-0.95):\n\nBehavior:\n- Only keep features with ≥80-95% valid values\n- Very little imputation needed\n- Strict quality control\n\nAdvantages:\n- High-quality features only\n- Minimal imputation\n- Model trains on real data\n\nDisadvantages:\n- May lose some features\n- Reduced feature set\n\nWhen to use:\n- RECOMMENDED (quality over quantity)\n- Large feature set (can afford to lose some)\n- Production models\n\nTypical results:\n- Accuracy: +0.5-1.5% (high-quality features)\n\n4.4) VERY HIGH (0.95-1.0):\n\nBehavior:\n- Only perfect or near-perfect features\n- Zero tolerance for NaN\n- Extremely strict\n\nAdvantages:\n- Only complete features\n- No imputation needed\n- Maximum quality\n\nDisadvantages:\n- May lose many features\n- Very small feature set possible\n\nWhen to use:\n- Institutional/production (zero tolerance)\n- Large feature set (100+ features, can afford to drop some)\n\nTypical results:\n- Accuracy: +1-2% (if enough features remain)\n\n5) TYPICAL RANGE / DEFAULT VALUES:\n\n- Minimum: 0.3 (keep features with 30% valid)\n- DEFAULT: 0.7 (recommended)\n- PRODUCTION: 0.9 (high quality)\n- Maximum: 1.0 (perfect features only)\n\nRecommendation:\n- Start with 0.7\n- If losing too many features → lower to 0.5\n- If data very clean → raise to 0.9\n\n6) ADDITIONAL NOTES / BEST PRACTICES:\n\nCauses of NaN values:\n- Insufficient data for indicator (200 SMA on 100 bars)\n- Missing data in raw OHLCV\n- Division by zero in calculations\n- Indicator undefined (RSI when no price change)\n\nDiagnosing NaN issues:\n- Check how many features dropped\n- If >50% features dropped → threshold too strict or data problems\n- If no features dropped → data very clean\n\nImputation methods (when NaN remain):\n- Forward fill (use previous value)\n- Mean imputation (use column mean)\n- Drop rows (lose samples)\n- Model-based imputation\n\nForexGPT imputation:\n- Forward fill first (most common)\n- Then backward fill (for leading NaN)\n- Drop row if still NaN (last resort)\n\nInteraction with warmup:\n- Higher warmup → fewer NaN (indicators warm)\n- Lower warmup → more NaN (indicators not ready)\n- Use both together: warmup 200 + min_coverage 0.8 = clean data"
      },
      
      "indicator_periods": {
        "atr_n": {
          "label": "ATR Period:",
          "tooltip": "ATR Period - Average True Range Lookback Window\n\n1) WHAT IT IS:\nNumber of bars used to calculate ATR moving average.\nATR_N = MA(True Range over N periods)\n\nStandard: 14 periods (widely used default)\n\n2) HOW AND WHEN TO USE:\n\nWhen to use SHORT period (5-10):\n- Faster response to volatility changes\n- Scalping/day trading\n- Markets with rapid volatility shifts\n\nWhen to use LONG period (20-30):\n- Smooth volatility estimate\n- Swing trading\n- Avoid overreacting to spikes\n\n3) WHY TO USE IT:\n- Controls ATR sensitivity\n- Different periods for different trading styles\n- Standard 14 well-tested across markets\n\n4) EFFECTS:\n\n4.1) SHORT (5-10):\n- Very responsive ATR\n- Captures quick volatility changes\n- More noise, less stable\n- Good for scalping\n\n4.2) MEDIUM (14):\n- DEFAULT and RECOMMENDED\n- Balanced responsiveness\n- Industry standard\n- Works for most strategies\n\n4.3) LONG (20-30):\n- Very smooth ATR\n- Filters short-term spikes\n- Good for position sizing\n- Slower to react\n\n5) TYPICAL RANGE:\n- Minimum: 5\n- DEFAULT: 14 (standard)\n- Maximum: 50\n\nRecommendation: Keep at 14 unless specific reason to change\n\n6) NOTES:\n- 14 is industry standard (used globally)\n- Changing from 14 makes comparison difficult\n- Only change if backtesting shows clear benefit"
        },
        
        "rsi_n": {
          "label": "RSI Period:",
          "tooltip": "RSI Period - Relative Strength Index Lookback Window\n\n1) WHAT IT IS:\nNumber of bars used to calculate RSI.\nRSI_N = 100 - (100 / (1 + AvgGain_N / AvgLoss_N))\n\nStandard: 14 periods (J. Welles Wilder's original)\n\n2) HOW AND WHEN TO USE:\n\nWhen to use SHORT period (7-10):\n- Faster, more signals\n- Scalping/day trading\n- More sensitive to price changes\n- More false signals\n\nWhen to use LONG period (21-30):\n- Slower, fewer signals\n- Swing trading\n- More reliable signals\n- May miss short-term moves\n\n3) WHY TO USE IT:\n- Controls RSI sensitivity\n- Trade-off: speed vs reliability\n- 14 balanced for most use cases\n\n4) EFFECTS:\n\n4.1) SHORT (7-10):\n- Fast RSI, many signals\n- Reaches overbought/oversold frequently\n- Good for active trading\n- More whipsaws\n\n4.2) MEDIUM (14):\n- DEFAULT and RECOMMENDED\n- Wilder's original period\n- Most tested and reliable\n- Industry standard\n\n4.3) LONG (21-30):\n- Slow RSI, few signals\n- Rarely reaches extremes\n- Very reliable when it does\n- May miss opportunities\n\n5) TYPICAL RANGE:\n- Minimum: 7\n- DEFAULT: 14 (Wilder's standard)\n- Maximum: 30\n\nRecommendation: Keep at 14 (most literature uses 14)\n\n6) NOTES:\n- 14 is J. Welles Wilder's original specification\n- Vast majority of RSI research uses 14\n- Overbought/oversold levels (70/30) calibrated for 14-period\n- Changing period changes where overbought/oversold occur"
        },
        
        "bb_n": {
          "label": "Bollinger Period:",
          "tooltip": "Bollinger Bands Period - Moving Average Length\n\n1) WHAT IT IS:\nNumber of bars for Bollinger Bands SMA and StdDev.\nMiddle = SMA(N)\nUpper/Lower = SMA(N) ± K×StdDev(N)\n\nStandard: 20 periods (John Bollinger's specification)\n\n2) HOW AND WHEN TO USE:\n\nWhen to use SHORT period (10-15):\n- Faster bands, more signals\n- Intraday trading\n- Bands adjust quickly to price\n- More breakout signals\n\nWhen to use LONG period (30-50):\n- Slower bands, fewer signals\n- Swing/position trading\n- Very smooth bands\n- More reliable signals\n\n3) WHY TO USE IT:\n- Controls band sensitivity\n- Different periods for different timeframes\n- 20 is Bollinger's tested default\n\n4) EFFECTS:\n\n4.1) SHORT (10-15):\n- Tight bands (close to price)\n- Frequent touches/crosses\n- Good for mean reversion\n- More false breakouts\n\n4.2) MEDIUM (20):\n- DEFAULT and RECOMMENDED\n- John Bollinger's specification\n- Most tested configuration\n- Industry standard\n\n4.3) LONG (30-50):\n- Wide bands (far from price)\n- Rare touches/crosses\n- Strong signals when occur\n- May miss short-term moves\n\n5) TYPICAL RANGE:\n- Minimum: 10\n- DEFAULT: 20 (Bollinger's standard)\n- Maximum: 50\n\nRecommendation: Keep at 20 (Bollinger's tested value)\n\n6) NOTES:\n- 20-period with 2.0 StdDev is Bollinger's specification\n- Changing period changes squeeze detection\n- All Bollinger literature uses 20\n- Works well across all timeframes and markets"
        },
        
        "hurst_window": {
          "label": "Hurst Window:",
          "tooltip": "Hurst Exponent Window - Trend Strength Estimation Period\n\n1) WHAT IT IS:\nNumber of bars used to calculate Hurst exponent (H).\nHurst exponent measures trend persistence vs mean reversion.\n\nH > 0.5: Trending (persistent)\nH = 0.5: Random walk\nH < 0.5: Mean reverting (anti-persistent)\n\n2) HOW AND WHEN TO USE:\n\nWhen to use SHORT window (20-50):\n- Fast adaptation to regime changes\n- Detect short-term trends\n- High-frequency trading\n- More responsive\n\nWhen to use LONG window (100-200):\n- Stable long-term trend measurement\n- Smooth regime detection\n- Swing/position trading\n- More reliable\n\n3) WHY TO USE IT:\n- Identifies market regime (trending vs ranging)\n- Adaptive strategy selection\n- Risk management (more caution in random markets)\n- Quantitative measure of predictability\n\n4) EFFECTS:\n\n4.1) VERY SHORT (10-30):\nBehavior:\n- Very responsive Hurst\n- Detects short-term regime changes\n- Noisy estimate\n\nAdvantages:\n- Fast regime detection\n- Good for scalping\n- Captures quick shifts\n\nDisadvantages:\n- Noisy (unreliable)\n- False regime signals\n- Overreacts to outliers\n\nWhen to use:\n- Scalping (1m-5m)\n- Fast-changing markets\n\nTypical results:\n- Hurst oscillates rapidly\n- Hard to trust signals\n\n4.2) SHORT (30-70):\nBehavior:\n- Responsive but reasonable\n- DEFAULT for most cases\n- Balanced stability\n\nAdvantages:\n- Good balance speed/stability\n- Works across timeframes\n- Reasonable regime detection\n\nDisadvantages:\n- Some noise remains\n\nWhen to use:\n- DEFAULT (most users)\n- 15m-1h timeframes\n- Day trading\n\nTypical results:\n- Hurst reasonably stable\n- Useful regime signals\n\n4.3) MEDIUM (70-150):\nBehavior:\n- Stable Hurst estimate\n- Slow to change regimes\n- Smooth measurement\n\nAdvantages:\n- Reliable regime detection\n- Low false signals\n- Good for strategic decisions\n\nDisadvantages:\n- Slow to detect regime changes\n- May lag actual shifts\n\nWhen to use:\n- Swing trading (4h-1d)\n- Strategic regime assessment\n- Risk management\n\nTypical results:\n- Hurst very stable\n- Clear regime identification\n\n4.4) LONG (150-300):\nBehavior:\n- Very stable, long-term Hurst\n- Identifies major regimes only\n- Extremely smooth\n\nAdvantages:\n- Maximum stability\n- Captures only major regime shifts\n- Good for position trading\n\nDisadvantages:\n- VERY SLOW to react\n- Misses short/medium term changes\n\nWhen to use:\n- Position trading (daily/weekly)\n- Long-term regime analysis\n\nTypical results:\n- Hurst almost constant\n- Only major shifts detected\n\n5) TYPICAL RANGE / DEFAULT VALUES:\n\n- Minimum: 20\n- DEFAULT: 50 (recommended)\n- PRODUCTION: 70-100\n- Maximum: 500\n\nRule of thumb:\n- Hurst window ≈ 3-5× prediction horizon\n- Example: horizon 4 → hurst_window 20-50\n- Example: horizon 24 → hurst_window 100-150\n\n6) ADDITIONAL NOTES / BEST PRACTICES:\n\nHurst interpretation:\n- H > 0.6: Strong trending (momentum strategies)\n- H = 0.5-0.6: Weak trend (mixed strategies)\n- H = 0.4-0.5: Weak mean reversion (range strategies)\n- H < 0.4: Strong mean reversion (contrarian strategies)\n\nTrading applications:\n- High Hurst → use trend following\n- Low Hurst → use mean reversion\n- H ≈ 0.5 → market unpredictable (reduce risk)\n\nModel usage:\n- ML model learns Hurst as feature\n- Adaptive: strategy switches based on Hurst\n- Risk: reduce position size when H ≈ 0.5\n\nCalculation method:\n- Rescaled Range (R/S) analysis\n- Detrended Fluctuation Analysis (DFA)\n- Both methods need sufficient data points\n\nStatistical significance:\n- Need >50 bars minimum for reasonable estimate\n- >100 bars for reliable estimate\n- >200 bars for publication-quality\n\nInteraction with timeframe:\n- 1m TF, 50-bar window = 50 minutes of Hurst\n- 1h TF, 50-bar window = 50 hours ≈ 2 days\n- Longer TF needs longer window for same reliability"
        },
        
        "returns_window": {
          "label": "Returns Window:",
          "tooltip": "Returns Calculation Window - Logarithmic Return Period\n\n1) WHAT IT IS:\nNumber of bars used to calculate returns (log(price_t / price_{t-N})).\nShort window = recent returns, Long window = longer-term returns.\n\nExample: window 5 → return over last 5 bars\n\n2) HOW AND WHEN TO USE:\n\nWhen to use SHORT window (1-5):\n- Capture recent momentum\n- High-frequency patterns\n- Fast-moving markets\n- Scalping/day trading\n\nWhen to use LONG window (10-30):\n- Capture longer trends\n- Smooth out noise\n- Swing trading\n- Position trading\n\n3) WHY TO USE IT:\n- Returns are core ML feature\n- Different windows capture different patterns\n- Model learns from multi-scale momentum\n- Standard in quantitative finance\n\n4) EFFECTS:\n\n4.1) VERY SHORT (1):\nBehavior:\n- Single-bar returns (most recent)\n- Bar-to-bar changes\n- Maximum noise\n\nAdvantages:\n- Captures immediate momentum\n- Ultra-responsive\n- Good for tick-level patterns\n\nDisadvantages:\n- VERY NOISY (mostly noise, little signal)\n- Random walk dominant\n- Hard to predict\n\nWhen to use:\n- Tick/second data\n- Noise modeling\n- Volatility estimation\n\nTypical results:\n- Feature very noisy\n- Low predictive power alone\n\n4.2) SHORT (2-7):\nBehavior:\n- Recent short-term returns\n- DEFAULT for most cases\n- Good signal-to-noise\n\nAdvantages:\n- Captures short momentum\n- Reasonable noise level\n- Works across timeframes\n\nDisadvantages:\n- May miss longer trends\n\nWhen to use:\n- DEFAULT (most users)\n- 1m-1h timeframes\n- Day trading\n\nTypical results:\n- Good predictive feature\n- Balances signal/noise\n\n4.3) MEDIUM (7-20):\nBehavior:\n- Medium-term returns\n- Captures swing momentum\n- Smoother than short\n\nAdvantages:\n- Less noise\n- Captures trends better\n- Good for swing trading\n\nDisadvantages:\n- Slower to react\n- May lag recent changes\n\nWhen to use:\n- Swing trading (4h-1d)\n- Trend following\n- Medium-term models\n\nTypical results:\n- Smooth feature\n- Good for trend detection\n\n4.4) LONG (20-50):\nBehavior:\n- Long-term returns\n- Very smooth\n- Captures major trends only\n\nAdvantages:\n- Minimal noise\n- Clear trend signal\n- Good for position trading\n\nDisadvantages:\n- VERY SLOW to react\n- Misses short-term opportunities\n\nWhen to use:\n- Position trading (daily/weekly)\n- Long-term trend models\n\nTypical results:\n- Very smooth feature\n- Lags current price significantly\n\n5) TYPICAL RANGE / DEFAULT VALUES:\n\n- Minimum: 1\n- DEFAULT: 5 (recommended)\n- SWING: 10-20\n- POSITION: 20-50\n- Maximum: 100\n\nRecommendation by timeframe:\n- 1m-5m: 3-7 bars\n- 15m-1h: 5-10 bars\n- 4h-1d: 10-30 bars\n\n6) ADDITIONAL NOTES / BEST PRACTICES:\n\nMultiple return windows:\n- ForexGPT often calculates multiple windows\n- Example: 1, 5, 10, 20 bar returns\n- Captures multi-scale momentum\n- ML model learns which matters when\n\nReturn calculation:\n- Log return: log(price_t / price_{t-N})\n- Preferred over simple return for ML\n- Additive (easier for models)\n- Handles large changes better\n\nInteraction with horizon:\n- Returns window should be < horizon\n- Example: horizon 4 → returns_window ≤ 4\n- Otherwise: using future information!\n\nStatistical properties:\n- Returns more stationary than prices\n- Mean ≈ 0, variance depends on window\n- Longer window → higher variance\n- Fat tails (extreme values common)\n\nFeature importance:\n- Returns typically top 5 features\n- Recent returns (1-5 bar) most predictive\n- Longer returns less predictive\n- But combination powerful"
        },
        
        "vp_window": {
          "label": "Volume Profile Window:",
          "tooltip": "Volume Profile Window - Volume Distribution Analysis Period\n\n1) WHAT IT IS:\nNumber of bars used to calculate volume profile (volume distribution by price level).\nAnalyzes where most volume traded over N bars.\n\nIdentifies support/resistance from volume clusters\n\n2) HOW AND WHEN TO USE:\n\nWhen to use SHORT window (20-50):\n- Recent volume levels\n- Intraday support/resistance\n- Fast-changing markets\n- Day trading\n\nWhen to use LONG window (200-500):\n- Long-term volume levels\n- Major support/resistance zones\n- Position trading\n- Institutional levels\n\n3) WHY TO USE IT:\n- Volume clusters = support/resistance\n- High-volume areas = fair value (price attracted)\n- Low-volume areas = fast moves (price repelled)\n- Used by institutional traders\n\n4) EFFECTS:\n\n4.1) VERY SHORT (20-50):\nBehavior:\n- Very recent volume distribution\n- Changes rapidly\n- Intraday levels\n\nAdvantages:\n- Current session support/resistance\n- Fast adaptation to new levels\n- Good for day trading\n\nDisadvantages:\n- Less significant levels\n- May not hold long\n- More noise\n\nWhen to use:\n- Day trading (1m-15m)\n- Intraday levels\n\nTypical results:\n- Many small volume nodes\n- Short-lived support/resistance\n\n4.2) SHORT (50-100):\nBehavior:\n- Recent days volume\n- DEFAULT for most cases\n- Reasonable stability\n\nAdvantages:\n- Good balance\n- Works for day/swing trading\n- Relevant levels\n\nDisadvantages:\n- May miss longer-term key levels\n\nWhen to use:\n- DEFAULT (most users)\n- Day and swing trading\n- 15m-1h timeframes\n\nTypical results:\n- Clear volume nodes\n- Useful support/resistance\n\n4.3) MEDIUM (100-200):\nBehavior:\n- Week(s) of volume data\n- Significant levels\n- Stable zones\n\nAdvantages:\n- Important volume levels\n- Tested support/resistance\n- Good for swing trading\n\nDisadvantages:\n- Slower to update\n- May include outdated levels\n\nWhen to use:\n- Swing trading (4h-1d)\n- Weekly analysis\n\nTypical results:\n- Strong volume nodes\n- Reliable support/resistance\n\n4.4) LONG (200-500):\nBehavior:\n- Month(s) of volume data\n- Major institutional levels\n- Very stable zones\n\nAdvantages:\n- MAJOR support/resistance\n- Institutional levels\n- Very significant zones\n\nDisadvantages:\n- VERY SLOW to update\n- May not reflect recent regime changes\n\nWhen to use:\n- Position trading (daily/weekly)\n- Long-term analysis\n- Institutional levels\n\nTypical results:\n- Few major volume nodes\n- Extremely strong levels\n\n5) TYPICAL RANGE / DEFAULT VALUES:\n\n- Minimum: 20\n- DAY TRADING: 50-100\n- DEFAULT: 100 (recommended)\n- SWING: 150-250\n- POSITION: 300-500\n- Maximum: 1000\n\nRecommendation by timeframe:\n- 1m-5m: 50-100 bars (intraday)\n- 15m-1h: 100-200 bars (recent days)\n- 4h-1d: 200-500 bars (weeks/months)\n\n6) ADDITIONAL NOTES / BEST PRACTICES:\n\nVolume Profile concepts:\n- Point of Control (POC): Price with most volume\n- Value Area: 70% of volume (fair value zone)\n- High/Low Volume Nodes: Support/resistance\n\nTrading applications:\n- Price near POC: Fair value (expect ranging)\n- Price at value area high: Resistance\n- Price at value area low: Support\n- Price outside value area: Extension (mean revert to POC)\n\nML feature usage:\n- Distance from POC (normalized)\n- Inside/outside value area (binary)\n- Volume at current price level\n- Model learns: price attracted to high-volume zones\n\nForex limitation:\n- Forex volume = tick volume (not true volume)\n- Less reliable than stocks/futures\n- Still useful for relative analysis\n- Consider with caution\n\nInteraction with timeframe:\n- 1m TF, 100-bar window = 100 minutes ≈ 1.5 hours\n- 1h TF, 100-bar window = 100 hours ≈ 4 days\n- 1d TF, 100-bar window = 100 days ≈ 5 months\n\nComputational cost:\n- Volume profile calculation expensive\n- Creates histogram (price bins)\n- Longer window = more computation\n- Typically cached/updated incrementally"
        }
      },
      
      "features": {
        "trading_sessions": {
          "label": "Trading Sessions Features:",
          "tooltip": "Trading Sessions - Time-of-Day Market Regime Features\n\n1) WHAT IT IS:\nFeatures encoding which trading session is active:\n- Asian session (Tokyo: 00:00-09:00 GMT)\n- European session (London: 08:00-17:00 GMT)\n- US session (New York: 13:00-22:00 GMT)\n- Overlaps (London-NY: highest volume)\n\n2) HOW AND WHEN TO USE:\n\nWhen to ENABLE:\n- Intraday trading (1m-1h timeframes)\n- Session-specific strategies\n- Forex markets (sessions matter most)\n- Volatility-aware trading\n\nWhen to DISABLE:\n- Daily+ timeframes (sessions irrelevant)\n- 24/7 markets without sessions (some crypto)\n- Simple models (reduce features)\n\n3) WHY TO USE IT:\n- Different sessions = different behavior\n- Asian: Low volatility, ranging\n- London: High volatility, trends start\n- NY: Highest volume, continuation/reversal\n- London-NY overlap: Maximum volatility\n\n4) EFFECTS:\n\n4.1) DISABLED:\n- Model doesn't know time of day\n- Misses session patterns\n- Same strategy 24/7\n\nTypical results:\n- Accuracy: Baseline\n- May trade poorly during wrong sessions\n\n4.2) ENABLED:\n- Model learns session-specific patterns\n- Different behavior per session\n- Adaptive to time of day\n\nTypical results:\n- Accuracy: +1-3% (intraday)\n- Better risk management (avoid Asian if want volatility)\n- Session-aware entry/exit\n\n5) TYPICAL RANGE:\n\n- DEFAULT: ENABLED (for intraday)\n- Disable: Only for daily+ timeframes\n\nFeatures generated:\n- is_asian_session (binary)\n- is_european_session (binary)\n- is_us_session (binary)\n- is_overlap_london_ny (binary)\n- hours_since_session_start (numeric)\n- ~5-10 features total\n\n6) NOTES:\n\nSession characteristics:\n- Asian (Tokyo): Lowest volatility, range-bound, JPY pairs active\n- European (London): Volatility picks up, trends begin, EUR/GBP active\n- US (NY): Highest volume, major moves, USD pairs active\n- Overlap (London+NY): Maximum volatility and volume\n\nTrading implications:\n- Scalping: Trade overlaps (high volume/volatility)\n- Trend following: Enter during London, hold through NY\n- Range trading: Trade Asian session\n- Avoid: Low liquidity times (weekends, holidays)\n\nML usage:\n- Model learns: Asian = mean reversion, NY = momentum\n- Adaptive strategy based on session\n- Risk management: reduce size during Asian\n\nForex specific:\n- EUR/USD: Most active London-NY overlap\n- USD/JPY: Active during Asian and NY\n- GBP/USD: Most active during London\n- AUD/USD: Active during Asian session"
        },
        
        "candlestick_patterns": {
          "label": "Candlestick Patterns:",
          "tooltip": "Candlestick Patterns - Japanese Technical Analysis Signals\n\n1) WHAT IT IS:\nRecognition of classic candlestick patterns:\n- Doji (indecision)\n- Hammer/Hanging Man (reversal)\n- Engulfing (strong reversal)\n- Morning/Evening Star (major reversal)\n- Three White Soldiers / Black Crows (continuation)\n- ~50+ patterns recognized\n\n2) HOW AND WHEN TO USE:\n\nWhen to ENABLE:\n- Using candlestick charts (OHLC data)\n- Reversal trading strategies\n- Want technical pattern signals\n- Combining ML with classical TA\n\nWhen to DISABLE:\n- Line charts (no OHLC)\n- Pure price-action models\n- Too many features already\n- Fundamental-only strategies\n\n3) WHY TO USE IT:\n- 300+ years of Japanese trading wisdom\n- Pattern recognition proven effective\n- Reversal/continuation signals\n- Works across all markets and timeframes\n\n4) EFFECTS:\n\n4.1) DISABLED:\n- Model doesn't see candlestick patterns\n- Misses classical reversal signals\n- Only raw OHLC values\n\nTypical results:\n- Accuracy: Baseline\n- May miss obvious pattern reversals\n\n4.2) ENABLED:\n- Model sees ~50+ candlestick patterns\n- Learns which patterns actually work\n- Combines patterns with other features\n\nTypical results:\n- Accuracy: +0.5-2% (if patterns relevant)\n- Better reversal detection\n- Improved entry/exit timing\n\n5) TYPICAL RANGE:\n\n- DEFAULT: ENABLED (recommended)\n- Adds ~50-100 features (pattern detected: yes/no)\n\nPatterns recognized (examples):\n- Reversal: Doji, Hammer, Engulfing, Star patterns\n- Continuation: Three soldiers/crows, Marubozu\n- Indecision: Spinning tops, High wave candles\n\n6) NOTES:\n\nTop candlestick patterns:\n1. Doji: Indecision, potential reversal\n2. Hammer: Bullish reversal (after downtrend)\n3. Shooting Star: Bearish reversal (after uptrend)\n4. Engulfing: Strong reversal (bullish/bearish)\n5. Morning/Evening Star: Major reversal signals\n\nPattern reliability:\n- Engulfing: High reliability (70%+)\n- Doji: Moderate (50-60%, needs confirmation)\n- Three soldiers: High (continuation 65%+)\n- Hammer: Moderate-high (60-70%)\n\nML advantage:\n- Classical TA: Use patterns blindly\n- ML approach: Learn which patterns work WHEN\n- Example: Doji works in ranging markets, not trends\n- Model learns context automatically\n\nMulti-timeframe:\n- Patterns more reliable on higher TF\n- Daily pattern > 1h pattern > 15m pattern\n- Model can learn this automatically\n\nCombination power:\n- Pattern + indicator confirmation = stronger\n- Example: Hammer + RSI oversold = high probability reversal\n- ML learns best combinations\n\nComputational cost:\n- Pattern recognition fast (rule-based)\n- Negligible overhead\n- Worth enabling almost always"
        }
      }
      
      "lightgbm": {
        "light_epochs": {
          "label": "LightGBM Epochs:",
          "tooltip": "LightGBM Training Iterations (Boosting Rounds)\n\n1) WHAT IT IS:\nNumber of boosting iterations (trees) to train.\nMore epochs = more trees = more complex model\n\nEach epoch adds one tree to the ensemble\n\n2) HOW AND WHEN TO USE:\n\nWhen to use FEW epochs (50-200):\n- Small dataset (<5K samples)\n- Avoid overfitting\n- Fast training\n- Simple patterns\n\nWhen to use MANY epochs (500-2000):\n- Large dataset (>50K samples)\n- Complex patterns\n- Have early stopping\n- Production models\n\n3) WHY TO USE IT:\n- Control model complexity\n- More epochs = better fit (up to a point)\n- Too many = overfitting\n- With early stopping, can set high and let it stop\n\n4) EFFECTS:\n\n4.1) VERY LOW (50-100):\n- Underfitting risk\n- Model too simple\n- Fast training (seconds)\n- May not capture patterns\nTypical accuracy: 54-56% (weak)\n\n4.2) LOW (100-300):\n- Good for small datasets\n- Balanced complexity\n- DEFAULT for beginners\n- Training: 1-3 minutes\nTypical accuracy: 56-60%\n\n4.3) MEDIUM (300-700):\n- RECOMMENDED for most cases\n- Good complexity\n- Works with early stopping\n- Training: 3-8 minutes\nTypical accuracy: 58-62%\n\n4.4) HIGH (1000-2000):\n- Maximum capacity\n- Requires early stopping\n- Large datasets only\n- Training: 10-30 minutes\nTypical accuracy: 60-64% (if dataset large enough)\n\n5) TYPICAL RANGE:\n- Minimum: 50\n- DEFAULT: 300 (with early stopping)\n- PRODUCTION: 500-1000 (with early stopping)\n- Maximum: 5000 (rarely beneficial)\n\n6) NOTES:\n- Use early stopping (stops when validation stops improving)\n- Set high (500-1000) + early stopping = safe\n- Without early stopping: keep low (100-300) to avoid overfit\n- Training time linear with epochs\n- Diminishing returns after 500-1000 epochs"
        },
        
        "light_batch": {
          "label": "LightGBM Batch Size:",
          "tooltip": "LightGBM Batch Size - NOT USED (Tree models don't use batches)\n\n1) WHAT IT IS:\nThis parameter is NOT used by LightGBM.\nTree-based models train on full dataset, not mini-batches.\n\nBatch training only for neural networks (diffusion models)\n\n2) WHY IT EXISTS:\n- Legacy parameter\n- Shared UI with neural models\n- Ignored by LightGBM training\n\n3) WHAT TO DO:\n- Ignore this parameter if using LightGBM\n- Only relevant for diffusion models\n- LightGBM trains on all data simultaneously\n\n4) EFFECTS:\n- NO EFFECT on LightGBM training\n- Parameter silently ignored\n\n5) NOTES:\n- Tree models (RF, LightGBM): No batches\n- Neural models (diffusion): Use batches\n- ForexGPT automatically handles this"
        },
        
        "light_val_frac": {
          "label": "LightGBM Validation Fraction:",
          "tooltip": "Validation Fraction - Train/Validation Split Ratio\n\n1) WHAT IT IS:\nFraction of data used for validation (not training).\nRange: 0.05 to 0.5\n\nExample: val_frac=0.2 → 80% training, 20% validation\n\n2) HOW AND WHEN TO USE:\n\nWhen to use SMALL fraction (0.05-0.15):\n- Small dataset (<5K samples)\n- Want maximum training data\n- Less concerned about overfitting\n\nWhen to use LARGE fraction (0.25-0.4):\n- Large dataset (>50K samples)\n- Want reliable validation metrics\n- Overfitting concerns\n\n3) WHY TO USE IT:\n- Estimate generalization performance\n- Early stopping (stop when validation plateaus)\n- Hyperparameter tuning\n- Prevent overfitting\n\n4) EFFECTS:\n\n4.1) VERY SMALL (0.05-0.10):\nBehavior:\n- 90-95% data for training\n- 5-10% for validation\n- Maximum training data\n\nAdvantages:\n- Use almost all data for training\n- Good for small datasets\n\nDisadvantages:\n- Validation metrics unreliable (small sample)\n- May not catch overfitting\n- Early stopping less effective\n\nWhen to use:\n- Small datasets (<5K samples)\n- Every sample precious\n\nTypical results:\n- Training: Maximum data used\n- Validation: Noisy metrics\n\n4.2) SMALL (0.15-0.20):\nBehavior:\n- 80-85% training, 15-20% validation\n- DEFAULT and RECOMMENDED\n- Good balance\n\nAdvantages:\n- Good balance\n- Reliable validation metrics\n- Works for most datasets\n\nDisadvantages:\n- None (well-balanced)\n\nWhen to use:\n- DEFAULT (most users)\n- Datasets 5K-50K samples\n- General-purpose training\n\nTypical results:\n- Training: Good data availability\n- Validation: Reliable metrics\n\n4.3) MEDIUM (0.20-0.30):\nBehavior:\n- 70-80% training, 20-30% validation\n- Conservative split\n- Very reliable validation\n\nAdvantages:\n- Very reliable validation metrics\n- Good overfitting detection\n- Robust early stopping\n\nDisadvantages:\n- Less training data\n- May underfit (if dataset small)\n\nWhen to use:\n- Large datasets (>20K samples)\n- Overfitting concerns\n- Hyperparameter tuning\n\nTypical results:\n- Validation: Very reliable\n- Training: Slightly less data\n\n4.4) LARGE (0.30-0.40):\nBehavior:\n- 60-70% training, 30-40% validation\n- Very conservative\n- Maximum validation reliability\n\nAdvantages:\n- Maximum validation reliability\n- Best overfitting detection\n- Good for hyperparameter search\n\nDisadvantages:\n- SIGNIFICANT training data loss\n- Underfitting risk (small datasets)\n\nWhen to use:\n- Very large datasets (>100K samples)\n- Extensive hyperparameter tuning\n- Research/competitions\n\nTypical results:\n- Validation: Maximum reliability\n- Training: Less data available\n\n5) TYPICAL RANGE / DEFAULT VALUES:\n\n- Minimum: 0.05 (5%)\n- SMALL DATASET: 0.10-0.15\n- DEFAULT: 0.20 (20%, recommended)\n- LARGE DATASET: 0.25-0.30\n- RESEARCH: 0.30-0.40\n- Maximum: 0.50 (50%, rarely used)\n\nRecommendation:\n- <5K samples: 0.10-0.15\n- 5K-50K samples: 0.20 (default)\n- >50K samples: 0.25-0.30\n\n6) ADDITIONAL NOTES / BEST PRACTICES:\n\nEarly stopping:\n- Monitors validation metric\n- Stops when validation stops improving\n- Prevents overfitting automatically\n- val_frac critical for early stopping reliability\n\nStratified split:\n- ForexGPT uses stratified split (if classification)\n- Preserves class distribution\n- Both train/val have similar distributions\n\nTemporal considerations:\n- Time series: use temporal split (not random)\n- Train on earlier data, validate on later\n- Prevents look-ahead bias\n- val_frac = last N% of data\n\nCross-validation:\n- For small datasets: consider k-fold CV\n- k-fold: train on k-1 folds, validate on 1\n- Example: 5-fold = effectively val_frac=0.20\n- More robust than single split\n\nInteraction with dataset size:\n- Large dataset (>50K): Can afford 0.25-0.30\n- Small dataset (<5K): Keep at 0.10-0.15\n- Rule: Validation needs >500 samples minimum\n\nHyperparameter tuning:\n- Tuning: Use higher val_frac (0.25-0.30)\n- Final training: Lower val_frac (0.10-0.15)\n- Or: Train on full data after tuning complete\n\nValidation metrics monitored:\n- LightGBM: Typically log loss or error\n- Early stopping patience: 10-50 rounds\n- Best iteration saved automatically"
        }
      },
      
      "encoder": {
        "patch_len": {
          "label": "Patch Length:",
          "tooltip": "Patch Length - Time Series Segmentation Window\n\n1) WHAT IT IS:\nNumber of consecutive bars grouped into one 'patch' for encoder.\nTime series split into non-overlapping patches of length N.\n\nExample: 100 bars, patch_len=10 → 10 patches of 10 bars each\n\n2) HOW AND WHEN TO USE:\n\nWhen to use SHORT patches (8-16):\n- Capture fine-grained patterns\n- High-frequency data (1m-5m)\n- More patches = more training samples for encoder\n- Better for short-term patterns\n\nWhen to use LONG patches (32-64):\n- Capture longer-term dependencies\n- Lower-frequency data (1h-4h)\n- Fewer patches = faster training\n- Better for trend recognition\n\n3) WHY TO USE IT:\n- Controls encoder granularity\n- Trade-off: local details vs global context\n- Affects encoder training samples\n- Standard in transformer architectures\n\n4) EFFECTS:\n\n4.1) VERY SHORT (4-8):\nBehavior:\n- Very fine-grained patches\n- Many patches (sequence/4 to sequence/8)\n- Encoder sees very local patterns\n\nAdvantages:\n- Maximum local detail\n- Good for noise patterns\n- Many training samples for encoder\n\nDisadvantages:\n- May miss longer trends\n- Encoder training slower (more patches)\n- Each patch has little context\n\nWhen to use:\n- Scalping (1m-5m timeframes)\n- Noise-heavy markets\n- Very large datasets\n\nTypical results:\n- Good for micro-patterns\n- May miss macro trends\n\n4.2) SHORT (10-20):\nBehavior:\n- DEFAULT for most cases\n- Balanced granularity\n- Patch size ~10-20 bars\n\nAdvantages:\n- Good balance local/global\n- Works across timeframes\n- RECOMMENDED starting point\n\nDisadvantages:\n- Not optimized for specific use case\n\nWhen to use:\n- DEFAULT (most users)\n- 15m-1h timeframes\n- General-purpose models\n\nTypical results:\n- Accuracy: Baseline (good balance)\n\n4.3) MEDIUM (24-32):\nBehavior:\n- Longer patches\n- Fewer patches per sequence\n- Each patch has more context\n\nAdvantages:\n- Captures longer patterns\n- Faster encoder training (fewer patches)\n- Good for trend following\n\nDisadvantages:\n- Less local detail\n- Fewer training samples for encoder\n\nWhen to use:\n- Swing trading (4h-1d)\n- Trend-following strategies\n- Smaller datasets\n\nTypical results:\n- Good for trend recognition\n\n4.4) LONG (48-64):\nBehavior:\n- Very long patches\n- Few patches per sequence\n- Maximum context per patch\n\nAdvantages:\n- Best for long-term patterns\n- Very fast encoder training\n- Good for position trading\n\nDisadvantages:\n- Minimal local detail\n- Very few patches (may underfit)\n- Not suitable for short TF\n\nWhen to use:\n- Position trading (daily/weekly)\n- Very long sequences (>500 bars)\n\nTypical results:\n- Good for macro trends\n\n5) TYPICAL RANGE / DEFAULT VALUES:\n\n- Minimum: 8\n- DEFAULT: 16 (recommended)\n- Maximum: 64\n\nRule of thumb:\n- patch_len ≈ horizon (prediction length)\n- Example: horizon 4 → patch_len 8-16\n- Example: horizon 24 → patch_len 24-32\n\nBy timeframe:\n- 1m-5m: 8-16\n- 15m-1h: 16-24\n- 4h-1d: 32-48\n\n6) ADDITIONAL NOTES / BEST PRACTICES:\n\nInteraction with sequence length:\n- Total bars must be divisible by patch_len (approximately)\n- Example: 100 bars / patch_len 16 = 6.25 patches (OK, truncated to 6)\n- Very long patch_len with short sequences = too few patches\n\nEncoder training samples:\n- num_patches = sequence_length / patch_len\n- More patches = more training data for encoder\n- Example: 1000 bars / 10 = 100 patches (good)\n- Example: 1000 bars / 50 = 20 patches (may be too few)\n\nComputational cost:\n- Shorter patches → more patches → slower encoder training\n- Longer patches → fewer patches → faster encoder training\n- Trade-off: quality vs speed\n\nPatching in transformers:\n- Standard technique (Vision Transformers use patches)\n- Reduces sequence length for attention mechanism\n- Makes training feasible on long sequences"
        },
        
        "latent_dim": {
          "label": "Latent Dimension:",
          "tooltip": "Latent Dimension - Encoder Bottleneck Size\n\n1) WHAT IT IS:\nDimensionality of compressed representation (latent space).\nEncoder compresses features from input_dim → latent_dim.\n\nExample: 100 features → latent_dim 32 → compressed to 32 dimensions\n\n2) HOW AND WHEN TO USE:\n\nWhen to use LOW dimension (8-32):\n- Strong compression (lossy)\n- Force encoder to learn only essential patterns\n- Regularization effect (prevent overfitting)\n- Smaller models, faster inference\n\nWhen to use HIGH dimension (64-128):\n- Preserve more information\n- Complex patterns need more dimensions\n- Large datasets can support it\n- Better reconstruction\n\n3) WHY TO USE IT:\n- Controls information bottleneck\n- Trade-off: compression vs information loss\n- Dimensionality reduction for diffusion model\n- Standard autoencoder concept\n\n4) EFFECTS:\n\n4.1) VERY LOW (4-16):\nBehavior:\n- Extreme compression\n- Encoder forced to learn only most important patterns\n- Significant information loss\n\nAdvantages:\n- Maximum regularization\n- Prevents overfitting\n- Very fast diffusion training (low dim)\n- Forces feature selection\n\nDisadvantages:\n- Information loss (may lose important patterns)\n- Underfitting risk\n- Poor reconstruction\n\nWhen to use:\n- Small datasets (<5K samples)\n- Many noisy features\n- Want strong regularization\n\nTypical results:\n- Accuracy: -1 to -2% (information loss)\n- But prevents overfitting\n\n4.2) LOW (16-32):\nBehavior:\n- Good compression\n- Reasonable information preserved\n- DEFAULT for most cases\n\nAdvantages:\n- Good balance compression/information\n- Works for most feature sets\n- Fast diffusion training\n\nDisadvantages:\n- May lose some patterns\n\nWhen to use:\n- DEFAULT (most users)\n- 50-100 input features\n- General-purpose models\n\nTypical results:\n- Accuracy: Baseline (good balance)\n\n4.3) MEDIUM (32-64):\nBehavior:\n- Moderate compression\n- Most information preserved\n- Good for complex patterns\n\nAdvantages:\n- Preserves complex patterns\n- Good reconstruction\n- Suitable for large feature sets\n\nDisadvantages:\n- Slower diffusion training (higher dim)\n- Risk of overfitting (less compression)\n\nWhen to use:\n- Large feature sets (100-200 features)\n- Complex patterns\n- Large datasets (>20K samples)\n\nTypical results:\n- Accuracy: +0.5-1% (more information preserved)\n\n4.4) HIGH (64-128):\nBehavior:\n- Minimal compression\n- Almost all information preserved\n- Very high-dimensional latent space\n\nAdvantages:\n- Maximum information retention\n- Best reconstruction\n- Good for very complex patterns\n\nDisadvantages:\n- Slow diffusion training\n- High risk of overfitting\n- Needs very large datasets\n\nWhen to use:\n- Very large feature sets (200+ features)\n- Very large datasets (>100K samples)\n- Maximum quality required\n\nTypical results:\n- Accuracy: +1-2% (if dataset large enough)\n- Risk overfitting if dataset too small\n\n5) TYPICAL RANGE / DEFAULT VALUES:\n\n- Minimum: 8\n- DEFAULT: 32 (recommended)\n- Maximum: 128\n\nRule of thumb:\n- latent_dim ≈ 1/3 to 1/2 of input features\n- Example: 100 features → latent_dim 32-50\n- Example: 50 features → latent_dim 16-24\n\n6) ADDITIONAL NOTES / BEST PRACTICES:\n\nCompression ratio:\n- Ratio = input_features / latent_dim\n- Example: 90 features / 30 latent = 3:1 compression\n- Typical: 2:1 to 4:1 compression\n- Higher compression = more information loss\n\nInteraction with encoder type:\n- PCA: latent_dim = number of components\n- Autoencoder: latent_dim = bottleneck size\n- VAE: latent_dim = latent space dimension\n- None: parameter ignored (no compression)\n\nDiffusion model impact:\n- Diffusion trains on latent_dim dimensions (not original)\n- Lower latent_dim → faster diffusion training\n- Higher latent_dim → slower but more accurate\n\nInformation preservation:\n- PCA: Check explained variance ratio\n- Autoencoder: Check reconstruction loss\n- Target: 80-95% information preserved"
        },
        
        "encoder_epochs": {
          "label": "Encoder Training Epochs:",
          "tooltip": "Encoder Training Epochs - Autoencoder/VAE Training Iterations\n\n1) WHAT IT IS:\nNumber of epochs to train encoder (autoencoder/VAE).\nNOT used for PCA or 'none' (only neural encoders).\n\nEach epoch = one pass through entire training dataset\n\n2) HOW AND WHEN TO USE:\n\nWhen to use FEW epochs (10-50):\n- Small dataset (<5K samples)\n- Fast training\n- Avoid overfitting\n- Simple patterns\n\nWhen to use MANY epochs (100-300):\n- Large dataset (>20K samples)\n- Complex patterns\n- Have early stopping\n- Production models\n\n3) WHY TO USE IT:\n- Controls encoder training duration\n- More epochs = better compression (up to a point)\n- Too many = overfitting encoder\n- Only affects autoencoder/VAE (not PCA)\n\n4) EFFECTS:\n\n4.1) VERY LOW (5-20):\nBehavior:\n- Encoder undertrained\n- Poor compression/reconstruction\n- Very fast training (seconds)\n\nAdvantages:\n- Fast experimentation\n- Good for prototyping\n\nDisadvantages:\n- Encoder not converged\n- Poor latent representations\n- Downstream model suffers\n\nWhen to use:\n- Quick tests only\n- AVOID for production\n\nTypical results:\n- Accuracy: -2 to -3% (poor encoder)\n\n4.2) LOW (20-50):\nBehavior:\n- Encoder partially trained\n- Reasonable compression\n- DEFAULT for small datasets\n\nAdvantages:\n- Good balance for small data\n- Not too slow\n- Prevents overfitting\n\nDisadvantages:\n- May not fully converge\n\nWhen to use:\n- Small datasets (<5K samples)\n- Fast training needed\n\nTypical results:\n- Accuracy: Baseline (adequate encoder)\n\n4.3) MEDIUM (50-150):\nBehavior:\n- Encoder well-trained\n- Good compression/reconstruction\n- RECOMMENDED for most cases\n\nAdvantages:\n- Good convergence\n- Works for most datasets\n- With early stopping: safe to set high\n\nDisadvantages:\n- Takes time (5-15 minutes)\n\nWhen to use:\n- DEFAULT (most users)\n- Datasets 5K-50K samples\n- Production models\n\nTypical results:\n- Accuracy: Baseline (good encoder)\n\n4.4) HIGH (150-300):\nBehavior:\n- Encoder fully converged\n- Maximum compression quality\n- Long training time\n\nAdvantages:\n- Best possible encoder\n- Complex patterns captured\n- Professional quality\n\nDisadvantages:\n- Slow training (15-30 minutes)\n- Risk overfitting (without early stopping)\n\nWhen to use:\n- Large datasets (>50K samples)\n- Complex patterns\n- Production deployment\n\nTypical results:\n- Accuracy: +0.5-1% (excellent encoder)\n\n5) TYPICAL RANGE / DEFAULT VALUES:\n\n- Minimum: 10\n- DEFAULT: 50 (recommended)\n- PRODUCTION: 100-150 (with early stopping)\n- Maximum: 500 (rarely needed)\n\nRecommendation:\n- Start with 50\n- Use early stopping (patience=10)\n- Monitor reconstruction loss\n\n6) ADDITIONAL NOTES / BEST PRACTICES:\n\nOnly for neural encoders:\n- Autoencoder: YES (needs training)\n- VAE: YES (needs training)\n- PCA: NO (analytical solution, instant)\n- None: NO (no encoder)\n\nEarly stopping:\n- Monitors validation reconstruction loss\n- Stops when no improvement for N epochs\n- Prevents overfitting\n- Can set epochs high (100-200) and let early stopping decide\n\nConvergence monitoring:\n- Watch reconstruction loss\n- Should decrease and plateau\n- If still decreasing at final epoch → increase epochs\n- If flat after few epochs → decrease epochs\n\nTraining time:\n- Depends on: dataset size, latent_dim, encoder architecture\n- Typical: 50 epochs on 10K samples = 2-5 minutes\n- Scales linearly with epochs\n\nInteraction with latent_dim:\n- Lower latent_dim → needs more epochs (harder compression)\n- Higher latent_dim → needs fewer epochs (easier compression)"
        }
      },
      
      "diffusion": {
        "diffusion_timesteps": {
          "label": "Diffusion Timesteps:",
          "tooltip": "Diffusion Timesteps - Noise Schedule Length\n\n1) WHAT IT IS:\nNumber of steps in diffusion noise schedule (T).\nForward process: Add noise over T steps.\nReverse process: Denoise over T steps.\n\nHigher T = smoother schedule, better quality, slower inference\n\n2) HOW AND WHEN TO USE:\n\nWhen to use FEW timesteps (50-200):\n- Fast inference (fewer denoising steps)\n- Prototyping/experimentation\n- Real-time constraints\n- DDIM sampler (works well with fewer steps)\n\nWhen to use MANY timesteps (500-1000):\n- Best quality\n- No inference time constraints\n- DDPM sampler (needs many steps)\n- Production models where quality > speed\n\n3) WHY TO USE IT:\n- Controls noise schedule granularity\n- Trade-off: quality vs inference speed\n- More steps = smoother diffusion = better samples\n- Standard DDPM uses 1000, DDIM can use 50\n\n4) EFFECTS:\n\n4.1) VERY LOW (10-50):\nBehavior:\n- Very fast inference (10-50 denoising steps)\n- Coarse noise schedule\n- Only viable with DDIM\n\nAdvantages:\n- Ultra-fast predictions (<1 second)\n- Good for real-time trading\n- DDIM handles it reasonably\n\nDisadvantages:\n- Lower quality samples\n- More sampling variance\n- Not suitable for DDPM\n\nWhen to use:\n- ONLY with DDIM sampler\n- Real-time inference needed\n- Prototyping\n\nTypical results:\n- Accuracy: -1 to -2% vs high timesteps\n- Inference: 0.5-2 seconds\n\n4.2) LOW (50-200):\nBehavior:\n- Fast inference (50-200 steps)\n- Reasonable noise schedule\n- DEFAULT for DDIM\n\nAdvantages:\n- Good balance quality/speed\n- Works well with DDIM\n- Practical for production\n\nDisadvantages:\n- Still lower quality than 1000\n\nWhen to use:\n- DDIM sampler\n- Production (speed matters)\n- Most trading scenarios\n\nTypical results:\n- Accuracy: -0.5 to -1% vs 1000 timesteps\n- Inference: 2-8 seconds\n\n4.3) MEDIUM (200-500):\nBehavior:\n- Moderate inference time\n- Good noise schedule\n- Works with DDPM and DDIM\n\nAdvantages:\n- High quality\n- Acceptable speed\n- Suitable for both samplers\n\nDisadvantages:\n- Slower inference than 50-200\n\nWhen to use:\n- Balanced quality/speed\n- Either sampler\n\nTypical results:\n- Accuracy: Near-optimal\n- Inference: 8-20 seconds\n\n4.4) HIGH (500-1000):\nBehavior:\n- Slow inference (500-1000 steps)\n- Very smooth noise schedule\n- DDPM standard\n\nAdvantages:\n- Best quality (original DDPM)\n- Most stable samples\n- Lowest sampling variance\n\nDisadvantages:\n- SLOW inference (20-60 seconds)\n- Not suitable for real-time\n\nWhen to use:\n- DDPM sampler\n- Quality critical\n- No time constraints (backtesting)\n\nTypical results:\n- Accuracy: Optimal\n- Inference: 20-60 seconds\n\n5) TYPICAL RANGE / DEFAULT VALUES:\n\n- Minimum: 10\n- DDIM DEFAULT: 100 (fast, good quality)\n- DDPM DEFAULT: 1000 (original specification)\n- Maximum: 4000 (overkill, no benefit)\n\nRecommendation by sampler:\n- DDIM: 50-200 (fast)\n- DDPM: 500-1000 (quality)\n- SSSD: 500 (structured)\n\n6) ADDITIONAL NOTES / BEST PRACTICES:\n\nSampler compatibility:\n- DDPM: Needs 500-1000 (designed for many steps)\n- DDIM: Works well with 50-200 (skip steps)\n- SSSD: 200-500 (structured diffusion)\n\nInference time scaling:\n- Inference time ≈ linear with timesteps\n- 100 steps: ~4 seconds\n- 1000 steps: ~40 seconds\n- Critical for live trading\n\nTraining vs inference:\n- Training always uses full T timesteps\n- DDIM inference can skip (e.g., use every 10th step)\n- DDPM inference must do all steps\n\nQuality vs speed trade-off:\n- Backtesting: Use 1000 (quality matters)\n- Live trading: Use 50-200 (speed matters)\n- Model evaluation: Use 1000 (fair comparison)"
        },
        
        "learning_rate": {
          "label": "Learning Rate:",
          "tooltip": "Learning Rate - Neural Network Step Size\n\n1) WHAT IT IS:\nStep size for gradient descent optimization.\nControls how much to adjust weights each iteration.\n\nLR too high → unstable training (diverge)\nLR too low → slow training (never converge)\n\n2) HOW AND WHEN TO USE:\n\nWhen to use HIGH learning rate (1e-3 to 1e-2):\n- Small models\n- Large batch sizes\n- Coarse initial search\n- Adam optimizer (adapts LR)\n\nWhen to use LOW learning rate (1e-5 to 1e-4):\n- Large models (transformers)\n- Small batch sizes\n- Fine-tuning\n- Stable training\n\n3) WHY TO USE IT:\n- Critical hyperparameter (most important!)\n- Controls training speed and stability\n- Wrong LR = training fails completely\n- Needs tuning for each model/dataset\n\n4) EFFECTS:\n\n4.1) VERY LOW (1e-6 to 1e-5):\nBehavior:\n- Very slow training\n- Stable but sluggish\n- May not converge in reasonable time\n\nAdvantages:\n- Very stable (no divergence)\n- Good for fine-tuning\n\nDisadvantages:\n- TOO SLOW (hundreds of epochs needed)\n- May get stuck in local minima\n- Wastes time\n\nWhen to use:\n- Fine-tuning pretrained models\n- Very large models\n- Last resort if higher LR diverges\n\nTypical results:\n- Training: Very slow convergence\n- May not finish in reasonable time\n\n4.2) LOW (1e-4 to 5e-4):\nBehavior:\n- Slow but steady training\n- Stable convergence\n- DEFAULT for diffusion models\n\nAdvantages:\n- Stable (rarely diverges)\n- Works for most architectures\n- RECOMMENDED starting point\n\nDisadvantages:\n- Slower than optimal\n\nWhen to use:\n- DEFAULT (most users)\n- Diffusion models (standard 1e-4)\n- Transformers\n- Production training\n\nTypical results:\n- Training: Steady convergence\n- Reliable results\n\n4.3) MEDIUM (5e-4 to 1e-3):\nBehavior:\n- Fast training\n- Good balance speed/stability\n- Works with Adam\n\nAdvantages:\n- Faster convergence\n- Still relatively stable\n- Good for experimentation\n\nDisadvantages:\n- May be unstable on some models\n- May overshoot optimal\n\nWhen to use:\n- Small-medium models\n- Large batch sizes\n- Fast prototyping\n\nTypical results:\n- Training: Fast convergence\n- May need monitoring\n\n4.4) HIGH (1e-3 to 1e-2):\nBehavior:\n- Very fast training (when works)\n- HIGH RISK of divergence\n- Oscillations common\n\nAdvantages:\n- Very fast convergence (if stable)\n- Good for small models\n\nDisadvantages:\n- Often diverges (loss → NaN)\n- Unstable training\n- Not suitable for large models\n\nWhen to use:\n- Small models only\n- Very large batch sizes\n- Only if 1e-3 proven stable\n\nTypical results:\n- High risk: Training diverges\n- If works: Very fast convergence\n\n5) TYPICAL RANGE / DEFAULT VALUES:\n\n- Minimum: 1e-6\n- DIFFUSION DEFAULT: 1e-4 (recommended)\n- TRANSFORMER DEFAULT: 5e-5\n- CNN DEFAULT: 1e-3\n- Maximum: 1e-2\n\nRecommendation:\n- Start with 1e-4 (safest)\n- If training too slow: Try 5e-4\n- If diverges: Lower to 5e-5\n\n6) ADDITIONAL NOTES / BEST PRACTICES:\n\nLearning rate scheduling:\n- Warmup: Start low, increase linearly\n- Cosine decay: Decrease over training\n- Step decay: Drop at specific epochs\n- ReduceOnPlateau: Lower when loss plateaus\n\nOptimizer interaction:\n- Adam: Adapts LR per parameter (forgiving)\n- SGD: Fixed LR (needs careful tuning)\n- AdamW: Adam + weight decay (recommended)\n\nSigns LR too high:\n- Loss diverges (goes to infinity)\n- Loss oscillates wildly\n- NaN values in loss\n- Training unstable\n\nSigns LR too low:\n- Loss decreases very slowly\n- Needs 100s of epochs\n- Gets stuck early\n- No progress after many epochs\n\nBatch size interaction:\n- Large batch → can use higher LR\n- Small batch → need lower LR\n- Rule: LR ∝ sqrt(batch_size)"
        },
        
        "batch_size_dl": {
          "label": "Diffusion Batch Size:",
          "tooltip": "Batch Size - Samples per Training Step\n\n1) WHAT IT IS:\nNumber of samples processed together per gradient update.\nLarger batch = more stable gradients, more GPU memory.\n\nExample: batch 32 → process 32 samples, then update weights\n\n2) HOW AND WHEN TO USE:\n\nWhen to use SMALL batch (8-32):\n- Limited GPU memory (4-8GB)\n- Small dataset (<5K samples)\n- Want more gradient updates\n- Regularization effect\n\nWhen to use LARGE batch (128-512):\n- Large GPU memory (16GB+)\n- Large dataset (>50K samples)\n- Faster training (fewer updates)\n- Stable gradients\n\n3) WHY TO USE IT:\n- Trade-off: speed vs memory vs generalization\n- Larger batch = faster epoch, more memory\n- Smaller batch = slower epoch, less memory, better generalization\n- GPU utilization (too small = inefficient)\n\n4) EFFECTS:\n\n4.1) VERY SMALL (4-16):\nBehavior:\n- Many gradient updates per epoch\n- Noisy gradients\n- Slow epoch time\n\nAdvantages:\n- Minimal GPU memory\n- Works on any GPU\n- Regularization (noise helps generalization)\n\nDisadvantages:\n- SLOW training (many small updates)\n- GPU underutilized\n- Inefficient\n\nWhen to use:\n- 4GB GPU\n- Small datasets\n\nTypical results:\n- Training: Very slow\n- Generalization: Good (noise helps)\n\n4.2) SMALL (16-64):\nBehavior:\n- DEFAULT for most GPUs\n- Good balance\n- Reasonable GPU utilization\n\nAdvantages:\n- Works on 8GB GPU\n- Good generalization\n- Reasonable speed\n\nDisadvantages:\n- Not maximum speed\n\nWhen to use:\n- DEFAULT (most users)\n- 8GB GPU (batch 32-64)\n- General-purpose training\n\nTypical results:\n- Training: Reasonable speed\n- Generalization: Good\n\n4.3) MEDIUM (64-256):\nBehavior:\n- Faster training\n- Stable gradients\n- Good GPU utilization\n\nAdvantages:\n- Fast epochs\n- Stable training\n- Efficient GPU usage\n\nDisadvantages:\n- Needs 16GB+ GPU\n- May generalize worse (too stable)\n\nWhen to use:\n- 16GB+ GPU\n- Large datasets\n- Fast training needed\n\nTypical results:\n- Training: Fast\n- Generalization: Good (if dataset large)\n\n4.4) LARGE (256-1024):\nBehavior:\n- Very fast epochs\n- Very stable gradients\n- Maximum GPU utilization\n\nAdvantages:\n- Fastest training\n- Most stable\n\nDisadvantages:\n- Needs 24GB+ GPU\n- Overfitting risk (too stable)\n- May need learning rate adjustment\n\nWhen to use:\n- 24GB+ GPU (A6000, A100)\n- Very large datasets (>100K)\n- Institutional training\n\nTypical results:\n- Training: Very fast\n- Generalization: OK (if dataset huge)\n\n5) TYPICAL RANGE / DEFAULT VALUES:\n\n- Minimum: 8\n- 8GB GPU: 32-64\n- 16GB GPU: 64-128\n- 24GB GPU: 128-256\n- Maximum: 1024\n\nRecommendation:\n- Start with 32 (safe)\n- Increase until GPU memory full\n- Monitor GPU utilization\n\n6) ADDITIONAL NOTES / BEST PRACTICES:\n\nGPU memory estimation:\n- Diffusion model: ~2-4GB base\n- Per sample: ~10-50MB\n- Batch 32: ~2-6GB total\n- Batch 128: ~8-12GB total\n\nGradient accumulation:\n- Simulate large batch on small GPU\n- Accumulate gradients over N mini-batches\n- Example: batch 32 × accumulate 4 = effective batch 128\n- Same result, less memory\n\nBatch size vs learning rate:\n- Larger batch → increase LR (rule: LR ∝ sqrt(batch))\n- Example: batch 32, LR 1e-4 → batch 128, LR 2e-4\n\nSpeed vs generalization:\n- Small batch: Slower, better generalization\n- Large batch: Faster, may overfit\n- For trading: Generalization critical!"
        },
        
        "model_channels": {
          "label": "Model Channels:",
          "tooltip": "Model Channels - Diffusion U-Net Base Width\n\n1) WHAT IT IS:\nBase number of channels in diffusion U-Net architecture.\nModel size scales with channels: 64, 128, 192, 256, etc.\n\nMore channels = larger model = more capacity\n\n2) HOW AND WHEN TO USE:\n\nWhen to use FEW channels (64-128):\n- Small datasets (<10K samples)\n- Fast training/inference\n- Limited GPU memory\n- Simple patterns\n\nWhen to use MANY channels (192-256):\n- Large datasets (>50K samples)\n- Complex patterns\n- Sufficient GPU memory (16GB+)\n- Production quality\n\n3) WHY TO USE IT:\n- Controls model capacity\n- Trade-off: capacity vs speed vs overfitting\n- More channels = more parameters = more memory\n- Standard U-Net scaling parameter\n\n4) EFFECTS:\n\n4.1) VERY LOW (32-64):\nBehavior:\n- Very small model (~1-5M parameters)\n- Fast training and inference\n- Minimal capacity\n\nAdvantages:\n- Very fast (seconds per epoch)\n- Works on 4GB GPU\n- No overfitting risk\n\nDisadvantages:\n- UNDERFITTING (too simple)\n- Can't capture complex patterns\n- Poor accuracy\n\nWhen to use:\n- Prototyping only\n- AVOID for production\n\nTypical results:\n- Accuracy: 52-56% (too weak)\n- Training: Very fast but poor results\n\n4.2) LOW (96-128):\nBehavior:\n- Small model (~10-20M parameters)\n- Good balance for small datasets\n- DEFAULT for most users\n\nAdvantages:\n- Fast training (reasonable)\n- Works on 8GB GPU\n- Good for <20K samples\n\nDisadvantages:\n- May underfit on complex data\n\nWhen to use:\n- DEFAULT (most users)\n- Small-medium datasets\n- 8GB GPU\n\nTypical results:\n- Accuracy: 58-60% (adequate)\n- Training: Fast\n\n4.3) MEDIUM (128-192):\nBehavior:\n- Medium model (~30-50M parameters)\n- Good capacity\n- RECOMMENDED for production\n\nAdvantages:\n- Good capacity for complex patterns\n- Still reasonable speed\n- Works on 16GB GPU\n\nDisadvantages:\n- Slower than 128 channels\n- Needs 16GB GPU\n\nWhen to use:\n- RECOMMENDED (production)\n- Large datasets (>20K samples)\n- 16GB+ GPU available\n\nTypical results:\n- Accuracy: 60-62% (good)\n- Training: Moderate speed\n\n4.4) HIGH (192-256):\nBehavior:\n- Large model (~60-100M parameters)\n- Maximum capacity\n- Professional/institutional\n\nAdvantages:\n- Best capacity\n- Can capture very complex patterns\n- State-of-the-art quality\n\nDisadvantages:\n- SLOW training/inference\n- Needs 24GB+ GPU\n- Overfitting risk (small datasets)\n\nWhen to use:\n- Very large datasets (>100K samples)\n- 24GB+ GPU (A6000, A100)\n- Maximum quality required\n\nTypical results:\n- Accuracy: 62-64% (if dataset large enough)\n- Training: Slow\n\n5) TYPICAL RANGE / DEFAULT VALUES:\n\n- Minimum: 64\n- DEFAULT: 128 (recommended)\n- PRODUCTION: 192\n- INSTITUTIONAL: 256\n- Maximum: 512 (rarely used)\n\nRecommendation:\n- 8GB GPU: 128 channels\n- 16GB GPU: 192 channels\n- 24GB+ GPU: 256 channels\n\n6) ADDITIONAL NOTES / BEST PRACTICES:\n\nParameter count estimation:\n- 64 channels: ~5M parameters\n- 128 channels: ~20M parameters\n- 192 channels: ~45M parameters\n- 256 channels: ~80M parameters\n\nGPU memory usage:\n- Scales quadratically with channels\n- 128 channels, batch 32: ~4-6GB\n- 192 channels, batch 32: ~8-10GB\n- 256 channels, batch 32: ~14-16GB\n\nTraining time:\n- Scales quadratically with channels\n- 128 channels: 1× baseline\n- 192 channels: ~2× slower\n- 256 channels: ~4× slower\n\nU-Net architecture:\n- Channels multiply at each downsample\n- Example: 128 base → 128, 256, 512, 1024 in layers\n- Deep layers have most parameters"
        },
        
        "dropout": {
          "label": "Dropout Rate:",
          "tooltip": "Dropout - Regularization by Random Neuron Dropping\n\n1) WHAT IT IS:\nProbability of dropping neurons during training.\nRange: 0.0 (no dropout) to 0.5 (drop 50% neurons).\n\nRandomly zero out neurons → prevent overfitting\n\n2) HOW AND WHEN TO USE:\n\nWhen to use NO dropout (0.0):\n- Very large datasets (>100K samples)\n- Model underfitting\n- Simple patterns\n\nWhen to use HIGH dropout (0.2-0.4):\n- Small datasets (<10K samples)\n- Model overfitting\n- Strong regularization needed\n\n3) WHY TO USE IT:\n- Prevent overfitting (co-adaptation)\n- Force redundancy in network\n- Improve generalization\n- Standard regularization technique\n\n4) EFFECTS:\n\n4.1) NONE (0.0):\nBehavior:\n- No regularization\n- All neurons always active\n- Maximum model capacity\n\nAdvantages:\n- Full model capacity\n- Faster convergence\n- Good for large datasets\n\nDisadvantages:\n- OVERFITTING risk (small datasets)\n- May memorize training data\n- Poor generalization\n\nWhen to use:\n- Very large datasets (>100K)\n- Model underfitting\n\nTypical results:\n- Training accuracy: High\n- Test accuracy: May be much lower (overfitting)\n\n4.2) LOW (0.05-0.15):\nBehavior:\n- Mild regularization\n- Most neurons active\n- Slight capacity reduction\n\nAdvantages:\n- Small regularization effect\n- Still fast convergence\n- Good for medium datasets\n\nDisadvantages:\n- May not prevent overfitting enough\n\nWhen to use:\n- Medium datasets (20K-50K)\n- Mild overfitting signs\n\nTypical results:\n- Training: Fast convergence\n- Generalization: Slightly better\n\n4.3) MEDIUM (0.1-0.2):\nBehavior:\n- DEFAULT and RECOMMENDED\n- Good regularization\n- Balanced capacity\n\nAdvantages:\n- Good overfitting prevention\n- Works for most datasets\n- Standard practice\n\nDisadvantages:\n- Slightly slower convergence\n\nWhen to use:\n- DEFAULT (most users)\n- Datasets 5K-50K samples\n- General-purpose training\n\nTypical results:\n- Generalization: Good improvement\n- Training: Slightly slower but more stable\n\n4.4) HIGH (0.3-0.5):\nBehavior:\n- Strong regularization\n- Many neurons dropped\n- Reduced capacity\n\nAdvantages:\n- Strong overfitting prevention\n- Forces robust features\n- Good for very small datasets\n\nDisadvantages:\n- SLOW convergence\n- May underfit (too much regularization)\n- Needs more epochs\n\nWhen to use:\n- Small datasets (<5K samples)\n- Strong overfitting observed\n\nTypical results:\n- Training: Slow convergence\n- Generalization: Very good (if not too high)\n\n5) TYPICAL RANGE / DEFAULT VALUES:\n\n- Minimum: 0.0 (no dropout)\n- DEFAULT: 0.1 (recommended)\n- HIGH: 0.2-0.3\n- Maximum: 0.5 (very aggressive)\n\nRecommendation:\n- Start with 0.1\n- If overfitting: Increase to 0.2-0.3\n- If underfitting: Decrease to 0.05 or 0.0\n\n6) ADDITIONAL NOTES / BEST PRACTICES:\n\nSigns of overfitting (increase dropout):\n- Training accuracy >> test accuracy\n- Training loss much lower than validation loss\n- Model memorizes training patterns\n\nSigns of underfitting (decrease dropout):\n- Training and test accuracy both low\n- Loss not decreasing much\n- Model too simple for data\n\nDropout at inference:\n- Disabled at inference (all neurons active)\n- Weights scaled to compensate\n- PyTorch handles this automatically\n\nOther regularization:\n- Weight decay (L2 regularization)\n- Data augmentation\n- Early stopping\n- Use together with dropout for best results"
        },
        
        "num_heads": {
          "label": "Attention Heads:",
          "tooltip": "Number of Attention Heads - Multi-Head Attention Parallelism\n\n1) WHAT IT IS:\nNumber of parallel attention mechanisms in transformer layers.\nEach head learns different patterns independently.\n\nMore heads = more diverse attention patterns\n\n2) HOW AND WHEN TO USE:\n\nWhen to use FEW heads (2-4):\n- Small models (64-128 channels)\n- Fast training\n- Simple patterns\n\nWhen to use MANY heads (8-16):\n- Large models (192-256 channels)\n- Complex patterns\n- Multi-scale features\n\n3) WHY TO USE IT:\n- Parallel attention patterns\n- Each head captures different relationships\n- Standard transformer design\n- Must divide model_channels evenly\n\n4) EFFECTS:\n\n4.1) FEW (2-4):\nBehavior:\n- Few parallel attention patterns\n- Each head has more dimensions\n- Simpler attention\n\nAdvantages:\n- Faster training\n- Works with small models\n- Less complexity\n\nDisadvantages:\n- Less diverse attention\n- May miss some patterns\n\nWhen to use:\n- Small models (64-128 channels)\n- Fast prototyping\n\nTypical results:\n- Speed: Fast\n- Capacity: Limited attention diversity\n\n4.2) MEDIUM (4-8):\nBehavior:\n- DEFAULT and RECOMMENDED\n- Good attention diversity\n- Balanced head dimensions\n\nAdvantages:\n- Good balance\n- Works for most models\n- Standard practice\n\nDisadvantages:\n- None (well-balanced)\n\nWhen to use:\n- DEFAULT (most users)\n- Models 128-192 channels\n\nTypical results:\n- Good attention patterns\n- Reasonable speed\n\n4.3) MANY (8-16):\nBehavior:\n- Many parallel attention patterns\n- Each head has fewer dimensions\n- Rich attention\n\nAdvantages:\n- Maximum attention diversity\n- Captures many pattern types\n- Good for complex data\n\nDisadvantages:\n- Slower (more heads to compute)\n- Needs large model (256+ channels)\n\nWhen to use:\n- Large models (256+ channels)\n- Complex patterns\n- Production quality\n\nTypical results:\n- Best attention patterns\n- Slower training\n\n5) TYPICAL RANGE / DEFAULT VALUES:\n\n- Minimum: 2\n- DEFAULT: 4 (for 128 channels)\n- DEFAULT: 8 (for 256 channels)\n- Maximum: 16\n\nIMPORTANT CONSTRAINT:\n- model_channels must be divisible by num_heads\n- Example: 128 channels → heads can be 2, 4, 8\n- Example: 192 channels → heads can be 2, 3, 4, 6, 8, 12\n\nRecommendation:\n- 128 channels → 4 heads\n- 192 channels → 6 or 8 heads\n- 256 channels → 8 heads\n\n6) ADDITIONAL NOTES / BEST PRACTICES:\n\nHead dimension calculation:\n- head_dim = model_channels / num_heads\n- Example: 128 channels, 4 heads → 32 dim per head\n- Typical head_dim: 32-64 (sweet spot)\n\nAttention mechanism:\n- Each head attends to different patterns\n- Head 1: Local patterns\n- Head 2: Long-term dependencies\n- Head 3: Cyclical patterns\n- Etc. (learned automatically)\n\nComputational cost:\n- Scales linearly with num_heads\n- 8 heads ≈ 2× slower than 4 heads\n\nCommon configurations:\n- Small: 64 channels, 2 heads (32 dim/head)\n- Medium: 128 channels, 4 heads (32 dim/head)\n- Large: 192 channels, 6 heads (32 dim/head)\n- XLarge: 256 channels, 8 heads (32 dim/head)"
        }
      },
      
      "nvidia": {
        "enable": {
          "label": "Enable NVIDIA Stack:",
          "tooltip": "NVIDIA Optimization Stack - GPU Acceleration Suite\n\n1) WHAT IT IS:\nEnables NVIDIA optimizations: AMP, TensorCores, Flash Attention, etc.\nRequires NVIDIA GPU (RTX/Tesla) and proper drivers.\n\nMassive speedup (2-5×) for diffusion models\n\n2) HOW AND WHEN TO USE:\n\nWhen to ENABLE:\n- Have NVIDIA GPU (RTX 3060+, Tesla T4+)\n- Diffusion models (DDPM/DDIM/SSSD)\n- Want maximum speed\n- Production training\n\nWhen to DISABLE:\n- AMD/Intel GPU\n- CPU training\n- Compatibility issues\n- Debugging\n\n3) WHY TO USE IT:\n- 2-5× faster training (huge!)\n- TensorCore acceleration\n- Mixed precision (FP16/BF16)\n- Flash Attention 2 (memory efficient)\n- Standard practice for GPU training\n\n4) EFFECTS:\n\n4.1) DISABLED:\nBehavior:\n- Standard PyTorch (FP32)\n- No special optimizations\n- Slower but compatible\n\nAdvantages:\n- Works on any hardware\n- More stable (FP32 precision)\n- Easier debugging\n\nDisadvantages:\n- SLOW (2-5× slower than enabled)\n- Higher GPU memory usage\n- Underutilizes modern GPUs\n\nWhen to use:\n- Non-NVIDIA GPU\n- Compatibility issues\n\nTypical results:\n- Training: 100% baseline (slow)\n- Memory: 100% baseline\n\n4.2) ENABLED:\nBehavior:\n- Mixed precision (FP16/BF16)\n- TensorCore acceleration\n- Flash Attention\n- Optimized kernels\n\nAdvantages:\n- 2-5× FASTER training (massive!)\n- 40-50% less GPU memory\n- TensorCores fully utilized\n- Industry standard\n\nDisadvantages:\n- Requires NVIDIA GPU (RTX/Tesla)\n- May have rare numerical issues (very rare)\n\nWhen to use:\n- ALWAYS (if have NVIDIA GPU)\n- Diffusion models\n- Production training\n\nTypical results:\n- Training: 2-5× faster (huge speedup!)\n- Memory: 40-50% reduction\n- Accuracy: Same (no loss)\n\n5) TYPICAL RANGE / DEFAULT VALUES:\n\n- DEFAULT: ENABLED (if NVIDIA GPU detected)\n- Disable only if compatibility issues\n\nRequirements:\n- NVIDIA GPU: RTX 2060+, Tesla T4+\n- CUDA: 11.7+\n- PyTorch: 2.0+\n- GPU Compute Capability: 7.0+ (Volta+)\n\nRecommendation:\n- Enable if have NVIDIA GPU (always)\n- Disable only for debugging\n\n6) ADDITIONAL NOTES / BEST PRACTICES:\n\nWhat gets enabled:\n- Automatic Mixed Precision (AMP)\n- TensorCore operations (FP16/BF16)\n- Flash Attention 2 (memory efficient attention)\n- Fused optimizers (faster weight updates)\n- torch.compile (JIT optimization)\n\nSpeedup by GPU:\n- RTX 3060/3070: 2-3× faster\n- RTX 3080/3090: 3-4× faster\n- RTX 4090: 4-5× faster\n- A100/H100: 5-8× faster (with BF16)\n\nMemory savings:\n- FP16 uses 50% memory vs FP32\n- Flash Attention: Additional 30-40% savings\n- Total: Can train 2× larger models\n\nNumerical stability:\n- Modern NVIDIA stack very stable\n- Rare issues with extreme values\n- Loss scaling handles underflow automatically\n\nCompatibility check:\n- Run: nvidia-smi (check GPU)\n- Run: python -c 'import torch; print(torch.cuda.is_available())'\n- Run: python -c 'import torch; print(torch.cuda.get_device_capability())'\n- Need: (7, 0) or higher for TensorCores"
        },
        
        "use_amp": {
          "label": "Use Automatic Mixed Precision:",
          "tooltip": "AMP - Automatic Mixed Precision Training\n\n1) WHAT IT IS:\nUses FP16 (half precision) for speed, FP32 for stability.\nAutomatic loss scaling prevents underflow.\n\n~2× faster, 50% less memory\n\n2) HOW AND WHEN TO USE:\n\nWhen to ENABLE:\n- NVIDIA GPU (RTX/Tesla)\n- Diffusion models\n- Want speed and memory savings\n- Production training (always)\n\nWhen to DISABLE:\n- Debugging numerical issues (rare)\n- Very sensitive models (very rare)\n\n3) WHY TO USE IT:\n- 2× faster training\n- 50% less GPU memory\n- TensorCore acceleration\n- Industry standard (everyone uses it)\n\n4) EFFECTS:\n\n4.1) DISABLED (FP32):\n- Full precision (FP32)\n- Slower, more memory\n- Maximum numerical stability\n\nTypical results:\n- Speed: Baseline (100%)\n- Memory: Baseline (100%)\n\n4.2) ENABLED (FP16):\n- Mixed precision (FP16 compute, FP32 accumulate)\n- TensorCores active\n- 2× faster, 50% less memory\n\nTypical results:\n- Speed: 2× faster!\n- Memory: 50% reduction!\n- Accuracy: Same (no loss)\n\n5) TYPICAL RANGE:\n\n- DEFAULT: ENABLED (always, if NVIDIA GPU)\n- Disable only for debugging\n\nRequirements:\n- NVIDIA GPU (Volta+, RTX 2060+)\n- CUDA 11.7+\n\n6) NOTES:\n\n- Automatic loss scaling (prevents underflow)\n- No accuracy loss\n- Standard in all modern training\n- Enable always unless debugging"
        },
        
        "compile_model": {
          "label": "Compile Model (torch.compile):",
          "tooltip": "torch.compile - PyTorch 2.0 JIT Compilation\n\n1) WHAT IT IS:\nPyTorch 2.0 JIT (Just-In-Time) compilation.\nCompiles model to optimized kernels before training.\n\nAdditional 20-40% speedup on top of AMP\n\n2) HOW AND WHEN TO USE:\n\nWhen to ENABLE:\n- PyTorch 2.0+ installed\n- Production training (large runs)\n- Want maximum speed\n- Model architecture stable\n\nWhen to DISABLE:\n- PyTorch <2.0\n- Debugging (compilation hides errors)\n- Prototyping (frequent model changes)\n- First-time runs (compilation takes time)\n\n3) WHY TO USE IT:\n- 20-40% additional speedup\n- Fuses operations (fewer kernel launches)\n- Memory access optimization\n- Graph-level optimizations\n\n4) EFFECTS:\n\n4.1) DISABLED:\nBehavior:\n- Standard eager execution\n- No compilation overhead\n- Easy debugging\n\nAdvantages:\n- Fast iteration (no compilation)\n- Easy debugging (clear stack traces)\n- Works with any PyTorch version\n\nDisadvantages:\n- Slower training (20-40%)\n- Not utilizing full optimization\n\nWhen to use:\n- PyTorch <2.0\n- Prototyping\n- Debugging issues\n\nTypical results:\n- Speed: Baseline (100%)\n- First run: Instant start\n\n4.2) ENABLED:\nBehavior:\n- JIT compilation on first run\n- Compiled kernels cached\n- Graph optimizations applied\n\nAdvantages:\n- 20-40% FASTER training\n- Optimized kernel fusion\n- Memory access patterns optimized\n\nDisadvantages:\n- First run slower (compilation time 1-5 min)\n- Obscure error messages (if model issues)\n- Requires PyTorch 2.0+\n\nWhen to use:\n- Production training (large runs)\n- PyTorch 2.0+\n- Model finalized\n\nTypical results:\n- Speed: 20-40% faster (after compilation)\n- First run: 1-5 min compilation overhead\n- Subsequent runs: Full speedup\n\n5) TYPICAL RANGE / DEFAULT VALUES:\n\n- DEFAULT: DISABLED (safer for prototyping)\n- PRODUCTION: ENABLED (maximum speed)\n\nRequirements:\n- PyTorch 2.0+ required\n- CUDA 11.7+ recommended\n- Stable model architecture\n\nRecommendation:\n- Prototyping: Disabled\n- Production: Enabled\n- First run: Disabled (test model works)\n- Large training runs: Enabled\n\n6) ADDITIONAL NOTES / BEST PRACTICES:\n\nCompilation time:\n- First run: 1-5 minutes compilation overhead\n- Subsequent runs: No overhead (cached)\n- Worth it for runs >30 minutes\n\nSpeedup by model:\n- Transformer models: 30-40% faster\n- U-Net diffusion: 20-30% faster\n- Simple models: 10-20% faster\n- Larger models = more speedup\n\nCombined with AMP:\n- AMP alone: 2× speedup\n- AMP + compile: 2.4-2.8× speedup\n- Multiplicative effect\n\nDebugging tips:\n- If errors after enabling: Disable compile first\n- Compilation errors often obscure\n- Test model works without compile first\n- Then enable for production runs\n\nBackends:\n- Default: inductor (recommended)\n- Alternative: aot_eager (debugging)\n- Alternative: cudagraphs (max speed, experimental)\n\nCompatibility:\n- Works with most models\n- Some dynamic models may fail\n- Custom CUDA kernels may not compile\n- Test before production\n\nWhen to avoid:\n- Model changes frequently\n- Debugging model issues\n- PyTorch version <2.0\n- Very short training runs (<10 min)\n\nBest practice workflow:\n1. Prototype: compile=False (fast iteration)\n2. Test: compile=False (verify model works)\n3. Production: compile=True (maximum speed)\n4. If errors: compile=False (debug), then re-enable"
        },
        
        "precision": {
          "label": "Training Precision:",
          "tooltip": "Training Precision - Floating Point Format Selection\n\n1) WHAT IT IS:\nNumerical precision used for model weights and computations.\n- FP32 (Float32): Standard precision (32-bit)\n- FP16 (Float16): Half precision (16-bit)\n- BF16 (BFloat16): Brain Float (16-bit, better range than FP16)\n\nLower precision = faster, less memory\n\n2) HOW AND WHEN TO USE:\n\nFP32 (Full Precision):\n- Maximum numerical stability\n- Debugging\n- CPU training\n- Small models (speed not critical)\n\nFP16 (Half Precision):\n- 2× faster than FP32\n- 50% less memory\n- NVIDIA Volta+ GPUs (RTX 2000+)\n- Most common choice\n\nBF16 (Brain Float):\n- Best of both worlds\n- Same speed as FP16\n- Better numerical stability than FP16\n- NVIDIA Ampere+ GPUs (RTX 3000+, A100)\n- RECOMMENDED if available\n\n3) WHY IT MATTERS:\n- Training speed: FP16/BF16 = 2× faster\n- Memory usage: FP16/BF16 = 50% less\n- Model size: Can train 2× larger models\n- Numerical stability: BF16 > FP32 > FP16\n\n4) EFFECTS:\n\n4.1) FP32 (Full Precision):\nCharacteristics:\n- Standard PyTorch default\n- Maximum precision\n- Baseline speed and memory\n\nAdvantages:\n- Most stable (no numerical issues)\n- Works everywhere (CPU, any GPU)\n- Easy debugging\n\nDisadvantages:\n- SLOW (2× slower than FP16/BF16)\n- HIGH memory usage (2× more than mixed)\n- Underutilizes modern GPUs\n\nWhen to use:\n- CPU training\n- Debugging numerical issues\n- Old GPUs (pre-Volta)\n\nTypical results:\n- Speed: Baseline (100%)\n- Memory: Baseline (100%)\n- Stability: Maximum\n\n4.2) FP16 (Half Precision):\nCharacteristics:\n- 16-bit floats\n- Automatic Mixed Precision (AMP)\n- Loss scaling prevents underflow\n\nAdvantages:\n- 2× FASTER training\n- 50% LESS memory\n- TensorCore acceleration\n- Industry standard\n\nDisadvantages:\n- Rare numerical issues (very rare)\n- Requires Volta+ GPU (RTX 2000+)\n- Loss scaling overhead (minimal)\n\nWhen to use:\n- DEFAULT for RTX 2000-4000 series\n- Production training\n- When BF16 not available\n\nTypical results:\n- Speed: 2× faster!\n- Memory: 50% less!\n- Stability: Very good (99.9% of cases)\n\n4.3) BF16 (Brain Float):\nCharacteristics:\n- 16-bit with better range than FP16\n- Designed by Google for ML\n- No loss scaling needed\n\nAdvantages:\n- 2× FASTER (same as FP16)\n- 50% LESS memory (same as FP16)\n- BETTER stability than FP16\n- No loss scaling needed\n- RECOMMENDED if available\n\nDisadvantages:\n- Requires Ampere+ GPU (RTX 3000+, A100)\n- Not available on older GPUs\n\nWhen to use:\n- RECOMMENDED for RTX 3000+ / A100\n- Production training\n- Maximum performance + stability\n\nTypical results:\n- Speed: 2× faster!\n- Memory: 50% less!\n- Stability: Excellent (better than FP32 in some cases)\n\n5) TYPICAL RANGE / DEFAULT VALUES:\n\n- CPU: FP32 only\n- RTX 2060-2080: FP16 (recommended)\n- RTX 3060-4090: BF16 (recommended)\n- A100/H100: BF16 (optimal)\n- DEFAULT: Auto-detect (BF16 if available, else FP16, else FP32)\n\nRecommendation by GPU:\n- GTX series: FP32 (no TensorCores)\n- RTX 20xx: FP16\n- RTX 30xx/40xx: BF16\n- A100/H100: BF16\n\n6) ADDITIONAL NOTES / BEST PRACTICES:\n\nPrecision comparison:\n```\nFormat | Bits | Range        | Precision | Speed | Memory\n──────────────────────────────────────────────────────────\nFP32   | 32   | 10^±38       | ~7 digits | 1×    | 100%\nFP16   | 16   | 10^±4        | ~3 digits | 2×    | 50%\nBF16   | 16   | 10^±38       | ~2 digits | 2×    | 50%\n```\n\nWhy BF16 is better:\n- Same range as FP32 (exponent: 8 bits)\n- FP16 range too small (exponent: 5 bits)\n- BF16 rarely needs loss scaling\n- FP16 needs loss scaling to prevent underflow\n\nAutomatic Mixed Precision (AMP):\n- Computes in FP16/BF16\n- Accumulates in FP32 (critical operations)\n- Best of both worlds\n- Transparent to user\n\nLoss scaling (FP16 only):\n- Scales loss to prevent underflow\n- Automatic in PyTorch\n- Minimal overhead\n- Not needed for BF16\n\nGPU compatibility check:\n```python\nimport torch\nprint(torch.cuda.is_bf16_supported())  # True for Ampere+\nprint(torch.cuda.get_device_capability())  # (8,0) = Ampere\n```\n\nPerformance by model:\n- Transformers: Maximum benefit (2×)\n- CNNs: Good benefit (1.8×)\n- Small models: Less benefit (1.5×)\n- Large models: Maximum benefit\n\nMemory savings example:\n- FP32: 256ch U-Net = 16GB\n- FP16/BF16: 256ch U-Net = 8GB\n- Can train larger models on same GPU!"
        },
        
        "use_fused_optimizer": {
          "label": "Use Fused Optimizer:",
          "tooltip": "Fused Optimizer - CUDA Kernel Fusion for Adam/AdamW\n\n1) WHAT IT IS:\nFused implementation of Adam/AdamW optimizer.\nCombines multiple CUDA operations into single kernel.\n\n5-15% additional speedup\n\n2) HOW AND WHEN TO USE:\n\nWhen to ENABLE:\n- NVIDIA GPU available\n- Using Adam/AdamW optimizer\n- Want maximum speed\n- Production training\n\nWhen to DISABLE:\n- CPU training\n- Using SGD optimizer\n- Debugging optimizer issues\n\n3) WHY TO USE IT:\n- Faster optimizer step (5-15% speedup)\n- Reduced memory transfers\n- Better GPU utilization\n- Free performance gain\n\n4) EFFECTS:\n\n4.1) DISABLED (Standard Optimizer):\n- Multiple CUDA kernels per step\n- More memory transfers\n- Standard speed\n\nTypical results:\n- Speed: Baseline\n- Works everywhere\n\n4.2) ENABLED (Fused Optimizer):\n- Single fused CUDA kernel\n- Fewer memory transfers\n- 5-15% faster\n\nTypical results:\n- Speed: 5-15% faster\n- Same accuracy\n- No downsides\n\n5) TYPICAL RANGE:\n\n- DEFAULT: ENABLED (if NVIDIA GPU)\n- Requires: NVIDIA GPU, Adam/AdamW optimizer\n\nSpeedup:\n- Small models: +5%\n- Large models: +15%\n- Very large models: +20%\n\n6) NOTES:\n\nCombined speedup:\n- AMP: 2×\n- torch.compile: +40%\n- Fused optimizer: +15%\n- Total: ~3.2× faster than baseline!\n\nCompatibility:\n- Adam: Yes\n- AdamW: Yes\n- SGD: No (use standard)\n\nImplementation:\n- PyTorch: torch.optim.Adam(fused=True)\n- NVIDIA Apex: apex.optimizers.FusedAdam\n- Automatic in ForexGPT"
        },
        
        "use_flash_attention": {
          "label": "Use Flash Attention:",
          "tooltip": "Flash Attention 2 - Memory-Efficient Attention Mechanism\n\n1) WHAT IT IS:\nOptimized attention implementation (Flash Attention 2).\nReduces memory usage from O(N²) to O(N).\n\n2-4× faster attention, 50-80% less memory\n\n2) HOW AND WHEN TO USE:\n\nWhen to ENABLE:\n- Using transformers/attention layers\n- Long sequences (>512 tokens)\n- Memory constrained\n- Production training\n\nWhen to DISABLE:\n- No attention layers (CNNs only)\n- Very short sequences (<64 tokens)\n- Debugging attention\n\n3) WHY TO USE IT:\n- 2-4× FASTER attention\n- 50-80% LESS memory for attention\n- Exact same results (not approximate)\n- Critical for long sequences\n\n4) EFFECTS:\n\n4.1) DISABLED (Standard Attention):\n- O(N²) memory complexity\n- Slower computation\n- Memory bottleneck on long sequences\n\nTypical results:\n- Speed: Baseline\n- Memory: Baseline (high for long sequences)\n- Sequence limit: ~512-1024 tokens\n\n4.2) ENABLED (Flash Attention 2):\n- O(N) memory complexity\n- Optimized IO operations\n- 2-4× faster\n\nTypical results:\n- Speed: 2-4× faster!\n- Memory: 50-80% less!\n- Sequence limit: 4096+ tokens possible\n\n5) TYPICAL RANGE:\n\n- DEFAULT: ENABLED (if attention layers present)\n- Requires: NVIDIA GPU (Ampere+ best), Flash Attention installed\n\nSpeedup by sequence length:\n- 64 tokens: +50%\n- 256 tokens: +2×\n- 1024 tokens: +4×\n- 4096 tokens: +8× (and actually works!)\n\n6) NOTES:\n\nMemory savings critical:\n- Standard: 1024 tokens = 16GB\n- Flash: 1024 tokens = 4GB\n- Enables much longer sequences\n\nDiffusion U-Net:\n- Uses attention in bottleneck layers\n- Flash Attention speeds up those layers\n- Overall speedup: 10-20%\n\nInstallation:\n```bash\npip install flash-attn --no-build-isolation\n```\n\nRequires:\n- CUDA 11.7+\n- PyTorch 2.0+\n- NVIDIA GPU (Ampere+ recommended)"
        },
        
        "grad_accumulation_steps": {
          "label": "Gradient Accumulation Steps:",
          "tooltip": "Gradient Accumulation - Simulate Large Batches on Small GPUs\n\n1) WHAT IT IS:\nNumber of mini-batches to accumulate before updating weights.\nSimulates larger batch size without using more memory.\n\nEffective batch = batch_size × grad_accumulation_steps\n\n2) HOW AND WHEN TO USE:\n\nWhen to use (accumulation > 1):\n- Small GPU memory (4-8GB)\n- Want large effective batch size\n- Memory-limited but need stable gradients\n- Can't fit desired batch size\n\nWhen NOT needed (accumulation = 1):\n- Large GPU memory (16GB+)\n- Batch size fits in memory\n- Speed priority (accumulation adds overhead)\n\n3) WHY TO USE IT:\n- Simulate large batches without memory\n- More stable gradients (large effective batch)\n- Same results as large batch\n- Trade: Memory for time\n\n4) EFFECTS:\n\n4.1) NO ACCUMULATION (steps=1):\nBehavior:\n- Update weights every batch\n- Standard training\n- Fastest (no accumulation overhead)\n\nAdvantages:\n- Maximum speed\n- Simple training loop\n- No additional latency\n\nDisadvantages:\n- Limited by GPU memory\n- Can't use large batches\n\nWhen to use:\n- Batch size fits in memory\n- Don't need larger batches\n\nTypical results:\n- Speed: Baseline (fastest)\n- Memory: As much as needed\n\n4.2) SMALL ACCUMULATION (2-4 steps):\nBehavior:\n- Accumulate 2-4 mini-batches\n- Effective batch 2-4× larger\n- Moderate overhead\n\nAdvantages:\n- 2-4× larger effective batch\n- Same memory as 1× batch\n- Reasonable speed\n\nDisadvantages:\n- 10-20% slower (accumulation overhead)\n\nWhen to use:\n- Want batch 64-128 but only have memory for 32\n- 8-16GB GPU\n\nExample:\n- Real batch: 32 (fits in 8GB)\n- Accumulation: 4 steps\n- Effective batch: 128\n- Memory used: Same as batch 32\n- Speed: ~15% slower than batch 32\n\n4.3) LARGE ACCUMULATION (8-16 steps):\nBehavior:\n- Accumulate 8-16 mini-batches\n- Effective batch 8-16× larger\n- Significant overhead\n\nAdvantages:\n- Very large effective batches (256-512)\n- Works on small GPUs (4GB)\n- Very stable gradients\n\nDisadvantages:\n- 30-50% SLOWER (significant overhead)\n- Updates less frequent\n\nWhen to use:\n- 4-8GB GPU, want batch 256+\n- Research requiring large batches\n- Stability critical\n\nExample:\n- Real batch: 16 (fits in 4GB)\n- Accumulation: 16 steps\n- Effective batch: 256\n- Memory used: Same as batch 16\n- Speed: ~40% slower than batch 16\n\n5) TYPICAL RANGE / DEFAULT VALUES:\n\n- DEFAULT: 1 (no accumulation)\n- Small GPU (4-8GB): 2-8 steps\n- Medium GPU (8-16GB): 1-4 steps\n- Large GPU (16GB+): 1 step (not needed)\n\nRecommendation:\n- If batch fits: Use 1 (no accumulation)\n- If memory limited: Use minimum needed\n- Balance: Memory vs speed\n\n6) ADDITIONAL NOTES / BEST PRACTICES:\n\nHow it works:\n```python\nfor batch in dataloader:\n    loss = model(batch)\n    loss = loss / accumulation_steps  # Scale loss\n    loss.backward()  # Accumulate gradients\n    \n    if (step + 1) % accumulation_steps == 0:\n        optimizer.step()  # Update weights\n        optimizer.zero_grad()  # Clear gradients\n```\n\nLearning rate adjustment:\n- Larger effective batch → may need higher LR\n- Rule: LR_new = LR_base × sqrt(effective_batch / base_batch)\n- Example: batch 32→128 (4×) → LR 1e-4 → 2e-4\n\nMemory vs speed trade-off:\n- Accumulation 1: Fast, memory-hungry\n- Accumulation 4: Moderate, balanced\n- Accumulation 16: Slow, memory-efficient\n\nOverhead sources:\n- Forward pass repeated\n- Gradient accumulation in memory\n- Less GPU utilization (smaller batches)\n\nWhen NOT to use:\n- Batch size already fits comfortably\n- Speed critical\n- Small models (memory not issue)\n\nOptimal settings:\n- Calculate: max_batch = GPU_memory / model_memory\n- If max_batch ≥ desired_batch: accumulation = 1\n- Else: accumulation = ceil(desired_batch / max_batch)\n\nExample calculation:\n- GPU: 8GB available\n- Model: batch 32 uses 6GB\n- Want: batch 128\n- Solution: batch 32 × accumulation 4 = effective 128\n- Memory: 6GB (same as batch 32)\n- Speed: ~15% slower than batch 32 alone"
        }
      },
      
      "feature_engineering": {
        "returns_window": {
          "label": "Returns Window:",
          "tooltip": "Returns Window - Period for Return and Volatility Features\n\n1) WHAT IT IS:\nNumber of bars used to calculate returns and volatility features:\n- Simple returns: (price[t] - price[t-n]) / price[t-n]\n- Log returns: log(price[t] / price[t-n])\n- Volatility: standard deviation of returns over window\n- Momentum: cumulative returns over window\n\n2) HOW AND WHEN TO USE:\n\nWhen to use SHORT window (3-10 bars):\n- Scalping strategies (1m-5m)\n- High-frequency patterns\n- Fast momentum signals\n- Intraday trading\n\nWhen to use MEDIUM window (10-30 bars):\n- DEFAULT for most cases\n- Swing trading (15m-1h)\n- Balanced momentum/volatility\n- General-purpose features\n\nWhen to use LONG window (30-100 bars):\n- Position trading (4h-1d)\n- Long-term trends\n- Stable volatility estimates\n- Lower timeframes modeling daily patterns\n\n3) WHY TO USE IT:\n- Returns = fundamental price feature\n- Volatility = risk/opportunity indicator\n- Momentum = trend strength\n- ML learns: High volatility → bigger moves expected\n- Statistical properties: returns more stationary than prices\n\n4) EFFECTS:\n\n4.1) SHORT WINDOW (3-10):\nBehavior:\n- Fast-changing returns\n- Noisy volatility estimates\n- Captures recent momentum\n\nAdvantages:\n- Quick reaction to changes\n- Good for scalping\n- Captures micro-movements\n\nDisadvantages:\n- Noisy (high variance)\n- Many false signals\n- Unstable volatility\n\nWhen to use:\n- Scalping (1m-5m timeframes)\n- High-frequency strategies\n- Want fast reaction\n\nTypical results:\n- Reactivity: Very high\n- Noise: High\n- Best for: Scalping\n\n4.2) MEDIUM WINDOW (10-30) DEFAULT:\nBehavior:\n- Balanced returns calculation\n- Stable volatility estimates\n- Good momentum indicator\n\nAdvantages:\n- Good signal-to-noise ratio\n- Stable features\n- Works across timeframes\n- Industry standard\n\nDisadvantages:\n- Not as fast as short\n- Not as stable as long\n- (But balanced is usually best!)\n\nWhen to use:\n- DEFAULT (most users)\n- Intraday trading (15m-1h)\n- Balanced strategies\n\nTypical results:\n- Reactivity: Balanced\n- Noise: Moderate\n- Best for: General use\n\n4.3) LONG WINDOW (30-100):\nBehavior:\n- Smooth returns\n- Very stable volatility\n- Long-term momentum\n\nAdvantages:\n- Low noise\n- Stable volatility estimates\n- Captures major trends\n- Good for position trading\n\nDisadvantages:\n- Slow to react\n- May miss short-term opportunities\n- Less relevant for scalping\n\nWhen to use:\n- Position trading (4h-1d)\n- Want stable features\n- Long-term strategies\n\nTypical results:\n- Reactivity: Slow\n- Noise: Low\n- Best for: Position trading\n\n5) TYPICAL RANGE / DEFAULT VALUES:\n\n- Minimum: 3 (very noisy)\n- DEFAULT: 20 (balanced, ~1 trading day on 15m)\n- Scalping: 5-10\n- Intraday: 10-30\n- Position: 30-100\n- Maximum: 100\n\nRecommendation by timeframe:\n- 1m: 5-10 bars (~5-10 minutes)\n- 5m: 10-20 bars (~50-100 minutes)\n- 15m: 20-40 bars (~5-10 hours)\n- 1h: 20-50 bars (~1-2 days)\n- 4h: 10-30 bars (~2-5 days)\n- 1d: 10-30 bars (~2-4 weeks)\n\n6) ADDITIONAL NOTES / BEST PRACTICES:\n\nFeatures generated:\n- returns_1, returns_5, returns_N (multiple horizons)\n- volatility (rolling standard deviation)\n- momentum (cumulative returns)\n- return_to_volatility_ratio (Sharpe-like)\n- ~10-15 return/volatility features\n\nWhy returns matter:\n- Prices non-stationary (trend upward over time)\n- Returns more stationary (mean-reverting)\n- ML models work better with stationary features\n- Returns capture relative changes (what matters for trading)\n\nVolatility applications:\n- High volatility → wider stops, smaller position size\n- Low volatility → tighter stops, larger position\n- ML learns: volatility regime → optimal strategy\n- Volatility clustering: high vol follows high vol\n\nMomentum features:\n- Positive momentum → trend continuation likely\n- Negative momentum → reversal or continuation down\n- ML learns momentum + other features → direction\n\nMulti-window approach:\n- Can generate features for multiple windows\n- Example: returns_5, returns_20, returns_50\n- ML learns multi-timeframe momentum\n- Captures short/medium/long-term patterns\n\nInteraction with timeframe:\n- Lower TF (1m-15m): Use shorter windows (5-20)\n- Higher TF (1h-1d): Use longer windows (20-50)\n- Rule: window × timeframe ≈ 4-24 hours worth of data\n\nComputational cost:\n- Very fast (rolling calculations)\n- Negligible overhead\n- Enable always\n\nStatistical properties:\n- Returns approximately normal distribution\n- Volatility follows GARCH processes\n- Mean reversion in volatility\n- ML captures these patterns automatically"
        },
        
        "session_overlap": {
          "label": "Session Overlap (minutes):",
          "tooltip": "Session Overlap - Transition Period Between Trading Sessions\n\n1) WHAT IT IS:\nNumber of minutes before/after session boundaries to mark as 'overlap zone'.\nAccounts for gradual session transitions (not instant).\n\nExample: 30 min → London-NY overlap = 12:30-13:30 GMT (not just 13:00)\n\n2) HOW AND WHEN TO USE:\n\nWhen to use NO overlap (0 min):\n- Strict session boundaries\n- Textbook session times\n- Simple models\n\nWhen to use SMALL overlap (15-30 min):\n- DEFAULT - Realistic\n- Accounts for gradual transition\n- Most accurate representation\n\nWhen to use LARGE overlap (60+ min):\n- Very smooth transitions\n- Conservative session markers\n- Research purposes\n\n3) WHY TO USE IT:\n- Sessions don't change instantly\n- Liquidity gradual (traders arrive/leave gradually)\n- Volume builds/fades gradually\n- More realistic than hard boundaries\n\n4) EFFECTS:\n\n4.1) NO OVERLAP (0 min):\nBehavior:\n- Hard session boundaries\n- Instant session change\n- Textbook definition\n\nAdvantages:\n- Simple, clear boundaries\n- Easy to interpret\n- Matches textbook times\n\nDisadvantages:\n- Unrealistic (sessions don't change instantly)\n- Misses transition dynamics\n- Binary: in session or not\n\nWhen to use:\n- Simple models\n- Educational purposes\n- Strict definitions\n\nTypical results:\n- Model sees: Instant session change\n- Misses: Transition period dynamics\n\n4.2) SMALL OVERLAP (15-30 min) DEFAULT:\nBehavior:\n- Gradual session transition\n- Overlap zone marked\n- Realistic representation\n\nAdvantages:\n- Realistic (matches market reality)\n- Captures transition dynamics\n- London-NY overlap properly marked\n- DEFAULT and RECOMMENDED\n\nDisadvantages:\n- Slightly more complex\n- Less precise boundaries\n\nWhen to use:\n- DEFAULT (most users)\n- Production trading\n- Want realistic sessions\n\nTypical results:\n- Model learns: Transition period different behavior\n- Example: Volume builds 30min before NY open\n- More accurate session modeling\n\n4.3) LARGE OVERLAP (60+ min):\nBehavior:\n- Very smooth transitions\n- Large overlap zones\n- Conservative approach\n\nAdvantages:\n- Maximum smoothing\n- Accounts for early/late traders\n- Conservative (no sharp boundaries)\n\nDisadvantages:\n- May blur session characteristics\n- Overlap zones very large\n- Less distinct sessions\n\nWhen to use:\n- Research purposes\n- Very smooth transitions\n- Accounting for pre-market activity\n\nTypical results:\n- Sessions blend together\n- Less distinct session patterns\n\n5) TYPICAL RANGE / DEFAULT VALUES:\n\n- Minimum: 0 (hard boundaries)\n- DEFAULT: 30 minutes (realistic)\n- Conservative: 60 minutes (smooth)\n- Maximum: 120 minutes\n\nRecommendation:\n- DEFAULT: 30 minutes (matches market reality)\n- Most traders active 30min before/after official times\n\n6) ADDITIONAL NOTES / BEST PRACTICES:\n\nRealistic session times:\nAsian (Tokyo): 00:00-09:00 GMT\n- With 30min overlap: Actually 23:30-09:30 GMT\n\nLondon: 08:00-17:00 GMT  \n- With 30min overlap: Actually 07:30-17:30 GMT\n\nNew York: 13:00-22:00 GMT\n- With 30min overlap: Actually 12:30-22:30 GMT\n\nLondon-NY overlap:\n- Official: 13:00-17:00 GMT (4 hours)\n- With 30min: 12:30-17:30 GMT (5 hours)\n- More accurate (traders active before/after)\n\nWhy overlap matters:\n- Pre-market: Traders arrive early (place orders)\n- Post-market: Traders stay late (close positions)\n- Volume builds gradually (not instant)\n- Volatility changes gradually\n\nML benefit:\n- Model learns: transition period = different behavior\n- Example: 30min before NY → volume increases\n- More realistic than instant change\n\nFeatures generated:\n- is_session_asian (with overlap)\n- is_session_london (with overlap)\n- is_session_ny (with overlap)\n- is_overlap_london_ny (extended)\n- minutes_to_session_change\n\nInteraction with timeframe:\n- Lower TF (1m-15m): Overlap matters more\n- Higher TF (1h+): Overlap less relevant (bars longer than overlap)\n- Recommendation: 30min for all timeframes\n\nMarket microstructure:\n- Institutional traders: Arrive early, leave late\n- Retail traders: Active during official hours\n- Overlap captures institutional behavior\n- More complete market picture"
        },
        
        "higher_tf": {
          "label": "Higher Timeframe:",
          "tooltip": "Higher Timeframe - Multi-Timeframe Pattern Recognition\n\n1) WHAT IT IS:\nHigher timeframe used for candlestick pattern recognition.\nCaptures longer-term patterns while trading shorter timeframe.\n\nExample: Trading 5m, but check candlestick patterns on 15m\n\n2) HOW AND WHEN TO USE:\n\nWhen to use SAME timeframe:\n- Simple strategies\n- Don't want multi-timeframe\n- Pattern recognition on trading TF only\n\nWhen to use 2-3× HIGHER:\n- DEFAULT and RECOMMENDED\n- Multi-timeframe analysis\n- Example: Trade 5m, patterns on 15m\n- Better pattern reliability\n\nWhen to use 10×+ HIGHER:\n- Long-term context\n- Major pattern signals\n- Example: Trade 15m, patterns on 4h\n\n3) WHY TO USE IT:\n- Higher TF patterns more reliable\n- Filters noise (lower TF patterns often false)\n- Institutional traders watch higher TF\n- Multi-timeframe = professional approach\n- Prevents trading against major trend\n\n4) EFFECTS:\n\n4.1) SAME AS TRADING TF:\nBehavior:\n- Patterns on same timeframe\n- Many pattern signals\n- High noise\n\nAdvantages:\n- Many signals\n- Fast reaction\n- Simple approach\n\nDisadvantages:\n- Lower TF patterns less reliable\n- Many false signals\n- Noise overwhelming\n\nWhen to use:\n- Scalping (want many signals)\n- Simple strategies\n\nTypical results:\n- Signals: Many (high noise)\n- Reliability: Lower\n- May overfit to noise\n\n4.2) 2-3× HIGHER (DEFAULT):\nBehavior:\n- Patterns on higher timeframe\n- Fewer but better signals\n- Balanced approach\n\nAdvantages:\n- Better pattern reliability\n- Filters noise\n- Still reasonably reactive\n- RECOMMENDED\n\nDisadvantages:\n- Fewer signals than same TF\n- Slightly slower reaction\n\nWhen to use:\n- DEFAULT (most users)\n- Intraday trading\n- Want reliable patterns\n\nExamples:\n- Trade 1m → patterns on 5m\n- Trade 5m → patterns on 15m (COMMON)\n- Trade 15m → patterns on 1h (COMMON)\n\nTypical results:\n- Signals: Moderate (good quality)\n- Reliability: High\n- Best balance\n\n4.3) 10×+ HIGHER:\nBehavior:\n- Patterns on much higher TF\n- Very few but very strong signals\n- Long-term context\n\nAdvantages:\n- Maximum pattern reliability\n- Major signals only\n- Aligns with institutional view\n\nDisadvantages:\n- Very few signals\n- Slow reaction\n- May miss shorter-term opportunities\n\nWhen to use:\n- Want only major signals\n- Align with higher TF trend\n- Conservative trading\n\nExamples:\n- Trade 5m → patterns on 1h\n- Trade 15m → patterns on 4h (swing traders)\n- Trade 1h → patterns on 1d (position traders)\n\nTypical results:\n- Signals: Few (very high quality)\n- Reliability: Maximum\n- Miss many shorter-term moves\n\n5) TYPICAL RANGE / DEFAULT VALUES:\n\nCommon combinations:\n- Trade 1m → patterns 5m (5× higher)\n- Trade 5m → patterns 15m (3× higher) COMMON\n- Trade 15m → patterns 1h (4× higher) COMMON\n- Trade 1h → patterns 4h (4× higher)\n- Trade 4h → patterns 1d (6× higher)\n\nRecommendation:\n- DEFAULT: 3-5× higher than trading timeframe\n- Scalping: 2-3× higher\n- Swing: 5-10× higher\n\n6) ADDITIONAL NOTES / BEST PRACTICES:\n\nPattern reliability by timeframe:\n- 1m patterns: ~45-50% reliability\n- 5m patterns: ~50-55% reliability\n- 15m patterns: ~55-60% reliability\n- 1h patterns: ~60-65% reliability\n- 4h patterns: ~65-70% reliability\n- 1d patterns: ~70-75% reliability\n\nWhy higher TF better:\n- More data per candle (more traders involved)\n- Less noise (market consensus)\n- Institutional traders watch higher TF\n- Daily patterns: Entire day's price action\n\nMulti-timeframe strategy:\n- Higher TF: Trend direction (1h/4h)\n- Medium TF: Entry signals (15m patterns)\n- Lower TF: Precise entry (5m price action)\n- Classic professional approach\n\nML advantage:\n- Model learns: Which higher TF to trust\n- Combines current TF + higher TF patterns\n- Context-aware: Higher TF pattern + lower TF confirmation\n\nFeatures generated:\n- Candlestick patterns detected on higher TF\n- ~50 pattern features (Doji, Hammer, Engulfing, etc.)\n- But on higher TF = more reliable\n\nExample combination:\nTrade 15m with patterns on 1h:\n- 1h Bullish Engulfing → strong buy context\n- Wait for 15m confirmation → precise entry\n- Exit on 15m signals → active management\n- Stop on 1h pattern invalidation → wide stop\n\nComputational cost:\n- Need to fetch higher TF data\n- Pattern recognition fast\n- Minimal overhead\n\nAlignment with trading psychology:\n- Prevents: Trading 5m against 1h downtrend\n- Ensures: Trade with higher TF momentum\n- Result: Better win rate, less whipsaw"
        },
        
        "vp_bins": {
          "label": "Volume Profile Bins:",
          "tooltip": "Volume Profile Bins - Price Level Granularity\n\n1) WHAT IT IS:\nNumber of price levels (bins) for volume profile histogram.\nMore bins = finer granularity, less bins = smoother profile.\n\nEach bin = price range where volume is aggregated\n\n2) HOW AND WHEN TO USE:\n\nWhen to use FEW bins (20-40):\n- Smooth volume profile\n- Major levels only\n- Higher timeframes (1h-1d)\n- Simple analysis\n\nWhen to use MEDIUM bins (40-80):\n- DEFAULT - Balanced\n- Good granularity\n- Works across timeframes\n- Most common\n\nWhen to use MANY bins (80-200):\n- Very detailed profile\n- Intraday scalping (1m-5m)\n- Precise POC/levels\n- Research purposes\n\n3) WHY IT MATTERS:\n- POC precision (Point of Control)\n- Level identification\n- Value Area accuracy\n- Too few = miss levels, too many = noise\n\n4) EFFECTS:\n\n4.1) FEW BINS (20-40):\nBehavior:\n- Large price ranges per bin\n- Smooth histogram\n- Broad levels\n\nAdvantages:\n- Clear major levels\n- No noise\n- Fast computation\n- Good for daily charts\n\nDisadvantages:\n- Low precision\n- May miss important levels\n- POC less accurate\n\nWhen to use:\n- Higher timeframes (4h-1d)\n- Want major levels only\n- Smooth analysis\n\nTypical results:\n- POC: Broad zone (less precise)\n- Levels: Major only\n- Best for: Position trading\n\n4.2) MEDIUM BINS (40-80) DEFAULT:\nBehavior:\n- Moderate price ranges\n- Balanced histogram\n- Good detail without noise\n\nAdvantages:\n- Good POC precision\n- Captures important levels\n- Balanced granularity\n- Works across timeframes\n- RECOMMENDED\n\nDisadvantages:\n- Not as smooth as few bins\n- Not as detailed as many bins\n- (But balance is usually best!)\n\nWhen to use:\n- DEFAULT (most users)\n- Intraday trading (15m-1h)\n- General purpose\n\nTypical results:\n- POC: Good precision\n- Levels: All important levels captured\n- Best for: General use\n\n4.3) MANY BINS (80-200):\nBehavior:\n- Small price ranges per bin\n- Detailed histogram\n- Very precise levels\n\nAdvantages:\n- Maximum precision\n- All micro-levels captured\n- Very accurate POC\n- Good for scalping\n\nDisadvantages:\n- May capture noise\n- Slower computation\n- Overly detailed (may confuse)\n\nWhen to use:\n- Scalping (1m-5m)\n- Want maximum precision\n- Research purposes\n\nTypical results:\n- POC: Very precise\n- Levels: All captured (including noise)\n- Best for: Scalping\n\n5) TYPICAL RANGE / DEFAULT VALUES:\n\n- Minimum: 10 (very smooth)\n- DEFAULT: 50 (balanced)\n- Scalping: 80-200 (detailed)\n- Position: 20-40 (smooth)\n- Maximum: 200\n\nRecommendation by timeframe:\n- 1m-5m: 80-150 bins (scalping)\n- 15m-1h: 40-80 bins (intraday) DEFAULT\n- 4h-1d: 20-40 bins (position)\n\nRecommendation by symbol volatility:\n- Low volatility (EUR/USD): More bins (80-100)\n- High volatility (GBP/JPY): Fewer bins (40-60)\n\n6) ADDITIONAL NOTES / BEST PRACTICES:\n\nWhat is a bin:\n- Price range: (high - low) / num_bins\n- Example: Range 1.0000-1.0100, 50 bins → each bin = 20 pips\n- Each bin accumulates volume at that price level\n\nPOC (Point of Control):\n- Bin with most volume\n- With more bins: POC more precise\n- Too many bins: POC jumps around (noise)\n- Sweet spot: 50 bins for most cases\n\nValue Area:\n- Price range containing 70% of volume\n- More bins: More precise boundaries\n- Fewer bins: Smoother boundaries\n\nHigh/Low Volume Nodes (HVN/LVN):\n- HVN: Bins with lots of volume (support/resistance)\n- LVN: Bins with little volume (fast moves)\n- More bins: More HVN/LVN detected\n- Fewer bins: Only major HVN/LVN\n\nComputational cost:\n- Linear with num_bins\n- 200 bins ≈ 4× slower than 50 bins\n- Still fast (milliseconds)\n\nInteraction with vp_window:\n- vp_window: How many bars to analyze\n- vp_bins: How many price levels\n- Both needed for complete volume profile\n- Example: 100 bars, 50 bins = volume distribution over 100 bars split into 50 price levels\n\nHistogram shape:\n- Normal distribution (bell curve): Balanced market\n- P-shaped: Bullish (volume at bottom → accumulation)\n- b-shaped: Bearish (volume at top → distribution)\n- More bins: Shape more detailed\n\nML feature engineering:\n- poc_distance: Distance from current price to POC\n- in_value_area: Boolean (inside 70% volume zone)\n- hvn_proximity: Distance to nearest HVN\n- lvn_proximity: Distance to nearest LVN\n- profile_shape: Distribution type\n- More bins = more precise features"
        },
        
        "vp_window": {
          "label": "Volume Profile Window:",
          "tooltip": "Volume Profile Window - Historical Period for VP Calculation\n\n1) WHAT IT IS:\nNumber of bars used to build volume profile histogram.\nLonger window = longer-term levels, shorter = recent levels.\n\nExample: 100 bars on 15m = 25 hours of volume distribution\n\n2) HOW AND WHEN TO USE:\n\nWhen to use SHORT window (20-50 bars):\n- Recent levels (intraday)\n- Scalping/day trading\n- Fast-changing markets\n- Current session focus\n\nWhen to use MEDIUM window (50-150 bars):\n- DEFAULT - Balanced\n- Swing trading\n- Works across timeframes\n- ~1-3 days context (on 15m-1h)\n\nWhen to use LONG window (150-500 bars):\n- Long-term levels\n- Position trading\n- Major support/resistance\n- Weekly/monthly context\n\n3) WHY TO USE IT:\n- Longer window = more stable levels\n- Shorter window = more reactive\n- Trade-off: Stability vs responsiveness\n- Match to trading horizon\n\n4) EFFECTS:\n\n4.1) SHORT WINDOW (20-50):\nBehavior:\n- Recent volume only\n- POC changes frequently\n- Intraday levels\n\nAdvantages:\n- Reactive to recent action\n- Current session levels\n- Good for scalping\n- Levels relevant to today\n\nDisadvantages:\n- POC unstable (changes often)\n- Less reliable levels\n- Short-term focus only\n\nWhen to use:\n- Scalping (1m-15m)\n- Day trading focus\n- Want current session levels\n\nExample:\n- 50 bars × 15m = 12.5 hours\n- ~1.5 trading sessions\n\nTypical results:\n- POC: Changes hourly\n- Levels: Intraday only\n- Best for: Scalping\n\n4.2) MEDIUM WINDOW (50-150) DEFAULT:\nBehavior:\n- Recent days of volume\n- Stable but responsive POC\n- Multi-day levels\n\nAdvantages:\n- Good stability\n- Reasonably reactive\n- Captures recent market structure\n- Works across timeframes\n- RECOMMENDED\n\nDisadvantages:\n- Not as reactive as short\n- Not as stable as long\n- (But balance usually best!)\n\nWhen to use:\n- DEFAULT (most users)\n- Intraday/swing trading\n- 15m-1h timeframes\n\nExample:\n- 100 bars × 15m = 25 hours\n- ~3 trading sessions\n- 100 bars × 1h = 4+ days\n\nTypical results:\n- POC: Stable (changes daily)\n- Levels: Recent week\n- Best for: General use\n\n4.3) LONG WINDOW (150-500):\nBehavior:\n- Long-term volume\n- Very stable POC\n- Major levels\n\nAdvantages:\n- Maximum stability\n- Major support/resistance\n- Institutional levels\n- Monthly/yearly context\n\nDisadvantages:\n- Slow to react\n- Levels may be old/stale\n- Misses recent changes\n\nWhen to use:\n- Position trading (4h-1d)\n- Want major levels only\n- Long-term context\n\nExample:\n- 300 bars × 1h = 12.5 days\n- 500 bars × 4h = 83 days (~3 months!)\n\nTypical results:\n- POC: Very stable (weekly changes)\n- Levels: Major long-term\n- Best for: Position trading\n\n5) TYPICAL RANGE / DEFAULT VALUES:\n\n- Minimum: 20 (very recent)\n- DEFAULT: 100 (balanced)\n- Scalping: 30-60\n- Intraday: 60-150\n- Swing: 100-250  \n- Position: 200-500\n- Maximum: 500\n\nRecommendation by timeframe:\n- 1m: 60-120 bars (1-2 hours)\n- 5m: 50-100 bars (4-8 hours)\n- 15m: 60-150 bars (15-37 hours) DEFAULT\n- 1h: 100-200 bars (4-8 days)\n- 4h: 100-300 bars (17-50 days)\n- 1d: 50-200 bars (2-8 months)\n\nRule of thumb:\n- window × timeframe ≈ 1-7 days worth of data\n- Scalping: 1 day\n- Swing: 3-5 days\n- Position: 1-3 months\n\n6) ADDITIONAL NOTES / BEST PRACTICES:\n\nWhat gets calculated:\n- Volume at each price level over window\n- POC: Price with most volume\n- Value Area: 70% volume zone\n- HVN/LVN: High/low volume nodes\n\nPOC interpretation:\n- Short window POC: Intraday support/resistance\n- Medium window POC: Weekly support/resistance\n- Long window POC: Monthly+ support/resistance\n- Different timeframes, different meanings\n\nMulti-window approach:\n- Can calculate multiple windows\n- Example: VP_30 (intraday), VP_100 (weekly), VP_300 (monthly)\n- ML sees all timeframe levels\n- Professional institutional approach\n\nValue Area shifts:\n- Short window: VA shifts daily\n- Medium window: VA shifts weekly\n- Long window: VA stable for months\n\nTrading applications:\n- Short window: Scalping levels (30-60 bars)\n- Medium window: Swing levels (100-150 bars) DEFAULT\n- Long window: Position levels (200-500 bars)\n- Use multiple for multi-timeframe analysis\n\nInteraction with vp_bins:\n- vp_window: How far back to look\n- vp_bins: How many price levels\n- Both needed together\n- Example: 100 bars, 50 bins = last 100 bars split into 50 price levels\n\nComputational cost:\n- Linear with window size\n- 500 bars = 5× slower than 100 bars\n- Still fast (10-50ms)\n- Worth the insight\n\nForex vs Stocks:\n- Forex: Use cautiously (tick volume not true volume)\n- Stocks/Futures: Very reliable (true volume)\n- Recommendation: Shorter windows for Forex (less reliable)\n\nSession-based approach:\n- Alternative: Use session volume (not fixed bars)\n- Example: Today's session volume profile\n- More advanced but powerful\n- ForexGPT supports both\n\nML features from VP:\n- poc_distance_short (30 bars)\n- poc_distance_medium (100 bars) \n- poc_distance_long (300 bars)\n- Model learns which timeframe POC matters when"
        },
        
        "vsa_volume_ma": {
          "label": "VSA Volume MA Period:",
          "tooltip": "VSA Volume MA - Moving Average Period for Volume Baseline\n\n1) WHAT IT IS:\nMoving average period for volume used as 'normal volume' baseline.\nVSA compares current volume vs this MA to detect anomalies.\n\nHigh volume = current > MA, Low volume = current < MA\n\n2) HOW AND WHEN TO USE:\n\nWhen to use SHORT MA (5-15):\n- Fast-changing volume baseline\n- Scalping (1m-15m)\n- Recent volume context\n- Quick anomaly detection\n\nWhen to use MEDIUM MA (15-30):\n- DEFAULT - Balanced\n- Intraday/swing trading\n- Works across timeframes\n- Standard VSA approach\n\nWhen to use LONG MA (30-100):\n- Stable volume baseline\n- Position trading\n- Long-term volume context\n- Filters intraday noise\n\n3) WHY TO USE IT:\n- Need baseline to define 'high' vs 'normal' volume\n- Climax volume = volume >> MA\n- No demand = volume << MA\n- MA period affects sensitivity\n\n4) EFFECTS:\n\n4.1) SHORT MA (5-15):\nBehavior:\n- Volume baseline changes quickly\n- Sensitive to recent volume\n- Many high/low volume signals\n\nAdvantages:\n- Quick reaction\n- Detects recent anomalies\n- Good for scalping\n- Current context\n\nDisadvantages:\n- Noisy (many signals)\n- Baseline unstable\n- May overreact\n\nWhen to use:\n- Scalping (1m-15m)\n- Want fast detection\n- Recent volume focus\n\nTypical results:\n- Signals: Many (high sensitivity)\n- Noise: High\n- Best for: Scalping\n\n4.2) MEDIUM MA (15-30) DEFAULT:\nBehavior:\n- Stable volume baseline\n- Balanced sensitivity\n- Good anomaly detection\n\nAdvantages:\n- Good signal quality\n- Stable baseline\n- Standard VSA practice\n- Works across timeframes\n- RECOMMENDED\n\nDisadvantages:\n- Not as fast as short MA\n- Not as stable as long MA\n- (But balance usually best!)\n\nWhen to use:\n- DEFAULT (most users)\n- Intraday/swing trading\n- Standard VSA analysis\n\nTypical results:\n- Signals: Moderate (good quality)\n- Baseline: Stable\n- Best for: General use\n- DEFAULT: 20 (Wyckoff standard)\n\n4.3) LONG MA (30-100):\nBehavior:\n- Very stable baseline\n- Low sensitivity\n- Only major anomalies detected\n\nAdvantages:\n- Maximum stability\n- Only major volume events\n- Filters noise completely\n- Long-term context\n\nDisadvantages:\n- Slow reaction\n- May miss shorter-term signals\n- Less sensitive\n\nWhen to use:\n- Position trading (4h-1d)\n- Want only major events\n- Long-term analysis\n\nTypical results:\n- Signals: Few (very high quality)\n- Baseline: Very stable\n- Best for: Position trading\n\n5) TYPICAL RANGE / DEFAULT VALUES:\n\n- Minimum: 5 (very sensitive)\n- DEFAULT: 20 (Wyckoff standard)\n- Scalping: 10-15\n- Intraday: 15-25\n- Swing: 20-40\n- Position: 30-100\n- Maximum: 100\n\nRecommendation:\n- DEFAULT: 20 (Thomas Wyckoff original)\n- Most tested configuration\n- Industry standard\n\n6) ADDITIONAL NOTES / BEST PRACTICES:\n\nVSA concepts using volume MA:\n\n1. Climax Volume:\n   - volume > 2× volume_MA\n   - Exhaustion signal\n   - Potential reversal\n\n2. High Volume:\n   - volume > 1.5× volume_MA\n   - Strong activity\n   - Institutional participation\n\n3. Normal Volume:\n   - volume ≈ volume_MA (0.8-1.2×)\n   - Standard market activity\n   - No special signals\n\n4. Low Volume:\n   - volume < 0.7× volume_MA\n   - Weak activity\n   - No demand/supply signals\n\nThresholds (using 20-period MA):\n- Climax: >2.0× MA\n- High: 1.5-2.0× MA\n- Normal: 0.7-1.5× MA  \n- Low: <0.7× MA\n\nInteraction with vsa_spread_ma:\n- volume_ma: Measures volume anomalies\n- spread_ma: Measures spread anomalies\n- VSA combines both:\n  - High volume + wide spread = strength\n  - High volume + narrow spread = absorption\n  - Low volume + wide spread = weakness\n\nWyckoff Method:\n- Original: 20-period MA\n- Thomas Wyckoff (1930s)\n- Still used by institutions\n- DEFAULT: 20 (historical standard)\n\nModern VSA:\n- Tom Williams (1990s) - formalized VSA\n- Also uses 20-period MA\n- Confirmed optimal through testing\n\nComputational cost:\n- Very fast (rolling average)\n- Negligible overhead\n- Enable always (if using VSA)\n\nForex caveat:\n- Forex = tick volume (not true volume)\n- VSA less reliable than stocks/futures\n- Still useful but with caution\n- Stocks/Futures: Very reliable\n\nML features generated:\n- volume_ratio (current / MA)\n- is_climax_volume (>2× MA)\n- is_high_volume (>1.5× MA)\n- is_low_volume (<0.7× MA)\n- volume_percentile (rolling)\n- ~10 volume-based VSA features"
        },
        
        "vsa_spread_ma": {
          "label": "VSA Spread MA Period:",
          "tooltip": "VSA Spread MA - Moving Average for Spread Baseline\n\n1) WHAT IT IS:\nMoving average period for candle spread (high-low range).\nUsed as baseline to identify wide/narrow spreads.\n\nSpread = high - low of candle\n\n2) HOW AND WHEN TO USE:\n\nWhen to use SHORT MA (5-15):\n- Fast-changing spread baseline\n- Scalping (1m-15m)\n- Recent volatility context\n- Quick anomaly detection\n\nWhen to use MEDIUM MA (15-30):\n- DEFAULT - Balanced\n- Standard VSA practice\n- Works across timeframes\n- Recommended\n\nWhen to use LONG MA (30-100):\n- Stable spread baseline\n- Position trading\n- Long-term volatility context\n- Filters short-term noise\n\n3) WHY TO USE IT:\n- Need baseline for 'wide' vs 'normal' spread\n- Wide spread = high volatility\n- Narrow spread = low volatility/indecision\n- Effort vs Result analysis\n\n4) EFFECTS:\n\n4.1) SHORT MA (5-15):\nBehavior:\n- Spread baseline adapts quickly\n- Sensitive to recent volatility\n- Many wide/narrow spread signals\n\nAdvantages:\n- Quick reaction to volatility changes\n- Current market context\n- Good for scalping\n\nDisadvantages:\n- Noisy (many signals)\n- Baseline unstable\n- May overreact\n\nWhen to use:\n- Scalping (1m-15m)\n- Fast volatility adaptation\n- Intraday focus\n\nTypical results:\n- Signals: Many (high sensitivity)\n- Baseline: Changes quickly\n- Best for: Scalping\n\n4.2) MEDIUM MA (15-30) DEFAULT:\nBehavior:\n- Stable spread baseline\n- Balanced sensitivity\n- Good anomaly detection\n\nAdvantages:\n- Good signal quality\n- Stable baseline\n- Standard VSA practice (20 period)\n- Works across timeframes\n- RECOMMENDED\n\nDisadvantages:\n- Not as fast as short MA\n- Not as stable as long MA\n- (But balance usually best!)\n\nWhen to use:\n- DEFAULT (most users)\n- Intraday/swing trading\n- Standard VSA analysis\n\nTypical results:\n- Signals: Moderate (good quality)\n- Baseline: Stable\n- Best for: General use\n- DEFAULT: 20 (Wyckoff standard)\n\n4.3) LONG MA (30-100):\nBehavior:\n- Very stable baseline\n- Low sensitivity\n- Only major spread changes\n\nAdvantages:\n- Maximum stability\n- Only major volatility events\n- Long-term context\n- Filters all noise\n\nDisadvantages:\n- Slow reaction\n- May miss shorter-term signals\n- Less sensitive to changes\n\nWhen to use:\n- Position trading (4h-1d)\n- Want only major volatility changes\n- Long-term analysis\n\nTypical results:\n- Signals: Few (very high quality)\n- Baseline: Very stable\n- Best for: Position trading\n\n5) TYPICAL RANGE / DEFAULT VALUES:\n\n- Minimum: 5 (very sensitive)\n- DEFAULT: 20 (Wyckoff standard)\n- Scalping: 10-15\n- Intraday: 15-25\n- Swing: 20-40\n- Position: 30-100\n- Maximum: 100\n\nRecommendation:\n- DEFAULT: 20 (matches vsa_volume_ma)\n- Same period for consistency\n- Industry standard\n\n6) ADDITIONAL NOTES / BEST PRACTICES:\n\nVSA spread concepts:\n\n1. Wide Spread:\n   - spread > 1.5× spread_MA\n   - High volatility\n   - Strong move\n   - Effort (energy) in market\n\n2. Normal Spread:\n   - spread ≈ spread_MA (0.7-1.5×)\n   - Standard volatility\n   - Balanced market\n\n3. Narrow Spread:\n   - spread < 0.7× spread_MA\n   - Low volatility\n   - Indecision or absorption\n   - No effort\n\nVSA Effort vs Result:\n```\nEffort = Volume (activity)\nResult = Spread (price movement)\n\nAnalysis:\n- High effort, high result = Strength (continuation)\n- High effort, low result = Absorption (reversal coming)\n- Low effort, high result = Strength (easy move, continuation)\n- Low effort, low result = No interest (avoid)\n```\n\nCombined VSA patterns:\n\n1. Climax (reversal):\n   - volume > 2× volume_MA (high effort)\n   - spread > 1.5× spread_MA (high result)\n   - At extreme (top/bottom)\n   - Interpretation: Exhaustion\n\n2. No Demand (bearish):\n   - volume < 0.7× volume_MA (low effort)\n   - spread < 0.7× spread_MA (low result)\n   - Price up (trying to go up)\n   - Interpretation: No buyers, expect down\n\n3. No Supply (bullish):\n   - volume < 0.7× volume_MA (low effort)\n   - spread < 0.7× spread_MA (low result)\n   - Price down (trying to go down)\n   - Interpretation: No sellers, expect up\n\n4. Stopping Volume (bullish):\n   - volume > 2× volume_MA (high effort)\n   - spread < 0.7× spread_MA (low result)\n   - Price down (selling absorbed)\n   - Interpretation: Smart money buying, expect up\n\nThresholds (using 20-period MA):\n- Wide spread: >1.5× MA\n- Normal spread: 0.7-1.5× MA\n- Narrow spread: <0.7× MA\n\nInteraction with volume MA:\n- MUST match vsa_volume_ma period\n- Recommendation: Both = 20\n- Consistent baseline for effort vs result\n\nWyckoff VSA:\n- Original method (1930s)\n- Both volume and spread use 20-period MA\n- Still industry standard\n- Institutions still use this\n\nML features generated:\n- spread_ratio (current / MA)\n- is_wide_spread (>1.5× MA)\n- is_narrow_spread (<0.7× MA)\n- effort_result_divergence\n- vsa_pattern (climax, no_demand, etc.)\n- ~15 VSA features total\n\nComputational cost:\n- Very fast (rolling average + comparisons)\n- Negligible overhead\n- Enable if using VSA\n\nForex vs Stocks:\n- Spread calculation same (high-low)\n- Works equally well\n- But volume less reliable on Forex\n- Combined: Use with caution on Forex"
        }
      },
      
      "diffusion_final": {
        "beta_schedule": {
          "label": "Beta Schedule (Noise Schedule):",
          "tooltip": "Beta Schedule - Noise Addition Pattern\n\n1) WHAT IT IS:\nHow noise is added over diffusion timesteps.\nControls the rate of noise addition from t=0 (clean) to t=T (pure noise).\n\nCommon schedules: linear, cosine, quadratic\n\n2) HOW AND WHEN TO USE:\n\nLINEAR:\n- Uniform noise addition\n- Simple, well-tested\n- DEFAULT for most cases\n\nCOSINE:\n- Slower noise at start/end\n- Better for images (preserves structure)\n- Recommended for high-quality generation\n\nQUADRATIC:\n- Faster noise at start\n- Good for quick diffusion\n\n3) WHY IT MATTERS:\n- Affects generation quality\n- Training stability\n- Sampling efficiency\n\n4) EFFECTS:\n\n4.1) LINEAR:\n- Constant rate β_t increases linearly\n- Simple and interpretable\n- Good baseline\n\nTypical results:\n- Quality: Good\n- Training: Stable\n- DEFAULT choice\n\n4.2) COSINE:\n- Slower noise at extremes\n- More timesteps for clean/noisy\n- Better perceptual quality\n\nTypical results:\n- Quality: Better (especially images)\n- Training: Very stable\n- RECOMMENDED for production\n\n4.3) QUADRATIC:\n- Aggressive noise early\n- Faster convergence\n- Less common\n\nTypical results:\n- Quality: Slightly worse\n- Training: Fast but less stable\n\n5) TYPICAL RANGE:\n\n- DEFAULT: Linear (most tested)\n- HIGH QUALITY: Cosine (recommended)\n- FAST: Quadratic (experimental)\n\n6) NOTES:\n\nSchedule formulas:\n- Linear: β_t = β_start + (β_end - β_start) × t/T\n- Cosine: More complex, preserves information better\n\nRecommendation:\n- Start: Linear (well-tested)\n- Production: Cosine (better quality)\n- Research: Experiment with both"
        }
      }
    }
  }
}
